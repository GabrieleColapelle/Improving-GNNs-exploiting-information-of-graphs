{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2e456a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: to be able to use all crisp methods, you need to install some additional packages:  {'wurlitzer', 'infomap', 'karateclub', 'graph_tool'}\n",
      "Note: to be able to use all overlapping methods, you need to install some additional packages:  {'karateclub', 'ASLPAw'}\n",
      "Note: to be able to use all bipartite methods, you need to install some additional packages:  {'wurlitzer', 'infomap'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import itertools\n",
    "\n",
    "\n",
    "import dgl\n",
    "from dgl.nn import HeteroGraphConv\n",
    "from dgl.utils import extract_node_subframes, set_new_frames\n",
    "\n",
    "from sklearn.metrics import f1_score, normalized_mutual_info_score, adjusted_rand_score\n",
    "\n",
    "import copy\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "import json\n",
    "import ast\n",
    "import collections\n",
    "from dgl import AddMetaPaths\n",
    "from cdlib import algorithms\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e77b847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_com_info_DBLP():    \n",
    "    with open('com_DBLP.txt') as f:\n",
    "        data_dict = f.read()\n",
    "    com_dict = ast.literal_eval(data_dict)\n",
    "    com = []\n",
    "    #find the max community label\n",
    "    val_p = max(com_dict['paper'])\n",
    "    val_a = max(com_dict['author'])\n",
    "    val_c = max(com_dict['conf'])\n",
    "    val_t = max(com_dict['term'])\n",
    "\n",
    "    com.append(val_p)\n",
    "    com.append(val_a)\n",
    "    com.append(val_c)\n",
    "    com.append(val_t)\n",
    "    num_com = max(com) + 1\n",
    "    com_dim_emb = 10\n",
    "    return com_dict,num_com,com_dim_emb\n",
    "\n",
    "def get_com_info_IMDb():    \n",
    "    with open('com_IMDb.txt') as f:\n",
    "        data_dict = f.read()\n",
    "    com_dict = ast.literal_eval(data_dict)\n",
    "    #find the max community label\n",
    "    com = []\n",
    "    val_a = max(com_dict['actor'])\n",
    "    val_m = max(com_dict['movie'])\n",
    "    val_d = max(com_dict['director'])\n",
    "   \n",
    "    com.append(val_a)\n",
    "    com.append(val_m)\n",
    "    com.append(val_d)\n",
    "    num_com = max(com) + 1\n",
    "    com_dim_emb = 10\n",
    "    return com_dict,num_com,com_dim_emb\n",
    "\n",
    "def get_com_info_ACM():    \n",
    "    with open('com_Louvain_ACM.txt') as f:\n",
    "        data_dict = f.read()\n",
    "    com_dict = ast.literal_eval(data_dict)\n",
    "    #find the max community label\n",
    "    com = []\n",
    "    val_a = max(com_dict['author'])\n",
    "    val_p = max(com_dict['paper'])\n",
    "    val_f = max(com_dict['field'])\n",
    "   \n",
    "    com.append(val_a)\n",
    "    com.append(val_p)\n",
    "    com.append(val_f)\n",
    "    num_com = max(com) + 1\n",
    "    com_dim_emb = 10\n",
    "    return com_dict,num_com,com_dim_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbbf1bb",
   "metadata": {},
   "source": [
    "###### DBLP COMMUNITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "019aa5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_com_Louvain_DBLP(hg):\n",
    "    \n",
    "    hg1 = copy.deepcopy(hg)\n",
    "    transform = AddMetaPaths({\n",
    "    #'apa': [('author', 'ap', 'paper'),('paper', 'pa', 'author')],\n",
    "    #'pap': [('paper', 'pa', 'author'),('author','ap','paper')],\n",
    "    #'ptp': [('paper', 'pt', 'term'),('term','tp','paper')],\n",
    "    #'tpt': [('term', 'tp', 'paper'),('paper','pt','term')],\n",
    "    #'pcp': [('paper', 'pc', 'conf'),('conf','cp','paper')],\n",
    "    #'cpc': [('conf', 'cp', 'paper'),('paper','pc','conf')]\n",
    "    })\n",
    "    hg2 = transform(hg1)\n",
    "    g = dgl.to_homogeneous(hg2)\n",
    "    nxg = g.to_networkx()\n",
    "    #n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    \n",
    "    d = community.louvain_communities(G2)\n",
    "    \n",
    "\n",
    "    data = []\n",
    "    for i in range(len(d)):\n",
    "        data.append(torch.Tensor(list(list(d)[i])).to(torch.int64))\n",
    "    \n",
    "    com_dict = {}\n",
    "    for i in range(len(data)):\n",
    "        for el in data[i]:\n",
    "            com_dict[el.item()] = i\n",
    "    ordered_com_dict = collections.OrderedDict(sorted(com_dict.items()))\n",
    "    \n",
    "    com = list(ordered_com_dict.values())\n",
    "    dict2 = {}\n",
    "    dict2['author'] = com[0:4057]\n",
    "    dict2['conf'] = com[4057:4077]\n",
    "    dict2['paper'] = com[4077:18405]\n",
    "    dict2['term'] = com[18405:26128]\n",
    "    with open('com_DBLP.txt', 'w') as convert_file:\n",
    "        convert_file.write(json.dumps(dict2))\n",
    "    name = 'com_DBLP.txt'\n",
    "    return name\n",
    "\n",
    "def get_com_Lpa_DBLP(hg):\n",
    "    \n",
    "    hg1 = copy.deepcopy(hg)\n",
    "    transform = AddMetaPaths({\n",
    "    #'apa': [('author', 'ap', 'paper'),('paper', 'pa', 'author')],\n",
    "    #'pap': [('paper', 'pa', 'author'),('author','ap','paper')],\n",
    "    #'ptp': [('paper', 'pt', 'term'),('term','tp','paper')],\n",
    "    #'tpt': [('term', 'tp', 'paper'),('paper','pt','term')],\n",
    "    #'pcp': [('paper', 'pc', 'conf'),('conf','cp','paper')],\n",
    "    'cpc': [('conf', 'cp', 'paper'),('paper','pc','conf')]\n",
    "    })\n",
    "    hg2 = transform(hg1)\n",
    "    g = dgl.to_homogeneous(hg2)\n",
    "    nxg = g.to_networkx()\n",
    "    #n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    \n",
    "    d = community.label_propagation_communities(G2)\n",
    "    \n",
    "\n",
    "    data = []\n",
    "    for i in range(len(d)):\n",
    "        data.append(torch.Tensor(list(list(d)[i])).to(torch.int64))\n",
    "    \n",
    "    com_dict = {}\n",
    "    for i in range(len(data)):\n",
    "        for el in data[i]:\n",
    "            com_dict[el.item()] = i\n",
    "    ordered_com_dict = collections.OrderedDict(sorted(com_dict.items()))\n",
    "    \n",
    "    com = list(ordered_com_dict.values())\n",
    "    dict2 = {}\n",
    "    dict2['author'] = com[0:4057]\n",
    "    dict2['conf'] = com[4057:4077]\n",
    "    dict2['paper'] = com[4077:18405]\n",
    "    dict2['term'] = com[18405:26128]\n",
    "    with open('com_DBLP.txt', 'w') as convert_file:\n",
    "        convert_file.write(json.dumps(dict2))\n",
    "    name = 'com_DBLP.txt'\n",
    "    return name\n",
    "\n",
    "def get_com_Leiden_DBLP(hg):\n",
    "    \n",
    "    hg1 = copy.deepcopy(hg)\n",
    "    #transform = AddMetaPaths({\n",
    "    #'apa': [('author', 'ap', 'paper'),('paper', 'pa', 'author')],\n",
    "    #'pap': [('paper', 'pa', 'author'),('author','ap','paper')],\n",
    "    #'ptp': [('paper', 'pt', 'term'),('term','tp','paper')],\n",
    "    #'tpt': [('term', 'tp', 'paper'),('paper','pt','term')],\n",
    "    #'pcp': [('paper', 'pc', 'conf'),('conf','cp','paper')],\n",
    "    #'cpc': [('conf', 'cp', 'paper'),('paper','pc','conf')]\n",
    "    #})\n",
    "    #hg2 = transform(hg1)\n",
    "    g = dgl.to_homogeneous(hg1)\n",
    "    nxg = g.to_networkx()\n",
    "    #n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    \n",
    "    d = algorithms.leiden(G2).communities\n",
    "    \n",
    "\n",
    "    data = []\n",
    "    for i in range(len(d)):\n",
    "        data.append(torch.Tensor(list(list(d)[i])).to(torch.int64))\n",
    "    \n",
    "    com_dict = {}\n",
    "    for i in range(len(data)):\n",
    "        for el in data[i]:\n",
    "            com_dict[el.item()] = i\n",
    "    ordered_com_dict = collections.OrderedDict(sorted(com_dict.items()))\n",
    "    \n",
    "    com = list(ordered_com_dict.values())\n",
    "    dict2 = {}\n",
    "    dict2['author'] = com[0:4057]\n",
    "    dict2['conf'] = com[4057:4077]\n",
    "    dict2['paper'] = com[4077:18405]\n",
    "    dict2['term'] = com[18405:26128]\n",
    "    with open('com_DBLP.txt', 'w') as convert_file:\n",
    "        convert_file.write(json.dumps(dict2))\n",
    "    name = 'com_DBLP.txt'\n",
    "    return name\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eb15d6",
   "metadata": {},
   "source": [
    "###### IMDB COMMUNITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5e4aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_com_LPA_IMDb(hg):\n",
    "    hg1 = copy.deepcopy(hg)\n",
    "    #find all actor of a film\n",
    "    #transform = AddMetaPaths({\n",
    "    #'mam': [('movie', 'ma', 'actor'),('actor', 'am', 'movie')],\n",
    "    #'ama': [('actor', 'am', 'movie'),('movie','ma','actor')],\n",
    "    #'dmd': [('director', 'dm', 'movie'),('movie','md','director')],\n",
    "    #'mdm': [('movie', 'md', 'director'),('director','dm','movie')]\n",
    "    #})\n",
    "    #hg2 = transform(hg1)\n",
    "    g = dgl.to_homogeneous(hg1)\n",
    "    nxg = g.to_networkx()\n",
    "    #n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    \n",
    "    d = community.label_propagation_communities(G2)\n",
    "    \n",
    "\n",
    "    data = []\n",
    "    for i in range(len(d)):\n",
    "        data.append(torch.Tensor(list(list(d)[i])).to(torch.int64))\n",
    "    \n",
    "    com_dict = {}\n",
    "    for i in range(len(data)):\n",
    "        for el in data[i]:\n",
    "            com_dict[el.item()] = i\n",
    "    ordered_com_dict = collections.OrderedDict(sorted(com_dict.items()))\n",
    "    \n",
    "    com = list(ordered_com_dict.values())\n",
    "    dict2 = {}\n",
    "    dict2['actor'] = com[0:5257]\n",
    "    dict2['director'] = com[5257:7338]\n",
    "    dict2['movie'] = com[7338:11616]\n",
    "    with open('com_IMDb.txt', 'w') as convert_file:\n",
    "        convert_file.write(json.dumps(dict2))\n",
    "    name = 'com_IMDb.txt'\n",
    "    return name\n",
    "\n",
    "\n",
    "def get_com_Louvain_IMDb(hg):\n",
    "    hg1 = copy.deepcopy(hg)\n",
    "    #find all actor of a film\n",
    "    transform = AddMetaPaths({\n",
    "    #'mam': [('movie', 'ma', 'actor'),('actor', 'am', 'movie')],\n",
    "    #'ama': [('actor', 'am', 'movie'),('movie','ma','actor')],\n",
    "    #'dmd': [('director', 'dm', 'movie'),('movie','md','director')],\n",
    "    'mdm': [('movie', 'md', 'director'),('director','dm','movie')]\n",
    "    })\n",
    "    hg2 = transform(hg1)\n",
    "    g = dgl.to_homogeneous(hg2)\n",
    "    nxg = g.to_networkx()\n",
    "    #n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    \n",
    "    d = community.louvain_communities(G2)\n",
    "    \n",
    "\n",
    "    data = []\n",
    "    for i in range(len(d)):\n",
    "        data.append(torch.Tensor(list(list(d)[i])).to(torch.int64))\n",
    "    \n",
    "    com_dict = {}\n",
    "    for i in range(len(data)):\n",
    "        for el in data[i]:\n",
    "            com_dict[el.item()] = i\n",
    "    ordered_com_dict = collections.OrderedDict(sorted(com_dict.items()))\n",
    "    \n",
    "    com = list(ordered_com_dict.values())\n",
    "    dict2 = {}\n",
    "    dict2['actor'] = com[0:5257]\n",
    "    dict2['director'] = com[5257:7338]\n",
    "    dict2['movie'] = com[7338:11616]\n",
    "    with open('com_IMDb.txt', 'w') as convert_file:\n",
    "        convert_file.write(json.dumps(dict2))\n",
    "    name = 'com_IMDb.txt'\n",
    "    return name\n",
    "\n",
    "\n",
    "def get_com_Leiden_IMDb(hg):\n",
    "    hg1 = copy.deepcopy(hg)\n",
    "    #find all actor of a film\n",
    "    transform = AddMetaPaths({\n",
    "    #'mam': [('movie', 'ma', 'actor'),('actor', 'am', 'movie')],\n",
    "    #'ama': [('actor', 'am', 'movie'),('movie','ma','actor')],\n",
    "    #'dmd': [('director', 'dm', 'movie'),('movie','md','director')],\n",
    "    'mdm': [('movie', 'md', 'director'),('director','dm','movie')]\n",
    "    })\n",
    "    hg2 = transform(hg1)\n",
    "    g = dgl.to_homogeneous(hg2)\n",
    "    nxg = g.to_networkx()\n",
    "    #n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    \n",
    "    d = algorithms.leiden(G2).communities\n",
    "    \n",
    "\n",
    "    data = []\n",
    "    for i in range(len(d)):\n",
    "        data.append(torch.Tensor(list(list(d)[i])).to(torch.int64))\n",
    "    \n",
    "    com_dict = {}\n",
    "    for i in range(len(data)):\n",
    "        for el in data[i]:\n",
    "            com_dict[el.item()] = i\n",
    "    ordered_com_dict = collections.OrderedDict(sorted(com_dict.items()))\n",
    "    \n",
    "    com = list(ordered_com_dict.values())\n",
    "    dict2 = {}\n",
    "    dict2['actor'] = com[0:5257]\n",
    "    dict2['director'] = com[5257:7338]\n",
    "    dict2['movie'] = com[7338:11616]\n",
    "    with open('com_IMDb.txt', 'w') as convert_file:\n",
    "        convert_file.write(json.dumps(dict2))\n",
    "    name = 'com_IMDb.txt'\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45d34c3",
   "metadata": {},
   "source": [
    "###### ACM COMMUNITIES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe921b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_com_LPA_ACM(hg):\n",
    "    \n",
    "    hg1 = copy.deepcopy(hg)\n",
    "    #transform = AddMetaPaths({\n",
    "    #'pap': [('paper', 'pa', 'author'),('author', 'ap', 'paper')],\n",
    "    #'pfp': [('paper', 'pf', 'field'),('field','fp','paper')],\n",
    "    #'apa': [('author', 'ap', 'paper'),('paper','pa','author')],\n",
    "    #'fpf': [('field', 'fp', 'paper'),('paper','pf','field')]\n",
    "    \n",
    "    #})\n",
    "    #new_g1 = transform(hg1)\n",
    "    g = dgl.to_homogeneous(hg1)\n",
    "    nxg = g.to_networkx()\n",
    "    n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(n)\n",
    "    d = community.label_propagation_communities(G2)\n",
    "    \n",
    "\n",
    "    data = []\n",
    "    for i in range(len(d)):\n",
    "        data.append(torch.Tensor(list(list(d)[i])).to(torch.int64))\n",
    "    authors = {}\n",
    "    papers = {}\n",
    "    fields = {}\n",
    "\n",
    "    a,p,f = 0, 0, 0\n",
    "    \n",
    "    com_dict = {}\n",
    "    for i in range(len(data)):\n",
    "        for el in data[i]:\n",
    "            com_dict[el.item()] = i\n",
    "    ordered_com_dict = collections.OrderedDict(sorted(com_dict.items()))\n",
    "    \n",
    "    com = list(ordered_com_dict.values())\n",
    "    dict2 = {}\n",
    "    dict2['author'] = com[0:17351]\n",
    "    dict2['field'] = com[17351:17423]\n",
    "    dict2['paper'] = com[17423:21449]\n",
    "    with open('com_Louvain_ACM_PAP.txt', 'w') as convert_file:\n",
    "        convert_file.write(json.dumps(dict2))\n",
    "    name = 'com_Louvain_ACM_PAP.txt'\n",
    "    return name\n",
    "    \n",
    "    \n",
    "\n",
    "def get_com_Louvain_ACM(hg):\n",
    "    hg1 = copy.deepcopy(hg)\n",
    "    transform = AddMetaPaths({\n",
    "    'pap': [('paper', 'pa', 'author'),('author', 'ap', 'paper')],\n",
    "    'pfp': [('paper', 'pf', 'field'),('field','fp','paper')],\n",
    "    #'apa': [('author', 'ap', 'paper'),('paper','pa','author')],\n",
    "    #'fpf': [('field', 'fp', 'paper'),('paper','pf','field')]\n",
    "    })\n",
    "    new_g1 = transform(hg)\n",
    "    g = dgl.to_homogeneous(hg1)\n",
    "    nxg = g.to_networkx()\n",
    "    n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(n)\n",
    "    d = community.louvain_communities(G2)\n",
    "    ids = nx.get_node_attributes(nxg, \"_TYPE\")\n",
    "\n",
    "    data = []\n",
    "    for i in range(len(d)):\n",
    "        data.append(torch.Tensor(list(list(d)[i])).to(torch.int64))\n",
    "    authors = {}\n",
    "    papers = {}\n",
    "    fields = {}\n",
    "\n",
    "    a,p,f = 0, 0, 0\n",
    "    \n",
    "    com_dict = {}\n",
    "    for i in range(len(data)):\n",
    "        for el in data[i]:\n",
    "            com_dict[el.item()] = i\n",
    "    ordered_com_dict = collections.OrderedDict(sorted(com_dict.items()))\n",
    "    \n",
    "    com = list(ordered_com_dict.values())\n",
    "    dict2 = {}\n",
    "    dict2['author'] = com[0:17351]\n",
    "    dict2['field'] = com[17351:17423]\n",
    "    dict2['paper'] = com[17423:21449]\n",
    "    with open('com_Louvain_ACM.txt', 'w') as convert_file:\n",
    "        convert_file.write(json.dumps(dict2))\n",
    "    name = 'com_Louvain_ACM.txt'\n",
    "    return name\n",
    "\n",
    "\n",
    "def get_com_Leiden_ACM(hg):\n",
    "    hg1 = copy.deepcopy(hg)\n",
    "    #transform = AddMetaPaths({\n",
    "    #'pap': [('paper', 'pa', 'author'),('author', 'ap', 'paper')],\n",
    "    #'pfp': [('paper', 'pf', 'field'),('field','fp','paper')],\n",
    "    #'apa': [('author', 'ap', 'paper'),('paper','pa','author')],\n",
    "    #'fpf': [('field', 'fp', 'paper'),('paper','pf','field')]\n",
    "    \n",
    "    #})\n",
    "    #new_g1 = transform(hg)\n",
    "    g = dgl.to_homogeneous(hg1)\n",
    "    nxg = g.to_networkx()\n",
    "    n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(n)\n",
    "    d = algorithms.leiden(G2).communities\n",
    "\n",
    "    data = []\n",
    "    for i in range(len(d)):\n",
    "        data.append(torch.Tensor(list(list(d)[i])).to(torch.int64))\n",
    "    authors = {}\n",
    "    papers = {}\n",
    "    fields = {}\n",
    "\n",
    "    a,p,f = 0, 0, 0\n",
    "    \n",
    "    com_dict = {}\n",
    "    for i in range(len(data)):\n",
    "        for el in data[i]:\n",
    "            com_dict[el.item()] = i\n",
    "    ordered_com_dict = collections.OrderedDict(sorted(com_dict.items()))\n",
    "    \n",
    "    com = list(ordered_com_dict.values())\n",
    "    dict2 = {}\n",
    "    dict2['author'] = com[0:17351]\n",
    "    dict2['field'] = com[17351:17423]\n",
    "    dict2['paper'] = com[17423:21449]\n",
    "    with open('com_Louvain_ACM_PAP.txt', 'w') as convert_file:\n",
    "        convert_file.write(json.dumps(dict2))\n",
    "    name = 'com_Louvain_ACM_PAP.txt'\n",
    "    return name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
