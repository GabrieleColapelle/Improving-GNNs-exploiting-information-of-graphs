{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcec58b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import itertools\n",
    "\n",
    "\n",
    "import dgl\n",
    "from dgl.nn import HeteroGraphConv\n",
    "from dgl.utils import extract_node_subframes, set_new_frames\n",
    "\n",
    "from sklearn.metrics import f1_score, normalized_mutual_info_score, adjusted_rand_score\n",
    "\n",
    "import copy\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "import json\n",
    "import ast\n",
    "import collections\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213b3850",
   "metadata": {},
   "source": [
    "The basic DGL dataset for creating graph datasets. This class defines a basic template class for DGL Dataset. The following steps will be executed automatically:\n",
    "\n",
    "Check whether there is a dataset cache on disk (already processed and stored on the disk) by invoking has_cache(). If true, goto 5.\n",
    "\n",
    "1-Call download() to download the data if url is not None.\n",
    "\n",
    "2-Call process() to process the data.\n",
    "\n",
    "3-Call save() to save the processed dataset on disk and goto 6.\n",
    "\n",
    "4-Call load() to load the processed dataset from disk.\n",
    "\n",
    "Done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0016dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph(name):\n",
    "    DATASET = {\n",
    "        'acm': ACMDataset(),\n",
    "        'imdb': IMDbDataset(),\n",
    "        'dblp': DBLPFourAreaDataset()\n",
    "    }\n",
    "\n",
    "    set_random_seed(1)\n",
    "    data = DATASET[name]\n",
    "    g = data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce63e72",
   "metadata": {},
   "source": [
    "#### ADD FEATURES TO NODES FOR ACM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dd3125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feat(hg,papers,authors,fields,feat):\n",
    "    \n",
    "    papers_dict, authors_dict, fields_dict = check_lenght_acm(papers,authors,fields)\n",
    "    types = ['paper','author','field'] \n",
    "    for ntype in types:\n",
    "        if ntype == 'paper' : \n",
    "            #print(ntype)\n",
    "            #print(movies_dict)\n",
    "            x = torch.empty(size=(hg.number_of_nodes(ntype), 1904))\n",
    "            for i in range(len(papers_dict)):\n",
    "                m1 = torch.tensor([papers_dict[i]])\n",
    "                tensor = torch.cat((hg.nodes[ntype].data['feat'][i],m1))\n",
    "                x[i] = tensor\n",
    "            hg.nodes[ntype].data[feat] = x\n",
    "        elif ntype == 'author':\n",
    "            #print(ntype)\n",
    "            #print(actors_dict)\n",
    "            x = torch.empty(size= (hg.number_of_nodes(ntype), 17))\n",
    "            for i in range(len(authors_dict)):  \n",
    "                a1 = torch.tensor([authors_dict[i]])\n",
    "                #print(a1)\n",
    "                tensor = torch.cat((hg.nodes[ntype].data['feat'][i],a1))\n",
    "                x[i] = tensor\n",
    "            hg.nodes[ntype].data[feat] = x\n",
    "        elif ntype == 'field':\n",
    "            #print(ntype)\n",
    "            x = torch.empty(size= (hg.number_of_nodes(ntype), 73))\n",
    "            for i in range(len(fields_dict)):\n",
    "                d1 = torch.tensor([fields_dict[i]])\n",
    "                #print(hg.nodes[ntype].data['feat'][i])\n",
    "                #print(ntype)\n",
    "                #print(i)\n",
    "                #print(d1)\n",
    "                tensor = torch.cat((hg.nodes[ntype].data['feat'][i],d1))\n",
    "                x[i] = tensor\n",
    "            hg.nodes[ntype].data[feat] = x\n",
    "    return hg\n",
    "\n",
    "def add_feat2(hg,papers,authors,fields,feat):\n",
    "    \n",
    "    papers_dict, authors_dict, fields_dict = check_lenght_acm(papers,authors,fields)\n",
    "    types = ['paper','author','field'] \n",
    "    for ntype in types:\n",
    "        if ntype == 'paper' : \n",
    "            #print(ntype)\n",
    "            #print(movies_dict)\n",
    "            x = torch.empty(size=(hg.number_of_nodes(ntype), 1905))\n",
    "            for i in range(len(papers_dict)):\n",
    "                m1 = torch.tensor([papers_dict[i]])\n",
    "                tensor = torch.cat((hg.nodes[ntype].data['lcc'][i],m1))\n",
    "                x[i] = tensor\n",
    "            hg.nodes[ntype].data[feat] = x\n",
    "        elif ntype == 'author':\n",
    "            #print(ntype)\n",
    "            #print(actors_dict)\n",
    "            x = torch.empty(size= (hg.number_of_nodes(ntype), 18))\n",
    "            for i in range(len(authors_dict)):  \n",
    "                a1 = torch.tensor([authors_dict[i]])\n",
    "                #print(a1)\n",
    "                tensor = torch.cat((hg.nodes[ntype].data['lcc'][i],a1))\n",
    "                x[i] = tensor\n",
    "            hg.nodes[ntype].data[feat] = x\n",
    "        elif ntype == 'field':\n",
    "            #print(ntype)\n",
    "            x = torch.empty(size= (hg.number_of_nodes(ntype), 74))\n",
    "            for i in range(len(fields_dict)):\n",
    "                d1 = torch.tensor([fields_dict[i]])\n",
    "                #print(hg.nodes[ntype].data['feat'][i])\n",
    "                #print(ntype)\n",
    "                #print(i)\n",
    "                #print(d1)\n",
    "                tensor = torch.cat((hg.nodes[ntype].data['lcc'][i],d1))\n",
    "                x[i] = tensor\n",
    "            hg.nodes[ntype].data[feat] = x\n",
    "    return hg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766c333a",
   "metadata": {},
   "source": [
    "#### UTILS FOR ACM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9879bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_degree_acm(g):\n",
    "    author_deg = []\n",
    "    paper_deg = []\n",
    "    field_deg = []\n",
    "    allow_zero_in_degree = True\n",
    "    for n in g.ntypes:\n",
    "        if n == 'author' :\n",
    "            types = ['pa']\n",
    "            for i in g.nodes(n):\n",
    "                deg = 0\n",
    "                for e in types:\n",
    "                    deg = g.in_degrees(i.item(),etype=e)\n",
    "                author_deg.append(deg)\n",
    "        elif n == 'paper':\n",
    "            types = ['ap','fp']\n",
    "            for i in g.nodes(n):\n",
    "                deg = 0\n",
    "                for e in types:\n",
    "                    deg += g.in_degrees(i.item(),etype=e)\n",
    "                paper_deg.append(deg)\n",
    "        elif n == 'field':\n",
    "            types = ['pf']\n",
    "            for i in g.nodes(n):\n",
    "                deg = 0\n",
    "                for e in types:\n",
    "                    deg = g.in_degrees(i.item(),etype=e)\n",
    "                field_deg.append(deg)\n",
    "                \n",
    "                \n",
    "    return author_deg, paper_deg, field_deg\n",
    "\n",
    "def add_degree_feat_acm(g):\n",
    "    a,p,f = get_degree_acm(g)\n",
    "    types = ['author','paper','field'] \n",
    "    for ntype in types:\n",
    "        if ntype == 'author' :\n",
    "            x = torch.empty(size=(g.number_of_nodes(ntype), 19))\n",
    "            for i in range(g.num_nodes(ntype)):\n",
    "                a1 = torch.tensor([a[i]])\n",
    "                tensor = torch.cat((g.nodes[ntype].data['triang_deg_PFP'][i],a1))\n",
    "                x[i] = tensor\n",
    "            g.nodes[ntype].data['feat_deg'] = x\n",
    "        elif ntype == 'paper':\n",
    "            x = torch.empty(size= (g.number_of_nodes(ntype), 1906))\n",
    "            for i in range(g.num_nodes(ntype)):\n",
    "                p1 = torch.tensor([p[i]])\n",
    "                tensor = torch.cat((g.nodes[ntype].data['triang_deg_PFP'][i],p1))\n",
    "                x[i] = tensor\n",
    "            g.nodes[ntype].data['feat_deg'] = x\n",
    "        elif ntype == 'field':\n",
    "            x = torch.empty(size= (g.number_of_nodes(ntype), 75))\n",
    "            for i in range(g.num_nodes(ntype)):\n",
    "                f1 = torch.tensor([f[i]])\n",
    "                tensor = torch.cat((g.nodes[ntype].data['triang_deg_PFP'][i],f1))\n",
    "                x[i] = tensor\n",
    "            g.nodes[ntype].data['feat_deg'] = x\n",
    "    return g\n",
    "\n",
    "\n",
    "def add_triangle_count_author_acm(hg):\n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    #find all actor of a film\n",
    "    for paper in range(hg.number_of_nodes('paper')):\n",
    "        succ = hg.successors(paper, etype='pa')\n",
    "        #successors contains all author of a paper\n",
    "        for i in range(len(succ)):\n",
    "            if i + 1 != len(succ):\n",
    "                hg2.add_edges(torch.tensor([succ[i]]), torch.tensor([succ[i+1]]), etype='aa')\n",
    "            else:\n",
    "                hg2.add_edges(torch.tensor([succ[i]]), torch.tensor([succ[0]]), etype='aa')\n",
    "    g = dgl.to_homogeneous(hg2, store_type=True)\n",
    "    nxg = g.to_networkx(node_attrs=['_ID', '_TYPE'], edge_attrs=['_ID', '_TYPE'])\n",
    "    n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(n)\n",
    "    d = nx.triangles(G2)\n",
    "    ids = nx.get_node_attributes(nxg, \"_TYPE\")\n",
    "    papers = {}\n",
    "    authors = {}\n",
    "    fields = {}\n",
    "\n",
    "    id_m = 0\n",
    "    id_a = 0\n",
    "    id_d = 0\n",
    "\n",
    "    for el in ids:\n",
    "        if ids[el] == 0:\n",
    "            papers[id_m] = d[el]\n",
    "            id_m += 1\n",
    "        if ids[el ]== 1:\n",
    "            authors[id_a] = d[el]\n",
    "            id_a += 1\n",
    "        if ids[el] == 2:\n",
    "            fields[id_d] = d[el]\n",
    "            id_d += 1\n",
    "\n",
    "    hg = add_feat(hg,papers,authors,fields,\"triang_deg_authors\")\n",
    "    return hg\n",
    "\n",
    "\n",
    "\n",
    "def add_triangle_count_PAP_acm(hg):\n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    #find all actor of a film\n",
    "    for paper in range(hg.number_of_nodes('paper')):\n",
    "        succ = hg.successors(paper, etype='pa')\n",
    "        #successors contains all actor of a movie\n",
    "        for i in range(len(succ)):\n",
    "            #get the first actor\n",
    "            author = succ[i]\n",
    "            #get the paper connected to author\n",
    "            papers = hg.successors(author, etype='ap')\n",
    "            for e in range(len(papers)):\n",
    "                #add edge between the two movies\n",
    "                hg2.add_edges(torch.tensor(papers[e]), paper, etype='pp')\n",
    "    g = dgl.to_homogeneous(hg2, store_type=True)\n",
    "    nxg = g.to_networkx(node_attrs=['_ID', '_TYPE'], edge_attrs=['_ID', '_TYPE'])\n",
    "    n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(n)\n",
    "    d = nx.triangles(G2)\n",
    "    ids = nx.get_node_attributes(nxg, \"_TYPE\")\n",
    "    papers = {}\n",
    "    authors = {}\n",
    "    fields = {}\n",
    "\n",
    "    id_p = 0\n",
    "    id_a = 0\n",
    "    id_f = 0\n",
    "\n",
    "    for el in ids:\n",
    "        if ids[el] == 0:\n",
    "            papers[id_p] = d[el]\n",
    "            id_p += 1\n",
    "        if ids[el ]== 1:\n",
    "            authors[id_a] = d[el]\n",
    "            id_a += 1\n",
    "        if ids[el] == 2:\n",
    "            fields[id_f] = d[el]\n",
    "            id_f += 1\n",
    "\n",
    "    hg = add_feat(hg,papers,authors,fields,\"triang_deg_PAP\")\n",
    "    return hg\n",
    "\n",
    "\n",
    "def add_triangle_count_PFP_acm(hg):\n",
    "    field_ex = []\n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    for paper in range(hg.number_of_nodes('paper')):\n",
    "        print(paper)\n",
    "        #field associated to a paper\n",
    "        succ = hg.successors(paper, etype='pf')\n",
    "        print(succ)\n",
    "        papers = hg.successors(succ, etype='fp')\n",
    "        if succ in field_ex:\n",
    "            print('già inserito')\n",
    "        else:\n",
    "            for e in range(len(papers)):\n",
    "                #add edge between the two paper\n",
    "                hg2.add_edges(torch.tensor(papers[e]), paper, etype='pp')\n",
    "                field_ex.append(succ)\n",
    "    g = dgl.to_homogeneous(hg2, store_type=True)\n",
    "    nxg = g.to_networkx(node_attrs=['_ID', '_TYPE'], edge_attrs=['_ID', '_TYPE'])\n",
    "    n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(n)\n",
    "    d = nx.triangles(G2)\n",
    "    ids = nx.get_node_attributes(nxg, \"_TYPE\")\n",
    "    papers = {}\n",
    "    authors = {}\n",
    "    fields = {}\n",
    "\n",
    "    id_p = 0\n",
    "    id_a = 0\n",
    "    id_f = 0\n",
    "\n",
    "    for el in ids:\n",
    "        if ids[el] == 0:\n",
    "            papers[id_p] = d[el]\n",
    "            id_p += 1\n",
    "        if ids[el ]== 1:\n",
    "            authors[id_a] = d[el]\n",
    "            id_a += 1\n",
    "        if ids[el] == 2:\n",
    "            fields[id_f] = d[el]\n",
    "            id_f += 1\n",
    "\n",
    "    hg = add_feat2(hg,papers,authors,fields,\"triang_deg_PFP\")\n",
    "    return hg\n",
    "\n",
    "\n",
    "def add_pageRank_acm(hg):\n",
    "    g = dgl.to_homogeneous(hg, store_type=True)\n",
    "    nxg = g.to_networkx(node_attrs=['_ID', '_TYPE'], edge_attrs=['_ID', '_TYPE'])\n",
    "    n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(n)\n",
    "    pr = nx.pagerank(G2, alpha=0.9)\n",
    "    print(pr)\n",
    "    \n",
    "    ids = nx.get_node_attributes(nxg, \"_TYPE\")\n",
    "    papers = {}\n",
    "    authors = {}\n",
    "    fields = {}\n",
    "\n",
    "    id_p = 0\n",
    "    id_a = 0\n",
    "    id_f = 0\n",
    "\n",
    "    for el in ids:\n",
    "        if ids[el] == 0:\n",
    "            papers[id_p] = pr[el]\n",
    "            id_p += 1\n",
    "        if ids[el ] == 1:\n",
    "            authors[id_a] = pr[el]\n",
    "            id_a += 1\n",
    "        if ids[el] == 2:\n",
    "            fields[id_f] = pr[el]\n",
    "            id_f += 1\n",
    "            \n",
    "    hg = add_feat2(hg,papers,authors,fields,\"page_rank\")\n",
    "    return hg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_LocalClusteringCoeff_acm(hg):\n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    field_ex = []\n",
    "    for paper in range(hg.number_of_nodes('paper')):\n",
    "        #field associated to a paper\n",
    "        succ = hg.successors(paper, etype='pf')\n",
    "        print(succ)\n",
    "        papers = hg.successors(succ, etype='fp')\n",
    "        if succ in field_ex:\n",
    "            print(str(succ) + \" gia inserito \")\n",
    "        else:\n",
    "            for e in range(len(papers)):\n",
    "                #add edge between the two paper\n",
    "                hg2.add_edges(torch.tensor(papers[e]), paper, etype='pp')\n",
    "                field_ex.append(succ)\n",
    "    for paper in range(hg.number_of_nodes('paper')):\n",
    "        succ = hg.successors(paper, etype='pa')\n",
    "        #successors contains all author of a paper\n",
    "        for i in range(len(succ)):\n",
    "            if i + 1 != len(succ):\n",
    "                hg2.add_edges(torch.tensor([succ[i]]), torch.tensor([succ[i+1]]), etype='aa')\n",
    "            else:\n",
    "                hg2.add_edges(torch.tensor([succ[i]]), torch.tensor([succ[0]]), etype='aa')\n",
    "    for paper in range(hg.number_of_nodes('paper')):\n",
    "        succ = hg.successors(paper, etype='pa')\n",
    "        #successors contains all actor of a movie\n",
    "        for i in range(len(succ)):\n",
    "            #get the first actor\n",
    "            author = succ[i]\n",
    "            #get the paper connected to author\n",
    "            papers = hg.successors(author, etype='ap')\n",
    "            for e in range(len(papers)):\n",
    "                #add edge between the two movies\n",
    "                hg2.add_edges(torch.tensor(papers[e]), paper, etype='pp')\n",
    "    \n",
    "    g = dgl.to_homogeneous(hg2, store_type=True)\n",
    "    nxg = g.to_networkx(node_attrs=['_ID', '_TYPE'], edge_attrs=['_ID', '_TYPE'])\n",
    "    n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(n)\n",
    "    d = nx.triangles(G2)\n",
    "    ids = nx.get_node_attributes(nxg, \"_TYPE\")\n",
    "    papers = {}\n",
    "    authors = {}\n",
    "    fields = {}\n",
    "\n",
    "    id_p = 0\n",
    "    id_a = 0\n",
    "    id_f = 0\n",
    "\n",
    "    for el in ids:\n",
    "        if ids[el] == 0:\n",
    "            papers[id_p] = d[el]\n",
    "            id_p += 1\n",
    "        if ids[el ]== 1:\n",
    "            authors[id_a] = d[el]\n",
    "            id_a += 1\n",
    "        if ids[el] == 2:\n",
    "            fields[id_f] = d[el]\n",
    "            id_f += 1\n",
    "\n",
    "    papers_dict, authors_dict, fields_dict = check_lenght_acm(papers,authors,fields)\n",
    "\n",
    "    hg = add_feat(hg,papers,authors,fields,\"lcc\")\n",
    "    return hg\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e010c30",
   "metadata": {},
   "source": [
    "#### ADD FEATURES TO NODES FOR IMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23f8256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feat_imdb(hg,movies,actors,directors,feat):\n",
    "\n",
    "    movies_dict, actors_dict, directors_dict = check_lenght_imdb(movies,actors,directors)\n",
    "\n",
    "    types = ['movie','actor','director'] \n",
    "    for ntype in types:\n",
    "        if ntype == 'movie' : \n",
    "            x = torch.empty(size=(hg.number_of_nodes(ntype), 1300))\n",
    "            for i in range(len(movies_dict)):\n",
    "                m1 = torch.tensor([movies_dict[i]])\n",
    "                tensor = torch.cat((hg.nodes[ntype].data['feat'][i],m1))\n",
    "                x[i] = tensor\n",
    "            hg.nodes[ntype].data[feat] = x\n",
    "        elif ntype == 'actor':\n",
    "            x = torch.empty(size= (hg.number_of_nodes(ntype), 17))\n",
    "            for i in range(len(actors_dict)):  \n",
    "                a1 = torch.tensor([actors_dict[i]])\n",
    "                tensor = torch.cat((hg.nodes[ntype].data['feat'][i],a1))\n",
    "                x[i] = tensor\n",
    "            hg.nodes[ntype].data[feat] = x\n",
    "        elif ntype == 'director':\n",
    "            x = torch.empty(size= (hg.number_of_nodes(ntype), 17))\n",
    "            for i in range(len(directors_dict)):\n",
    "                d1 = torch.tensor([directors_dict[i]])\n",
    "                tensor = torch.cat((hg.nodes[ntype].data['feat'][i],d1))\n",
    "                x[i] = tensor\n",
    "            hg.nodes[ntype].data[feat] = x\n",
    "    return hg\n",
    "\n",
    "\n",
    "def add_feat_imdb2(hg,movies,actors,directors,feat):\n",
    "\n",
    "    movies_dict, actors_dict, directors_dict = check_lenght_imdb(movies,actors,directors)\n",
    "\n",
    "    types = ['movie','actor','director'] \n",
    "    for ntype in types:\n",
    "        if ntype == 'movie' : \n",
    "            x = torch.empty(size=(hg.number_of_nodes(ntype), 1301))\n",
    "            for i in range(len(movies_dict)):\n",
    "                m1 = torch.tensor([movies_dict[i]])\n",
    "                tensor = torch.cat((hg.nodes[ntype].data['page_rank'][i],m1))\n",
    "                x[i] = tensor\n",
    "            hg.nodes[ntype].data[feat] = x\n",
    "        elif ntype == 'actor':\n",
    "            x = torch.empty(size= (hg.number_of_nodes(ntype), 18))\n",
    "            for i in range(len(actors_dict)):  \n",
    "                a1 = torch.tensor([actors_dict[i]])\n",
    "                tensor = torch.cat((hg.nodes[ntype].data['page_rank'][i],a1))\n",
    "                x[i] = tensor\n",
    "            hg.nodes[ntype].data[feat] = x\n",
    "        elif ntype == 'director':\n",
    "            x = torch.empty(size= (hg.number_of_nodes(ntype), 18))\n",
    "            for i in range(len(directors_dict)):\n",
    "                d1 = torch.tensor([directors_dict[i]])\n",
    "                tensor = torch.cat((hg.nodes[ntype].data['page_rank'][i],d1))\n",
    "                x[i] = tensor\n",
    "            hg.nodes[ntype].data[feat] = x\n",
    "    return hg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe98862",
   "metadata": {},
   "source": [
    "#### UTILS FOR IMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "282e73d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_degree_imdb(g):\n",
    "    actors_deg = []\n",
    "    movies_deg = []\n",
    "    directors_deg = []\n",
    "   \n",
    "    allow_zero_in_degree = True\n",
    "    for n in g.ntypes:\n",
    "        print(\"tipo nodo \" + n)\n",
    "        if n == 'actor' :\n",
    "            types = ['ma']\n",
    "            for i in g.nodes(n):\n",
    "                deg = 0\n",
    "                for e in types:\n",
    "                    deg = g.in_degrees(i.item(),etype=e)\n",
    "                actors_deg.append(deg)\n",
    "        elif n == 'movie':\n",
    "            types = ['am','dm']\n",
    "            for i in g.nodes(n):\n",
    "                deg = 0 \n",
    "                for e in types:\n",
    "                    deg = deg + g.in_degrees(i.item(),etype=e)\n",
    "                movies_deg.append(deg)       \n",
    "        elif n == 'director':\n",
    "            types = ['md']\n",
    "            for i in g.nodes(n):\n",
    "                deg = 0 \n",
    "                for e in types:\n",
    "                    deg = g.in_degrees(i.item(),etype=e)\n",
    "                directors_deg.append(deg)\n",
    "                \n",
    "                \n",
    "    return  actors_deg, movies_deg, directors_deg\n",
    "\n",
    "def add_degree_feat_imdb(g):\n",
    "    a,m,d = get_degree_imdb(g)\n",
    "    types = ['actor','movie','director'] \n",
    "    for ntype in types:\n",
    "        if ntype == 'actor' :\n",
    "            x = torch.empty(size=(g.number_of_nodes(ntype), 17))\n",
    "            for i in range(g.num_nodes(ntype)):\n",
    "                a1 = torch.tensor([a[i]])\n",
    "                tensor = torch.cat((g.nodes[ntype].data['feat'][i],a1))\n",
    "                x[i] = tensor\n",
    "            g.nodes[ntype].data['feat_deg'] = x\n",
    "        elif ntype == 'movie':\n",
    "            x = torch.empty(size= (g.number_of_nodes(ntype), 1300))\n",
    "            for i in range(g.num_nodes(ntype)):\n",
    "                m1 = torch.tensor([m[i]])\n",
    "                tensor = torch.cat((g.nodes[ntype].data['feat'][i],m1))\n",
    "                x[i] = tensor\n",
    "            g.nodes[ntype].data['feat_deg'] = x\n",
    "        elif ntype == 'director':\n",
    "            x = torch.empty(size= (g.number_of_nodes(ntype), 17))\n",
    "            for i in range(g.num_nodes(ntype)):\n",
    "                d1 = torch.tensor([d[i]])\n",
    "                tensor = torch.cat((g.nodes[ntype].data['feat'][i],d1))\n",
    "                x[i] = tensor\n",
    "            g.nodes[ntype].data['feat_deg'] = x\n",
    "    return g\n",
    "\n",
    "def add_triangle_count_actor_imdb(hg):\n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    #find all actor of a film\n",
    "    for movie in range(hg.number_of_nodes('movie')):\n",
    "        succ = hg.successors(movie, etype='ma')\n",
    "        #successors contains all actor of a movie\n",
    "        res = [(a, b) for idx, a in enumerate(succ) for b in succ[idx + 1:]]\n",
    "        for i in range(len(res)):\n",
    "            if len(res) != 0:\n",
    "                hg2.add_edges(torch.tensor(res[i][0]), torch.tensor([res[i][1]]), etype='aa')\n",
    "                print(\"arco tra\"+ str(res[i][0]) + \" \" + str(res[i][1]))\n",
    "    g = dgl.to_homogeneous(hg2, store_type=True)\n",
    "    nxg = g.to_networkx(node_attrs=['_ID', '_TYPE'], edge_attrs=['_ID', '_TYPE'])\n",
    "    n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(n)\n",
    "    d = nx.triangles(G2)\n",
    "    ids = nx.get_node_attributes(nxg, \"_TYPE\")\n",
    "    movies = {}\n",
    "    actors = {}\n",
    "    directors = {}\n",
    "\n",
    "    id_m = 0\n",
    "    id_a = 0\n",
    "    id_d = 0\n",
    "\n",
    "    for el in ids:\n",
    "        if ids[el] == 0:\n",
    "            movies[id_m] = d[el]\n",
    "            id_m += 1\n",
    "        if ids[el ]== 1:\n",
    "            actors[id_a] = d[el]\n",
    "            id_a += 1\n",
    "        if ids[el] == 2:\n",
    "            directors[id_d] = d[el]\n",
    "            id_d += 1\n",
    "\n",
    "    hg = add_feat_imdb(hg,movies,actors,directors,\"triang_count_actor\")\n",
    "    return hg\n",
    "\n",
    "\n",
    "def add_triangle_count_MAM_imdb(hg):\n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    #find all actor of a film\n",
    "    for movie in range(hg.number_of_nodes('movie')):\n",
    "        succ = hg.successors(movie, etype='ma')\n",
    "        #successors contains all actor of a movie\n",
    "        for i in range(len(succ)):\n",
    "            #get the first actor\n",
    "            actor1 = succ[i]\n",
    "            #get the movies connected to actor1\n",
    "            movies = hg.successors(actor1, etype='am')\n",
    "            for e in range(len(movies)):\n",
    "                #add edge between the two movies\n",
    "                hg2.add_edges(torch.tensor(movies[e]), movie, etype='mm')\n",
    "    g = dgl.to_homogeneous(hg2, store_type=True)\n",
    "    nxg = g.to_networkx(node_attrs=['_ID', '_TYPE'], edge_attrs=['_ID', '_TYPE'])\n",
    "    n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(n)\n",
    "    d = nx.triangles(G2)\n",
    "    ids = nx.get_node_attributes(nxg, \"_TYPE\")\n",
    "    movies = {}\n",
    "    actors = {}\n",
    "    directors = {}\n",
    "\n",
    "    id_m = 0\n",
    "    id_a = 0\n",
    "    id_d = 0\n",
    "\n",
    "    for el in ids:\n",
    "        if ids[el] == 0:\n",
    "            movies[id_m] = d[el]\n",
    "            id_m += 1\n",
    "        if ids[el ]== 1:\n",
    "            actors[id_a] = d[el]\n",
    "            id_a += 1\n",
    "        if ids[el] == 2:\n",
    "            directors[id_d] = d[el]\n",
    "            id_d += 1\n",
    "\n",
    "    hg = add_feat_imdb(hg,movies,actors,directors,\"triang_count_MAM\")\n",
    "    return hg\n",
    "\n",
    "    \n",
    "\n",
    "def add_triangle_count_DMAMD_imdb(hg):\n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    #find all directors\n",
    "    for director in range(hg.number_of_nodes('director')):\n",
    "        succ = hg.successors(director, etype='dm')\n",
    "        print(\"movies of director \" + str(director) + \" are \" + str(succ))\n",
    "        #successors contains all movie of a director\n",
    "        for i in range(len(succ)):\n",
    "            #get the first movie\n",
    "            movie1 = succ[i]\n",
    "            #get the actors connected to movie1\n",
    "            actors = hg.successors(movie1, etype='ma')\n",
    "            print(\"actors of movie \" + str(movie1)+ \" are \" + str(actors))\n",
    "            for e in range(len(actors)):\n",
    "                actor1 = actors[e]\n",
    "                movies = hg.successors(actor1, etype='am')\n",
    "                print(\"movie of actor \" +str(actor1)+ \" are \" +str(movies))\n",
    "                for j in range(len(movies)):\n",
    "                    movie1 = movies[j]\n",
    "                    directors = hg.successors(movie1, etype='md')\n",
    "                    print(\"directors of movie \" + str(movie1)+\" sono \"+ str(directors))\n",
    "                    for k in range(len(directors)):\n",
    "                        hg2.add_edges(torch.tensor(directors[k]), director, etype='dd')\n",
    "                        print(\"arco aggiunto tra \" + str(directors[k])+ \" \" + str(director))\n",
    "                        print(\"---------------------\")\n",
    "    g = dgl.to_homogeneous(hg2, store_type=True)\n",
    "    nxg = g.to_networkx(node_attrs=['_ID', '_TYPE'], edge_attrs=['_ID', '_TYPE'])\n",
    "    n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(n)\n",
    "    d = nx.triangles(G2)\n",
    "    ids = nx.get_node_attributes(nxg, \"_TYPE\")\n",
    "    movies = {}\n",
    "    actors = {}\n",
    "    directors = {}\n",
    "\n",
    "    id_m = 0\n",
    "    id_a = 0\n",
    "    id_d = 0\n",
    "\n",
    "    for el in ids:\n",
    "        if ids[el] == 0:\n",
    "            movies[id_m] = d[el]\n",
    "            id_m += 1\n",
    "        if ids[el ]== 1:\n",
    "            actors[id_a] = d[el]\n",
    "            id_a += 1\n",
    "        if ids[el] == 2:\n",
    "            directors[id_d] = d[el]\n",
    "            id_d += 1\n",
    "\n",
    "    hg = add_feat_imdb(hg,movies,actors,directors,\"triang_count_DMAMD\")\n",
    "    return hg\n",
    "\n",
    "\n",
    "def add_pageRank_imdb(hg):\n",
    "    g = dgl.to_homogeneous(hg, store_type=True)\n",
    "    nxg = g.to_networkx(node_attrs=['_ID', '_TYPE'], edge_attrs=['_ID', '_TYPE'])\n",
    "    n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(n)\n",
    "    pr = nx.pagerank(G2, alpha=0.9)\n",
    "    ids = nx.get_node_attributes(nxg, \"_TYPE\")\n",
    "    movies = {}\n",
    "    actors = {}\n",
    "    directors = {}\n",
    "\n",
    "    id_m = 0\n",
    "    id_a = 0\n",
    "    id_d = 0\n",
    "\n",
    "    for el in ids:\n",
    "        if ids[el] == 0:\n",
    "            movies[id_m] = pr[el]\n",
    "            id_m += 1\n",
    "        if ids[el ]== 1:\n",
    "            actors[id_a] = pr[el]\n",
    "            id_a += 1\n",
    "        if ids[el] == 2:\n",
    "            directors[id_d] = pr[el]\n",
    "            id_d += 1\n",
    "\n",
    "    hg = add_feat_imdb(hg,movies,actors,directors,\"page_rank\")\n",
    "    return hg\n",
    "\n",
    "\n",
    "def get_LocalClusteringCoeff_imdb(hg):\n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    #find all actor of a film\n",
    "    for movie in range(hg.number_of_nodes('movie')):\n",
    "        succ = hg.successors(movie, etype='ma')\n",
    "        #successors contains all actor of a movie\n",
    "        for i in range(len(succ)):\n",
    "            #get the first actor\n",
    "            actor1 = succ[i]\n",
    "            #get the movies connected to actor1\n",
    "            movies = hg.successors(actor1, etype='am')\n",
    "            for e in range(len(movies)):\n",
    "                #add edge between the two movies\n",
    "                hg2.add_edges(torch.tensor(movies[e]), movie, etype='mm')\n",
    "    for movie in range(hg.number_of_nodes('movie')):\n",
    "        succ = hg.successors(movie, etype='ma')\n",
    "        #successors contains all actor of a movie\n",
    "        for i in range(len(succ)):\n",
    "            if i + 1 != len(succ):\n",
    "                hg2.add_edges(torch.tensor([succ[i]]), torch.tensor([succ[i+1]]), etype='aa')\n",
    "            else:\n",
    "                hg2.add_edges(torch.tensor([succ[i]]), torch.tensor([succ[0]]), etype='aa')\n",
    "    for director in range(hg.number_of_nodes('director')):\n",
    "        succ = hg.successors(director, etype='dm')\n",
    "        #successors contains all movie of a director\n",
    "        for i in range(len(succ)):\n",
    "            #get the first movie\n",
    "            movie1 = succ[i]\n",
    "            #get the actors connected to movie1\n",
    "            actors = hg.successors(movie1, etype='ma')\n",
    "            for e in range(len(actors)):\n",
    "                actor1 = actors[e]\n",
    "                movies = hg.successors(actor1, etype='am')\n",
    "                for j in range(len(movies)):\n",
    "                    movie1 = movies[j]\n",
    "                    directors = hg.successors(movie1, etype='md')\n",
    "                    for k in range(len(directors)):\n",
    "                        hg2.add_edges(torch.tensor(directors[k]), director, etype='dd')\n",
    "\n",
    "    g = dgl.to_homogeneous(hg2, store_type=True)\n",
    "    nxg = g.to_networkx(node_attrs=['_ID', '_TYPE'], edge_attrs=['_ID', '_TYPE'])\n",
    "    n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(n)\n",
    "    lcc= nx.clustering(G2)\n",
    "    ids = nx.get_node_attributes(nxg, \"_TYPE\")\n",
    "    \n",
    "    movies = {}\n",
    "    actors = {}\n",
    "    directors = {}\n",
    "\n",
    "    id_m = 0\n",
    "    id_a = 0\n",
    "    id_d = 0\n",
    "\n",
    "    for el in ids:\n",
    "        if ids[el] == 0:\n",
    "            movies[id_m] = lcc[el]\n",
    "            id_m += 1\n",
    "        if ids[el ]== 1:\n",
    "            actors[id_a] = lcc[el]\n",
    "            id_a += 1\n",
    "        if ids[el] == 2:\n",
    "            directors[id_d] = lcc[el]\n",
    "            id_d += 1\n",
    "\n",
    "    hg = add_feat_imdb2(hg,movies,actors,directors,\"lcc\")\n",
    "    return hg\n",
    "\n",
    "def add_triangle_count_actorMovie_imdb(hg):\n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    #find all actor of a film\n",
    "    for movie in range(hg.number_of_nodes('movie')):\n",
    "        succ = hg.successors(movie, etype='ma')\n",
    "        #successors contains all actor of a movie\n",
    "        for i in range(len(succ)):\n",
    "            if i + 1 != len(succ):\n",
    "                hg2.add_edges(torch.tensor([succ[i]]), torch.tensor([succ[i+1]]), etype='aa')\n",
    "            else:\n",
    "\n",
    "                hg2.add_edges(torch.tensor([succ[i]]), torch.tensor([succ[0]]), etype='aa')\n",
    "    #find all actor of a film\n",
    "    for movie in range(hg.number_of_nodes('movie')):\n",
    "        succ = hg.successors(movie, etype='ma')\n",
    "        #successors contains all actor of a movie\n",
    "        for i in range(len(succ)):\n",
    "            #get the first actor\n",
    "            actor1 = succ[i]\n",
    "            #get the movies connected to actor1\n",
    "            movies = hg.successors(actor1, etype='am')\n",
    "            for e in range(len(movies)):\n",
    "                #add edge between the two movies\n",
    "                hg2.add_edges(torch.tensor(movies[e]), movie, etype='mm')\n",
    "    g = dgl.to_homogeneous(hg2, store_type=True)\n",
    "    nxg = g.to_networkx(node_attrs=['_ID', '_TYPE'], edge_attrs=['_ID', '_TYPE'])\n",
    "    n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(n)\n",
    "    d = nx.triangles(G2)\n",
    "    ids = nx.get_node_attributes(nxg, \"_TYPE\")\n",
    "    movies = {}\n",
    "    actors = {}\n",
    "    directors = {}\n",
    "\n",
    "    id_m = 0\n",
    "    id_a = 0\n",
    "    id_d = 0\n",
    "\n",
    "    for el in ids:\n",
    "        if ids[el] == 0:\n",
    "            movies[id_m] = d[el]\n",
    "            id_m += 1\n",
    "        if ids[el ]== 1:\n",
    "            actors[id_a] = d[el]\n",
    "            id_a += 1\n",
    "        if ids[el] == 2:\n",
    "            directors[id_d] = d[el]\n",
    "            id_d += 1\n",
    "\n",
    "    movies_dict, actors_dict, directors_dict = check_lenght_imdb(movies,actors,directors)\n",
    "\n",
    "    types = ['movie','actor','director'] \n",
    "    for ntype in types:\n",
    "        if ntype == 'movie' : \n",
    "            x = torch.empty(size=(hg.number_of_nodes(ntype), 1300))\n",
    "            for i in range(len(movies_dict)):\n",
    "                m1 = torch.tensor([movies_dict[i]])\n",
    "                tensor = torch.cat((hg.nodes[ntype].data['feat'][i],m1))\n",
    "                x[i] = tensor\n",
    "            hg.nodes[ntype].data['triang_deg_am'] = x\n",
    "        elif ntype == 'actor':\n",
    "            #print(ntype)\n",
    "            #print(actors_dict)\n",
    "            x = torch.empty(size= (hg.number_of_nodes(ntype), 1300))\n",
    "            for i in range(len(actors_dict)):  \n",
    "                a1 = torch.tensor([actors_dict[i]])\n",
    "                #print(a1)\n",
    "                tensor = torch.cat((hg.nodes[ntype].data['feat'][i],a1))\n",
    "                x[i] = tensor\n",
    "            hg.nodes[ntype].data['triang_deg_am'] = x\n",
    "        elif ntype == 'director':\n",
    "            #print(ntype)\n",
    "            x = torch.empty(size= (hg.number_of_nodes(ntype), 1300))\n",
    "            for i in range(len(directors_dict)):\n",
    "                d1 = torch.tensor([directors_dict[i]])\n",
    "                tensor = torch.cat((hg.nodes[ntype].data['feat'][i],d1))\n",
    "                x[i] = tensor\n",
    "            hg.nodes[ntype].data['triang_deg_am'] = x\n",
    "    return hg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2399a370",
   "metadata": {},
   "source": [
    "#### ADD FEATURES TO NODES FOR DBLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e45586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feat_dblp(hg,papers,authors,conf,terms,feat):\n",
    "\n",
    "    papers_dict, authors_dict, conf_dict, terms_dict = check_lenght_dblp(papers,authors,conf,terms)\n",
    "\n",
    "    types = ['paper','author','conf', 'term'] \n",
    "    for ntype in types:\n",
    "        if ntype == 'paper' : \n",
    "            x = torch.empty(size=(hg.number_of_nodes(ntype), 17))\n",
    "            for i in range(len(papers_dict)):\n",
    "                m1 = torch.tensor([papers_dict[i]])\n",
    "                tensor = torch.cat((hg.nodes[ntype].data['feat'][i],m1))\n",
    "                x[i] = tensor\n",
    "            hg.nodes[ntype].data[feat] = x\n",
    "        elif ntype == 'author':\n",
    "            x = torch.empty(size= (hg.number_of_nodes(ntype), 335))\n",
    "            for i in range(len(authors_dict)):  \n",
    "                a1 = torch.tensor([authors_dict[i]])\n",
    "                #print(a1)\n",
    "                tensor = torch.cat((hg.nodes[ntype].data['feat'][i],a1))\n",
    "                x[i] = tensor\n",
    "            hg.nodes[ntype].data[feat] = x\n",
    "        elif ntype == 'conf':\n",
    "            x = torch.empty(size= (hg.number_of_nodes(ntype), 17))\n",
    "            for i in range(len(conf_dict)):\n",
    "                d1 = torch.tensor([conf_dict[i]])\n",
    "                tensor = torch.cat((hg.nodes[ntype].data['feat'][i],d1))\n",
    "                x[i] = tensor\n",
    "            hg.nodes[ntype].data[feat] = x\n",
    "        elif ntype == 'term':\n",
    "            x = torch.empty(size= (hg.number_of_nodes(ntype), 17))\n",
    "            for i in range(len(terms_dict)):\n",
    "                d1 = torch.tensor([terms_dict[i]])\n",
    "                tensor = torch.cat((hg.nodes[ntype].data['feat'][i],d1))\n",
    "                x[i] = tensor\n",
    "            hg.nodes[ntype].data[feat] = x\n",
    "    return hg\n",
    "\n",
    "\n",
    "def add_feat_dblp2(hg,papers,authors,conf,terms,feat):\n",
    "\n",
    "    papers_dict, authors_dict, conf_dict, terms_dict = check_lenght_dblp(papers,authors,conf,terms)\n",
    "\n",
    "    types = ['paper','author','conf', 'term'] \n",
    "    for ntype in types:\n",
    "        if ntype == 'paper' : \n",
    "            x = torch.empty(size=(hg.number_of_nodes(ntype), 19))\n",
    "            for i in range(len(papers_dict)):\n",
    "                m1 = torch.tensor([papers_dict[i]])\n",
    "                tensor = torch.cat((hg.nodes[ntype].data['feat_deg'][i],m1))\n",
    "                x[i] = tensor\n",
    "            hg.nodes[ntype].data[feat] = x\n",
    "        elif ntype == 'author':\n",
    "            x = torch.empty(size= (hg.number_of_nodes(ntype), 337))\n",
    "            for i in range(len(authors_dict)):  \n",
    "                a1 = torch.tensor([authors_dict[i]])\n",
    "                #print(a1)\n",
    "                tensor = torch.cat((hg.nodes[ntype].data['feat_deg'][i],a1))\n",
    "                x[i] = tensor\n",
    "            hg.nodes[ntype].data[feat] = x\n",
    "        elif ntype == 'conf':\n",
    "            x = torch.empty(size= (hg.number_of_nodes(ntype), 19))\n",
    "            for i in range(len(conf_dict)):\n",
    "                d1 = torch.tensor([conf_dict[i]])\n",
    "                tensor = torch.cat((hg.nodes[ntype].data['feat_deg'][i],d1))\n",
    "                x[i] = tensor\n",
    "            hg.nodes[ntype].data[feat] = x\n",
    "        elif ntype == 'term':\n",
    "            x = torch.empty(size= (hg.number_of_nodes(ntype), 19))\n",
    "            for i in range(len(terms_dict)):\n",
    "                d1 = torch.tensor([terms_dict[i]])\n",
    "                tensor = torch.cat((hg.nodes[ntype].data['feat_deg'][i],d1))\n",
    "                x[i] = tensor\n",
    "            hg.nodes[ntype].data[feat] = x\n",
    "    return hg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12104209",
   "metadata": {},
   "source": [
    "#### UTILS FOR DBLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cfc55bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_degree_dblp(g):\n",
    "    author_deg = []\n",
    "    paper_deg = []\n",
    "    term_deg = []\n",
    "    conf_deg = []\n",
    "\n",
    "    allow_zero_in_degree = True\n",
    "    for n in g.ntypes:\n",
    "        if n == 'author' :\n",
    "            types = ['pa']\n",
    "            for i in g.nodes(n):\n",
    "                deg = 0\n",
    "                for e in types:\n",
    "                    deg = g.in_degrees(i.item(),etype=e)\n",
    "                author_deg.append(deg)\n",
    "        elif n == 'paper':\n",
    "            types = ['ap','cp','tp']\n",
    "            for i in g.nodes(n):\n",
    "                deg = 0\n",
    "                for e in types:\n",
    "                    deg += g.in_degrees(i.item(),etype=e)\n",
    "                paper_deg.append(deg)\n",
    "        elif n == 'conf':\n",
    "            types = ['pc']\n",
    "            for i in g.nodes(n):\n",
    "                deg = 0 \n",
    "                for e in types:\n",
    "                    deg = g.in_degrees(i.item(),etype=e)\n",
    "                conf_deg.append(deg)\n",
    "        elif n == 'term':\n",
    "            types = ['pt']\n",
    "            for i in g.nodes(n):\n",
    "                deg = 0 \n",
    "                for e in types:\n",
    "                    deg = g.in_degrees(i.item(),etype=e)\n",
    "                term_deg.append(deg)\n",
    "                \n",
    "    return author_deg, paper_deg, term_deg, conf_deg\n",
    "\n",
    "def add_degree_feat_dblp(g):\n",
    "    a,p,t,c = get_degree_dblp(g)\n",
    "    types = ['author','paper','term','conf'] #,'paper','term','conf']\n",
    "    for ntype in types:\n",
    "        if ntype == 'author' :\n",
    "            x = torch.empty(size=(g.number_of_nodes(ntype), 335))\n",
    "            for i in range(g.num_nodes(ntype)):\n",
    "                a1 = torch.tensor([a[i]])\n",
    "                tensor = torch.cat((g.nodes[ntype].data['feat'][i],a1))\n",
    "                x[i] = tensor\n",
    "            g.nodes[ntype].data['feat_deg'] = x\n",
    "        elif ntype == 'paper':\n",
    "            x = torch.empty(size= (g.number_of_nodes(ntype), 17))\n",
    "            for i in range(g.num_nodes(ntype)):\n",
    "                p1 = torch.tensor([p[i]])\n",
    "                tensor = torch.cat((g.nodes[ntype].data['feat'][i],p1))\n",
    "                x[i] = tensor\n",
    "            g.nodes[ntype].data['feat_deg'] = x\n",
    "        elif ntype == 'term':\n",
    "            x = torch.empty(size= (g.number_of_nodes(ntype), 17))\n",
    "            for i in range(g.num_nodes(ntype)):\n",
    "                t1 = torch.tensor([t[i]])\n",
    "                tensor = torch.cat((g.nodes[ntype].data['feat'][i],t1))\n",
    "                x[i] = tensor\n",
    "            g.nodes[ntype].data['feat_deg'] = x\n",
    "        elif ntype == 'conf':\n",
    "            x = torch.empty(size= (g.number_of_nodes(ntype), 17))\n",
    "            for i in range(g.num_nodes(ntype)):\n",
    "                c1 = torch.tensor([c[i]])\n",
    "                tensor = torch.cat((g.nodes[ntype].data['feat'][i],c1))\n",
    "                x[i] = tensor\n",
    "            g.nodes[ntype].data['feat_deg'] = x\n",
    "    return g\n",
    "\n",
    "\n",
    "def add_triangle_count_author_dblp(hg):\n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    #find all author of a paper\n",
    "    for paper in range(hg.number_of_nodes('paper')):\n",
    "        succ = hg.successors(paper, etype='pa')\n",
    "        print(succ)\n",
    "        res = [(a, b) for idx, a in enumerate(succ) for b in succ[idx + 1:]]\n",
    "        for i in range(len(res)):\n",
    "            if len(res) != 0:\n",
    "                hg2.add_edges(torch.tensor(res[i][0]), torch.tensor([res[i][1]]), etype='aa')\n",
    "                print(\"arco tra\"+ str(res[i][0]) + \" \" + str(res[i][1]))\n",
    "    g = dgl.to_homogeneous(hg2, store_type=True)\n",
    "    nxg = g.to_networkx(node_attrs=['_ID', '_TYPE'], edge_attrs=['_ID', '_TYPE'])\n",
    "    n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(n)\n",
    "    d = nx.triangles(G2)\n",
    "    ids = nx.get_node_attributes(nxg, \"_TYPE\")\n",
    "    papers = {}\n",
    "    authors = {}\n",
    "    conf = {}\n",
    "    terms = {}\n",
    "\n",
    "    id_p = 0\n",
    "    id_a = 0\n",
    "    id_c = 0\n",
    "    id_t = 0\n",
    "\n",
    "    for el in ids:\n",
    "        if ids[el] == 0:\n",
    "            papers[id_p] = d[el]\n",
    "            id_p += 1\n",
    "        if ids[el ]== 1:\n",
    "            authors[id_a] = d[el]\n",
    "            id_a += 1\n",
    "        if ids[el] == 2:\n",
    "            conf[id_c] = d[el]\n",
    "            id_c += 1\n",
    "        if ids[el] == 3:\n",
    "            terms[id_t] = d[el]\n",
    "            id_t += 1\n",
    "            \n",
    "    hg = add_feat_dblp(hg,papers,authors,conf,terms,\"triang_count_author\") \n",
    "    return hg\n",
    "\n",
    "\n",
    "def add_triangle_count_APA_dblp(hg):\n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    for author in range(hg.number_of_nodes('author')):\n",
    "        #successors contains all paper of an  author \n",
    "        succ = hg.successors(author, etype='ap')\n",
    "        #consider one author\n",
    "        for i in range(len(succ)):\n",
    "            paper = succ[i]\n",
    "            #get the movies connected to author\n",
    "            authors = hg.successors(paper, etype='pa')\n",
    "            for e in range(len(authors)):\n",
    "                #add edge between the two author\n",
    "                hg2.add_edges(torch.tensor(authors[e]), author, etype='aa')\n",
    "    g = dgl.to_homogeneous(hg2, store_type=True)\n",
    "    nxg = g.to_networkx(node_attrs=['_ID', '_TYPE'], edge_attrs=['_ID', '_TYPE'])\n",
    "    n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(n)\n",
    "    d = nx.triangles(G2)\n",
    "    ids = nx.get_node_attributes(nxg, \"_TYPE\")\n",
    "    papers = {}\n",
    "    authors = {}\n",
    "    conf = {}\n",
    "    terms = {}\n",
    "\n",
    "    id_p = 0\n",
    "    id_a = 0\n",
    "    id_c = 0\n",
    "    id_t = 0\n",
    "\n",
    "    for el in ids:\n",
    "        if ids[el] == 0:\n",
    "            papers[id_p] = d[el]\n",
    "            id_p += 1\n",
    "        if ids[el ]== 1:\n",
    "            authors[id_a] = d[el]\n",
    "            id_a += 1\n",
    "        if ids[el] == 2:\n",
    "            conf[id_c] = d[el]\n",
    "            id_c += 1\n",
    "        if ids[el] == 3:\n",
    "            terms[id_t] = d[el]\n",
    "            id_t += 1\n",
    "\n",
    "    hg = add_feat_dblp(hg,papers,authors,conf,terms,\"triang_count_APA\") \n",
    "    return hg\n",
    "\n",
    "\n",
    "def add_triangle_count_APCPA_dblp(hg):\n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    conf_listed = []\n",
    "    for author in range(hg.number_of_nodes('author')):\n",
    "        #successors contains all paper of an  author \n",
    "        papers = hg.successors(author, etype='ap')\n",
    "        #consider one author\n",
    "        for i in range(len(papers)):\n",
    "            paper = papers[i]\n",
    "            conferences = hg.successors(paper, etype='pc')\n",
    "            for e in range(len(conferences)):\n",
    "                conf = conferences[e]\n",
    "                if conf not in conf_listed:\n",
    "                    papers = hg.successors(conf, etype='cp')\n",
    "                    conf_listed.append(conf)\n",
    "                    for k in range(len(papers)):\n",
    "                        paper = papers[k]\n",
    "                        authors = hg.successors(paper, etype='pa')\n",
    "                        for j in range(len(authors)):\n",
    "                            hg2.add_edges(torch.tensor(authors[j]), author, etype='aa')\n",
    "    g = dgl.to_homogeneous(hg2, store_type=True)\n",
    "    nxg = g.to_networkx(node_attrs=['_ID', '_TYPE'], edge_attrs=['_ID', '_TYPE'])\n",
    "    n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(n)\n",
    "    d = nx.triangles(G2)\n",
    "    ids = nx.get_node_attributes(nxg, \"_TYPE\")\n",
    "    papers = {}\n",
    "    authors = {}\n",
    "    conf = {}\n",
    "    terms = {}\n",
    "\n",
    "    id_p = 0\n",
    "    id_a = 0\n",
    "    id_c = 0\n",
    "    id_t = 0\n",
    "\n",
    "    for el in ids:\n",
    "        if ids[el] == 0:\n",
    "            papers[id_p] = d[el]\n",
    "            id_p += 1\n",
    "        if ids[el ]== 1:\n",
    "            authors[id_a] = d[el]\n",
    "            id_a += 1\n",
    "        if ids[el] == 2:\n",
    "            conf[id_c] = d[el]\n",
    "            id_c += 1\n",
    "        if ids[el] == 3:\n",
    "            terms[id_t] = d[el]\n",
    "            id_t += 1\n",
    "\n",
    "    hg = add_feat_dblp(hg,papers,authors,conf,terms,\"triang_count_APCPA\") \n",
    "    return hg\n",
    "\n",
    "\n",
    "def add_triangle_count_APTPA_dblp(hg):\n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    term_listed = []\n",
    "    for author in range(hg.number_of_nodes('author')):\n",
    "        #successors contains all paper of an  author \n",
    "        papers = hg.successors(author, etype='ap')\n",
    "        #consider one author\n",
    "        for i in range(len(papers)):\n",
    "            paper = papers[i]\n",
    "            terms = hg.successors(paper, etype='pt')\n",
    "            for e in range(len(terms)):\n",
    "                term = terms[e]\n",
    "                if term not in term_listed:\n",
    "                    papers2 = hg.successors(term, etype='tp')\n",
    "                    term_listed.append(term)\n",
    "                    for k in range(len(papers2)):\n",
    "                        paper1 = papers2[k]\n",
    "                        authors = hg.successors(paper1, etype='pa')\n",
    "                        for j in range(len(authors)):\n",
    "                            hg2.add_edges(torch.tensor(authors[j]), author, etype='aa')\n",
    "    g = dgl.to_homogeneous(hg2, store_type=True)\n",
    "    nxg = g.to_networkx(node_attrs=['_ID', '_TYPE'], edge_attrs=['_ID', '_TYPE'])\n",
    "    n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(n)\n",
    "    d = nx.triangles(G2)\n",
    "    ids = nx.get_node_attributes(nxg, \"_TYPE\")\n",
    "    papers = {}\n",
    "    authors = {}\n",
    "    conf = {}\n",
    "    terms = {}\n",
    "\n",
    "    id_p = 0\n",
    "    id_a = 0\n",
    "    id_c = 0\n",
    "    id_t = 0\n",
    "\n",
    "    for el in ids:\n",
    "        if ids[el] == 0:\n",
    "            papers[id_p] = d[el]\n",
    "            id_p += 1\n",
    "        if ids[el ]== 1:\n",
    "            authors[id_a] = d[el]\n",
    "            id_a += 1\n",
    "        if ids[el] == 2:\n",
    "            conf[id_c] = d[el]\n",
    "            id_c += 1\n",
    "        if ids[el] == 3:\n",
    "            terms[id_t] = d[el]\n",
    "            id_t += 1\n",
    "\n",
    "    hg = add_feat_dblp(hg,papers,authors,conf,terms,\"triang_count_APTPA\") \n",
    "    return hg\n",
    "\n",
    "\n",
    "def add_pageRank_dblp(hg):\n",
    "    g = dgl.to_homogeneous(hg, store_type=True)\n",
    "    nxg = g.to_networkx(node_attrs=['_ID', '_TYPE'], edge_attrs=['_ID', '_TYPE'])\n",
    "    n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(n)\n",
    "    pr = nx.pagerank(G2, alpha=0.9)\n",
    "    print(pr)\n",
    "    ids = nx.get_node_attributes(nxg, \"_TYPE\")\n",
    "    papers = {}\n",
    "    authors = {}\n",
    "    conf = {}\n",
    "    terms = {}\n",
    "\n",
    "    id_p = 0\n",
    "    id_a = 0\n",
    "    id_c = 0\n",
    "    id_t = 0\n",
    "\n",
    "    for el in ids:\n",
    "        if ids[el] == 0:\n",
    "            papers[id_p] = pr[el]\n",
    "            id_p += 1\n",
    "        if ids[el ]== 1:\n",
    "            authors[id_a] = pr[el]\n",
    "            id_a += 1\n",
    "        if ids[el] == 2:\n",
    "            conf[id_c] = pr[el]\n",
    "            id_c += 1\n",
    "        if ids[el] == 3:\n",
    "            terms[id_t] = pr[el]\n",
    "            id_t += 1\n",
    "\n",
    "    hg = add_feat_dblp2(hg,papers,authors,conf,terms,\"page_rank\")\n",
    "    return hg\n",
    "\n",
    "\n",
    "def get_LocalClusteringCoeff_dblp(hg):\n",
    "    #APA\n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    for author in range(hg.number_of_nodes('author')):\n",
    "        #successors contains all paper of an  author \n",
    "        succ = hg.successors(author, etype='ap')\n",
    "        #consider one author\n",
    "        for i in range(len(succ)):\n",
    "            paper = succ[i]\n",
    "            #get the movies connected to author\n",
    "            authors = hg.successors(paper, etype='pa')\n",
    "            for e in range(len(authors)):\n",
    "                #add edge between the two author\n",
    "                hg2.add_edges(torch.tensor(authors[e]), author, etype='aa')\n",
    "    #APCPA\n",
    "    conf_listed = []\n",
    "    for author in range(hg.number_of_nodes('author')):\n",
    "        #successors contains all paper of an  author \n",
    "        papers = hg.successors(author, etype='ap')\n",
    "        #consider one author\n",
    "        for i in range(len(papers)):\n",
    "            paper = papers[i]\n",
    "            conferences = hg.successors(paper, etype='pc')\n",
    "            for e in range(len(conferences)):\n",
    "                conf = conferences[e]\n",
    "                if conf not in conf_listed:\n",
    "                    papers = hg.successors(conf, etype='cp')\n",
    "                    conf_listed.append(conf)\n",
    "                    for k in range(len(papers)):\n",
    "                        paper = papers[k]\n",
    "                        authors = hg.successors(paper, etype='pa')\n",
    "                        for j in range(len(authors)):\n",
    "                            hg2.add_edges(torch.tensor(authors[j]), author, etype='aa')\n",
    "    #APTPA\n",
    "    term_listed = []\n",
    "    for author in range(hg.number_of_nodes('author')):\n",
    "        #successors contains all paper of an  author \n",
    "        papers = hg.successors(author, etype='ap')\n",
    "        #consider one author\n",
    "        for i in range(len(papers)):\n",
    "            paper = papers[i]\n",
    "            terms = hg.successors(paper, etype='pt')\n",
    "            for e in range(len(terms)):\n",
    "                term = terms[e]\n",
    "                if term not in term_listed:\n",
    "                    papers2 = hg.successors(term, etype='tp')\n",
    "                    term_listed.append(term)\n",
    "                    for k in range(len(papers2)):\n",
    "                        paper1 = papers2[k]\n",
    "                        authors = hg.successors(paper1, etype='pa')\n",
    "                        for j in range(len(authors)):\n",
    "                            hg2.add_edges(torch.tensor(authors[j]), author, etype='aa')\n",
    "    g = dgl.to_homogeneous(hg2, store_type=True)\n",
    "    nxg = g.to_networkx(node_attrs=['_ID', '_TYPE'], edge_attrs=['_ID', '_TYPE'])\n",
    "    n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(n)\n",
    "    lcc= nx.clustering(G2)\n",
    "    ids = nx.get_node_attributes(nxg, \"_TYPE\")\n",
    "    \n",
    "    papers = {}\n",
    "    authors = {}\n",
    "    conf = {}\n",
    "    terms = {}\n",
    "\n",
    "    id_p = 0\n",
    "    id_a = 0\n",
    "    id_c = 0\n",
    "    id_t = 0\n",
    "\n",
    "    for el in ids:\n",
    "        if ids[el] == 0:\n",
    "            papers[id_p] = lcc[el]\n",
    "            id_p += 1\n",
    "        if ids[el ]== 1:\n",
    "            authors[id_a] = lcc[el]\n",
    "            id_a += 1\n",
    "        if ids[el] == 2:\n",
    "            conf[id_c] = lcc[el]\n",
    "            id_c += 1\n",
    "        if ids[el] == 3:\n",
    "            terms[id_t] = lcc[el]\n",
    "            id_t += 1\n",
    "\n",
    "    hg = add_feat_dblp(hg,papers,authors,conf,terms,\"lcc\")\n",
    "    return hg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d3e460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    dgl.seed(seed)\n",
    "    \n",
    "    \n",
    "def accuracy(logits, labels):\n",
    "    \"\"\"Calculate accuracy\n",
    "    :param logits: tensor(N, C) Prediction probability, N is the number of samples, C is the number of categories\n",
    "    :param labels: tensor(N) correct label\n",
    "    :return: float accuracy\n",
    "    \"\"\"\n",
    "    return torch.sum(torch.argmax(logits, dim=1) == labels).item() * 1.0 / len(labels)\n",
    "\n",
    "\n",
    "def micro_macro_f1_score(logits, labels):\n",
    "    \"\"\"Calculate Micro-F1 and Macro-F1 scores\n",
    "    :param logits: tensor(N, C) Prediction probability, N is the number of samples, C is the number of categories\n",
    "    :param labels: tensor(N) \n",
    "    Macro-average precision score can be defined as the arithmetic mean of all the precision scores of different classes.\n",
    "    \"\"\"\n",
    "    prediction = torch.argmax(logits, dim=1).long().numpy()\n",
    "    labels = labels.numpy()\n",
    "    micro_f1 = f1_score(labels, prediction, average='micro')\n",
    "    macro_f1 = f1_score(labels, prediction, average='macro')\n",
    "    return micro_f1, macro_f1\n",
    "\n",
    "def split_idx(samples, train_size, val_size, random_state=None):\n",
    "    \"\"\"The samples are divided into training set, test set and validation set, which must be satisfied (represented by floating point numbers):\n",
    "    * 0 < train_size < 1\n",
    "    * 0 < val_size < 1\n",
    "    * train_size + val_size < 1\n",
    "    \"\"\"\n",
    "    train, val = train_test_split(samples, train_size=train_size, random_state=random_state)\n",
    "    if isinstance(val_size, float):\n",
    "        val_size *= len(samples) / len(val)\n",
    "    val, test = train_test_split(val, train_size=val_size, random_state=random_state)\n",
    "    return train, val, test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae83255e",
   "metadata": {},
   "source": [
    "#### old utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e028c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_lenght_imdb(movies,actors,directors):\n",
    "    if len(movies) == 2081:\n",
    "        directors_dict = movies.copy()\n",
    "    if len(movies) == 5257:\n",
    "        actors_dict = movies.copy()\n",
    "    if len(movies) == 4278:\n",
    "        movies_dict = movies.copy()\n",
    "\n",
    "    if len(actors) == 2081:\n",
    "        directors_dict = actors.copy()\n",
    "    if len(actors) == 4278:\n",
    "        movies_dict = actors.copy()\n",
    "    if len(actors) == 5257:\n",
    "        actors_dict = actors.copy()\n",
    "\n",
    "    if len(directors) == 5257:\n",
    "        actors_dict = directors.copy()\n",
    "    if len(directors) == 4278:\n",
    "        movies_dict = directors.copy()\n",
    "    if len(directors) == 2081:\n",
    "        directors_dict = directors.copy()\n",
    "        \n",
    "    return movies_dict, actors_dict, directors_dict\n",
    "\n",
    "def check_lenght_acm(papers,authors,fields):\n",
    "    if len(papers) == 72:\n",
    "        fields_dict = papers.copy()\n",
    "    if len(papers) == 17351:\n",
    "        authors_dict = papers.copy()\n",
    "    if len(papers) == 4025:\n",
    "        papers_dict = papers.copy()\n",
    "\n",
    "    if len(authors) == 72:\n",
    "        fields_dict = authors.copy()\n",
    "    if len(authors) == 4025:\n",
    "        papers_dict = authors.copy()\n",
    "    if len(authors) == 17352:\n",
    "        actors_dict = authors.copy()\n",
    "\n",
    "    if len(fields) == 17325:\n",
    "        authors_dict = fields.copy()\n",
    "    if len(fields) == 4025:\n",
    "        papers_dict = fields.copy()\n",
    "    if len(fields) == 72:\n",
    "        fields_dict = fields.copy()\n",
    "        \n",
    "    return papers_dict, authors_dict, fields_dict\n",
    "\n",
    "def check_lenght_dblp(papers,authors,conf,terms):\n",
    "    \n",
    "    if len(papers) == 20:\n",
    "        conf_dict = papers.copy()\n",
    "    if len(papers) == 4057:\n",
    "        authors_dict = papers.copy()\n",
    "    if len(papers) == 14328:\n",
    "        papers_dict = papers.copy()\n",
    "    if len(papers) == 7723:\n",
    "        terms_dict = papers.copy()\n",
    "\n",
    "\n",
    "    if len(authors) == 20:\n",
    "        conf_dict = authors.copy()\n",
    "    if len(authors) == 4057:\n",
    "        authors_dict = authors.copy()\n",
    "    if len(authors) == 14328:\n",
    "        papers_dict = authors.copy()\n",
    "    if len(authors) == 7723:\n",
    "        terms_dict = authors.copy()\n",
    "\n",
    "\n",
    "    if len(conf) == 20:\n",
    "        conf_dict = conf.copy()\n",
    "    if len(conf) == 4057:\n",
    "        authors_dict = conf.copy()\n",
    "    if len(conf) == 14328:\n",
    "        papers_dict = conf.copy()\n",
    "    if len(conf) == 7723:\n",
    "        terms_dict = conf.copy()\n",
    "           \n",
    "    if len(terms) == 20:\n",
    "        conf_dict = terms.copy()\n",
    "    if len(terms) == 4057:\n",
    "        authors_dict = terms.copy()\n",
    "    if len(terms) == 14328:\n",
    "        papers_dict = terms.copy()\n",
    "    if len(terms) == 7723:\n",
    "        terms_dict = terms.copy()\n",
    "        \n",
    "    return papers_dict, authors_dict, conf_dict, terms_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
