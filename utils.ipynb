{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcec58b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import itertools\n",
    "\n",
    "\n",
    "import dgl\n",
    "from dgl.nn import HeteroGraphConv\n",
    "from dgl.utils import extract_node_subframes, set_new_frames\n",
    "\n",
    "from sklearn.metrics import f1_score, normalized_mutual_info_score, adjusted_rand_score\n",
    "\n",
    "import copy\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "import json\n",
    "import ast\n",
    "import collections\n",
    "from dgl import AddMetaPaths\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0016dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph(name):\n",
    "    DATASET = {\n",
    "        'acm': ACMDataset(),\n",
    "        'imdb': IMDbDataset(),\n",
    "        'dblp': DBLPFourAreaDataset()\n",
    "    }\n",
    "\n",
    "    set_random_seed(1)\n",
    "    data = DATASET[name]\n",
    "    g = data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766c333a",
   "metadata": {},
   "source": [
    "## UTILS FOR ACM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9879bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_degree_acm(hg):\n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    #find all actor of a film\n",
    "    #transform = AddMetaPaths({\n",
    "    #'pap': [('paper', 'pa', 'author'),('author', 'ap', 'paper')],\n",
    "    #'pfp': [('paper', 'pf', 'field'),('field','fp','paper')],\n",
    "    #'apa': [('author', 'ap', 'paper'),('paper','pa','author')],\n",
    "    #'fpf': [('field', 'fp', 'paper'),('paper','pf','field')]\n",
    "    #})\n",
    "    #hg1 = transform(hg2)\n",
    "    g = dgl.to_homogeneous(hg2)\n",
    "    nxg = g.to_networkx()\n",
    "    #n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    \n",
    "    d = dict(G2.degree)\n",
    "    \n",
    "    \n",
    "    allow_zero_in_degree = True\n",
    "    \n",
    "    authors = {k: d[k] for k in list(d)[:17351]}\n",
    "    fields = {k: d[k] for k in list(d)[17351:17423]}\n",
    "    papers = {k: d[k] for k in list(d)[17423:21449]}\n",
    "    \n",
    "    authors = list(authors.values())\n",
    "    papers= list(papers.values())\n",
    "    fields = list(fields.values())\n",
    " \n",
    "    author = torch.FloatTensor(authors)\n",
    "    paper = torch.FloatTensor(papers)\n",
    "    field = torch.FloatTensor(fields)\n",
    "    \n",
    "    author = torch.unsqueeze(author, 1)\n",
    "    paper = torch.unsqueeze(paper, 1)\n",
    "    field = torch.unsqueeze(field, 1)\n",
    "    \n",
    "    tupl_au = (hg.nodes[\"author\"].data['feat'] ,author)\n",
    "    tupl_pa = (hg.nodes[\"paper\"].data['feat'] ,paper)\n",
    "    tupl_fi = (hg.nodes[\"field\"].data['feat'] ,field)\n",
    "    \n",
    "    author = torch.cat(tupl_au,1)\n",
    "    paper = torch.cat(tupl_pa,1)\n",
    "    field = torch.cat(tupl_fi,1)\n",
    "    \n",
    "    hg.nodes[\"author\"].data['feat'] =  author\n",
    "    hg.nodes[\"paper\"].data['feat'] =  paper\n",
    "    hg.nodes[\"field\"].data['feat'] =  field\n",
    "    \n",
    "    print(hg.nodes[\"author\"].data['feat'])\n",
    "    print(hg.nodes[\"paper\"].data['feat'])\n",
    "    print(hg.nodes[\"field\"].data['feat'])\n",
    "                \n",
    "    return hg\n",
    "    \n",
    "\n",
    "def get_pageRank_acm(hg):\n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    \n",
    "    transform = AddMetaPaths({\n",
    "    #'pap': [('paper', 'pa', 'author'),('author', 'ap', 'paper')],\n",
    "    'pfp': [('paper', 'pf', 'field'),('field','fp','paper')],\n",
    "    #'apa': [('author', 'ap', 'paper'),('paper','pa','author')],\n",
    "    #'fpf': [('field', 'fp', 'paper'),('paper','pf','field')]\n",
    "        \n",
    "    })\n",
    "    hg3 = transform(hg2)\n",
    "    g = dgl.to_homogeneous(hg3)\n",
    "    nxg = g.to_networkx()\n",
    "    #n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    \n",
    "    d = nx.pagerank(G2, alpha=0.9)\n",
    "        \n",
    "    authors = {k: d[k] for k in list(d)[:17351]}\n",
    "    fields = {k: d[k] for k in list(d)[17351:17423]}\n",
    "    papers = {k: d[k] for k in list(d)[17423:21449]}\n",
    "    \n",
    "    authors = list(authors.values())\n",
    "    papers= list(papers.values())\n",
    "    fields = list(fields.values())\n",
    " \n",
    "    author = torch.FloatTensor(authors)\n",
    "    paper = torch.FloatTensor(papers)\n",
    "    field = torch.FloatTensor(fields)\n",
    "    \n",
    "    author = torch.unsqueeze(author, 1)\n",
    "    paper = torch.unsqueeze(paper, 1)\n",
    "    field = torch.unsqueeze(field, 1)\n",
    "    \n",
    "    tupl_au = (hg.nodes[\"author\"].data['feat'] ,author)\n",
    "    tupl_pa = (hg.nodes[\"paper\"].data['feat'] ,paper)\n",
    "    tupl_fi = (hg.nodes[\"field\"].data['feat'] ,field)\n",
    "    \n",
    "    author = torch.cat(tupl_au,1)\n",
    "    paper = torch.cat(tupl_pa,1)\n",
    "    field = torch.cat(tupl_fi,1)\n",
    "    \n",
    "    hg.nodes[\"author\"].data['feat'] =  author\n",
    "    hg.nodes[\"paper\"].data['feat'] =  paper\n",
    "    hg.nodes[\"field\"].data['feat'] =  field\n",
    "    \n",
    "    print(hg.nodes[\"author\"].data['feat'])\n",
    "    print(hg.nodes[\"paper\"].data['feat'])\n",
    "    print(hg.nodes[\"field\"].data['feat'])\n",
    "    \n",
    "    return hg\n",
    "\n",
    "\n",
    "def get_LCC_acm(hg):\n",
    "    \n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    \n",
    "    transform = AddMetaPaths({\n",
    "    'pap': [('paper', 'pa', 'author'),('author', 'ap', 'paper')],\n",
    "    'pfp': [('paper', 'pf', 'field'),('field','fp','paper')],\n",
    "    #'apa': [('author', 'ap', 'paper'),('paper','pa','author')],\n",
    "    #'fpf': [('field', 'fp', 'paper'),('paper','pf','field')]\n",
    "    })\n",
    "    hg2 = transform(hg2)\n",
    "    g = dgl.to_homogeneous(hg2)\n",
    "    nxg = g.to_networkx()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    d= nx.clustering(G2)\n",
    "    \n",
    "    \n",
    "    authors = {k: d[k] for k in list(d)[:17351]}\n",
    "    fields = {k: d[k] for k in list(d)[17351:17423]}\n",
    "    papers = {k: d[k] for k in list(d)[17423:21449]}\n",
    "    \n",
    "    authors = list(authors.values())\n",
    "    papers= list(papers.values())\n",
    "    fields = list(fields.values())\n",
    " \n",
    "    author = torch.LongTensor(authors)\n",
    "    paper = torch.LongTensor(papers)\n",
    "    field = torch.LongTensor(fields)\n",
    "    \n",
    "    author = torch.unsqueeze(author, 1)\n",
    "    paper = torch.unsqueeze(paper, 1)\n",
    "    field = torch.unsqueeze(field, 1)\n",
    "    \n",
    "    tupl_au = (hg.nodes[\"author\"].data['feat'] ,author)\n",
    "    tupl_pa = (hg.nodes[\"paper\"].data['feat'] ,paper)\n",
    "    tupl_fi = (hg.nodes[\"field\"].data['feat'] ,field)\n",
    "    \n",
    "    author = torch.cat(tupl_au,1)\n",
    "    paper = torch.cat(tupl_pa,1)\n",
    "    field = torch.cat(tupl_fi,1)\n",
    "    \n",
    "    hg.nodes[\"author\"].data['feat'] =  author\n",
    "    hg.nodes[\"paper\"].data['feat'] =  paper\n",
    "    hg.nodes[\"field\"].data['feat'] =  field\n",
    "    \n",
    "    return hg\n",
    "\n",
    "def get_degree_centrality_acm(hg):\n",
    "    \n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    \n",
    "    #transform = AddMetaPaths({\n",
    "    #'pap': [('paper', 'pa', 'author'),('author', 'ap', 'paper')],\n",
    "    #'pfp': [('paper', 'pf', 'field'),('field','fp','paper')],\n",
    "    #'apa': [('author', 'ap', 'paper'),('paper','pa','author')],\n",
    "    #'fpf': [('field', 'fp', 'paper'),('paper','pf','field')]\n",
    "    #})\n",
    "    #hg2 = transform(hg2)\n",
    "    g = dgl.to_homogeneous(hg2)\n",
    "    nxg = g.to_networkx()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    d= nx.degree_centrality(G2)\n",
    "    \n",
    "    \n",
    "    authors = {k: d[k] for k in list(d)[:17351]}\n",
    "    fields = {k: d[k] for k in list(d)[17351:17423]}\n",
    "    papers = {k: d[k] for k in list(d)[17423:21449]}\n",
    "    \n",
    "    authors = list(authors.values())\n",
    "    papers= list(papers.values())\n",
    "    fields = list(fields.values())\n",
    " \n",
    "    author = torch.LongTensor(authors)\n",
    "    paper = torch.LongTensor(papers)\n",
    "    field = torch.LongTensor(fields)\n",
    "    \n",
    "    author = torch.unsqueeze(author, 1)\n",
    "    paper = torch.unsqueeze(paper, 1)\n",
    "    field = torch.unsqueeze(field, 1)\n",
    "    \n",
    "    tupl_au = (hg.nodes[\"author\"].data['feat'] ,author)\n",
    "    tupl_pa = (hg.nodes[\"paper\"].data['feat'] ,paper)\n",
    "    tupl_fi = (hg.nodes[\"field\"].data['feat'] ,field)\n",
    "    \n",
    "    author = torch.cat(tupl_au,1)\n",
    "    paper = torch.cat(tupl_pa,1)\n",
    "    field = torch.cat(tupl_fi,1)\n",
    "    \n",
    "    hg.nodes[\"author\"].data['feat'] =  author\n",
    "    hg.nodes[\"paper\"].data['feat'] =  paper\n",
    "    hg.nodes[\"field\"].data['feat'] =  field\n",
    "    \n",
    "    return hg\n",
    "\n",
    "\n",
    "\n",
    "def get_triangle_count_acm(hg):\n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    \n",
    "    transform = AddMetaPaths({\n",
    "    #'pap': [('paper', 'pa', 'author'),('author', 'ap', 'paper')],\n",
    "    #'pfp': [('paper', 'pf', 'field'),('field','fp','paper')],\n",
    "    'apa': [('author', 'ap', 'paper'),('paper','pa','author')],\n",
    "    'fpf': [('field', 'fp', 'paper'),('paper','pf','field')]\n",
    "    \n",
    "    })\n",
    "    \n",
    "    hg3 = transform(hg2)\n",
    "    g = dgl.to_homogeneous(hg2)\n",
    "    nxg = g.to_networkx()\n",
    "    n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(n)\n",
    "    d = nx.triangles(G2)\n",
    "\n",
    "    authors = {k: d[k] for k in list(d)[:17351]}\n",
    "    fields = {k: d[k] for k in list(d)[17351:17423]}\n",
    "    papers = {k: d[k] for k in list(d)[17423:21449]}\n",
    "    authors = list(authors.values())\n",
    "    papers= list(papers.values())\n",
    "    fields = list(fields.values())\n",
    " \n",
    "    author = torch.LongTensor(authors)\n",
    "    paper = torch.LongTensor(papers)\n",
    "    field = torch.LongTensor(fields)\n",
    "    \n",
    "    author = torch.unsqueeze(author, 1)\n",
    "    paper = torch.unsqueeze(paper, 1)\n",
    "    field = torch.unsqueeze(field, 1)\n",
    "    \n",
    "    tupl_au = (hg.nodes[\"author\"].data['feat'] ,author)\n",
    "    tupl_pa = (hg.nodes[\"paper\"].data['feat'] ,paper)\n",
    "    tupl_fi = (hg.nodes[\"field\"].data['feat'] ,field)\n",
    "    \n",
    "    author = torch.cat(tupl_au,1)\n",
    "    paper = torch.cat(tupl_pa,1)\n",
    "    field = torch.cat(tupl_fi,1)\n",
    "    \n",
    "    hg.nodes[\"author\"].data['feat'] =  author\n",
    "    hg.nodes[\"paper\"].data['feat'] =  paper\n",
    "    hg.nodes[\"field\"].data['feat'] =  field\n",
    "    \n",
    "    return hg\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe98862",
   "metadata": {},
   "source": [
    "## UTILS FOR IMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "282e73d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_degree_imdb(hg):\n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    #find all actor of a film\n",
    "    transform = AddMetaPaths({\n",
    "    #'mam': [('movie', 'ma', 'actor'),('actor', 'am', 'movie')],\n",
    "    #'ama': [('actor', 'am', 'movie'),('movie','ma','actor')],\n",
    "    #'dmd': [('director', 'dm', 'movie'),('movie','md','director')],\n",
    "    'mdm': [('movie', 'md', 'director'),('director','dm','movie')]\n",
    "    })\n",
    "    hg1 = transform(hg2)\n",
    "    g = dgl.to_homogeneous(hg1)\n",
    "    nxg = g.to_networkx()\n",
    "    #n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    \n",
    "    d = dict(G2.degree)\n",
    "    \n",
    "    \n",
    "    allow_zero_in_degree = True\n",
    "    \n",
    "    actors = {k: d[k] for k in list(d)[:5257]}\n",
    "    directors = {k: d[k] for k in list(d)[5257:7338]}\n",
    "    movies = {k: d[k] for k in list(d)[7338:11616]}\n",
    "    \n",
    "    actors_deg = list(actors.values())\n",
    "    directors_deg = list(directors.values())\n",
    "    movies_deg = list(movies.values())\n",
    "    \n",
    "    actor = torch.LongTensor(actors_deg)\n",
    "    director = torch.LongTensor(directors_deg)\n",
    "    movie = torch.LongTensor(movies_deg)\n",
    "    \n",
    "    actor = torch.unsqueeze(actor, 1)\n",
    "    director = torch.unsqueeze(director, 1)\n",
    "    movie = torch.unsqueeze(movie, 1)\n",
    "    \n",
    "    tupl_a = (hg.nodes[\"actor\"].data['feat'] ,actor)\n",
    "    tupl_d = (hg.nodes[\"director\"].data['feat'] ,director)\n",
    "    tupl_m = (hg.nodes[\"movie\"].data['feat'] ,movie)\n",
    "    \n",
    "    actor = torch.cat(tupl_a,1)\n",
    "    director = torch.cat(tupl_d,1)\n",
    "    movie = torch.cat(tupl_m,1)\n",
    "    \n",
    "    hg.nodes[\"actor\"].data['feat'] =  actor\n",
    "    hg.nodes[\"director\"].data['feat'] =  director\n",
    "    hg.nodes[\"movie\"].data['feat'] =  movie\n",
    "    \n",
    "    print(hg.nodes[\"actor\"].data['feat'])\n",
    "                    \n",
    "    return hg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_triangle_count_imdb(hg):\n",
    "    hg1 = copy.deepcopy(hg)\n",
    "    #find all actor of a film\n",
    "    transform = AddMetaPaths({\n",
    "    #'mam': [('movie', 'ma', 'actor'),('actor', 'am', 'movie')],\n",
    "    #'ama': [('actor', 'am', 'movie'),('movie','ma','actor')],\n",
    "    'dmd': [('director', 'dm', 'movie'),('movie','md','director')],\n",
    "    #'mdm': [('movie', 'md', 'director'),('director','dm','movie')]\n",
    "    })\n",
    "    hg2 = transform(hg1)\n",
    "    g = dgl.to_homogeneous(hg2)\n",
    "    nxg = g.to_networkx()\n",
    "    #n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    d = nx.triangles(G2)\n",
    "    \n",
    "    \n",
    "    actors = {k: d[k] for k in list(d)[:5257]}\n",
    "    directors = {k: d[k] for k in list(d)[5257:7338]}\n",
    "    movies = {k: d[k] for k in list(d)[7338:11616]}\n",
    "    \n",
    "    actors_count = list(actors.values())\n",
    "    directors_count = list(directors.values())\n",
    "    movies_count = list(movies.values())\n",
    "    \n",
    "    actor = torch.LongTensor(actors_count)\n",
    "    director = torch.LongTensor(directors_count)\n",
    "    movie = torch.LongTensor(movies_count)\n",
    "    \n",
    "    actor = torch.unsqueeze(actor, 1)\n",
    "    director = torch.unsqueeze(director, 1)\n",
    "    movie = torch.unsqueeze(movie, 1)\n",
    "    \n",
    "    tupl_a = (hg.nodes[\"actor\"].data['feat'] ,actor)\n",
    "    tupl_d = (hg.nodes[\"director\"].data['feat'] ,director)\n",
    "    tupl_m = (hg.nodes[\"movie\"].data['feat'] ,movie)\n",
    "    \n",
    "    actor = torch.cat(tupl_a,1)\n",
    "    director = torch.cat(tupl_d,1)\n",
    "    movie = torch.cat(tupl_m,1)\n",
    "    \n",
    "    hg.nodes[\"actor\"].data['feat'] =  actor\n",
    "    hg.nodes[\"director\"].data['feat'] =  director\n",
    "    hg.nodes[\"movie\"].data['feat'] =  movie\n",
    "    \n",
    "    print(hg.nodes[\"actor\"].data['feat'])\n",
    "    print(len(hg.nodes[\"actor\"].data['feat'][0]))\n",
    "\n",
    "    return hg\n",
    "\n",
    "\n",
    "\n",
    "def get_LCC_imdb(hg):\n",
    "    hg1 = copy.deepcopy(hg)\n",
    "    #find all actor of a film\n",
    "    transform = AddMetaPaths({\n",
    "    'mam': [('movie', 'ma', 'actor'),('actor', 'am', 'movie')],\n",
    "    #'ama': [('actor', 'am', 'movie'),('movie','ma','actor')],\n",
    "    #'dmd': [('director', 'dm', 'movie'),('movie','md','director')],\n",
    "    #'mdm': [('movie', 'md', 'director'),('director','dm','movie')]\n",
    "    })\n",
    "    hg2 = transform(hg1)\n",
    "    g = dgl.to_homogeneous(hg2)\n",
    "    nxg = g.to_networkx()\n",
    "    #n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    d= nx.clustering(G2)\n",
    "    \n",
    "    \n",
    "    actors = {k: d[k] for k in list(d)[:5257]}\n",
    "    directors = {k: d[k] for k in list(d)[5257:7338]}\n",
    "    movies = {k: d[k] for k in list(d)[7338:11616]}\n",
    "    \n",
    "    actors_count = list(actors.values())\n",
    "    directors_count = list(directors.values())\n",
    "    movies_count = list(movies.values())\n",
    "    \n",
    "    actor = torch.LongTensor(actors_count)\n",
    "    director = torch.LongTensor(directors_count)\n",
    "    movie = torch.LongTensor(movies_count)\n",
    "    \n",
    "    actor = torch.unsqueeze(actor, 1)\n",
    "    director = torch.unsqueeze(director, 1)\n",
    "    movie = torch.unsqueeze(movie, 1)\n",
    "    \n",
    "    tupl_a = (hg.nodes[\"actor\"].data['feat'] ,actor)\n",
    "    tupl_d = (hg.nodes[\"director\"].data['feat'] ,director)\n",
    "    tupl_m = (hg.nodes[\"movie\"].data['feat'] ,movie)\n",
    "    \n",
    "    actor = torch.cat(tupl_a,1)\n",
    "    director = torch.cat(tupl_d,1)\n",
    "    movie = torch.cat(tupl_m,1)\n",
    "    \n",
    "    hg.nodes[\"actor\"].data['feat'] =  actor\n",
    "    hg.nodes[\"director\"].data['feat'] =  director\n",
    "    hg.nodes[\"movie\"].data['feat'] =  movie\n",
    "    \n",
    "    print(hg.nodes[\"actor\"].data['feat'])\n",
    "    print(len(hg.nodes[\"actor\"].data['feat'][0]))\n",
    "    print(len(hg.nodes[\"movie\"].data['feat'][0]))\n",
    "\n",
    "    return hg\n",
    "\n",
    "def get_PageRank_imdb(hg):\n",
    "    hg1 = copy.deepcopy(hg)\n",
    "    #find all actor of a film\n",
    "    transform = AddMetaPaths({\n",
    "    'mam': [('movie', 'ma', 'actor'),('actor', 'am', 'movie')],\n",
    "    #'ama': [('actor', 'am', 'movie'),('movie','ma','actor')],\n",
    "    #'dmd': [('director', 'dm', 'movie'),('movie','md','director')],\n",
    "    #'mdm': [('movie', 'md', 'director'),('director','dm','movie')]\n",
    "    })\n",
    "    hg2 = transform(hg1)\n",
    "    g = dgl.to_homogeneous(hg2)\n",
    "    nxg = g.to_networkx()\n",
    "    #n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    d = nx.pagerank(G2, alpha=0.9)\n",
    "    \n",
    "    \n",
    "    actors = {k: d[k] for k in list(d)[:5257]}\n",
    "    directors = {k: d[k] for k in list(d)[5257:7338]}\n",
    "    movies = {k: d[k] for k in list(d)[7338:11616]}\n",
    "    \n",
    "    actors_count = list(actors.values())\n",
    "    directors_count = list(directors.values())\n",
    "    movies_count = list(movies.values())\n",
    "    \n",
    "    actor = torch.FloatTensor(actors_count)\n",
    "    director = torch.FloatTensor(directors_count)\n",
    "    movie = torch.FloatTensor(movies_count)\n",
    "    \n",
    "    actor = torch.unsqueeze(actor, 1)\n",
    "    director = torch.unsqueeze(director, 1)\n",
    "    movie = torch.unsqueeze(movie, 1)\n",
    "    \n",
    "    tupl_a = (hg.nodes[\"actor\"].data['feat'] ,actor)\n",
    "    tupl_d = (hg.nodes[\"director\"].data['feat'] ,director)\n",
    "    tupl_m = (hg.nodes[\"movie\"].data['feat'] ,movie)\n",
    "    \n",
    "    actor = torch.cat(tupl_a,1)\n",
    "    director = torch.cat(tupl_d,1)\n",
    "    movie = torch.cat(tupl_m,1)\n",
    "    \n",
    "    hg.nodes[\"actor\"].data['feat'] =  actor\n",
    "    hg.nodes[\"director\"].data['feat'] =  director\n",
    "    hg.nodes[\"movie\"].data['feat'] =  movie\n",
    "    \n",
    "    print(hg.nodes[\"actor\"].data['feat'])\n",
    "    print(len(hg.nodes[\"actor\"].data['feat'][0]))\n",
    "\n",
    "    return hg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12104209",
   "metadata": {},
   "source": [
    "## UTILS FOR DBLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cfc55bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_degree_dblp(hg):\n",
    "    \n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    \n",
    "    #transform = AddMetaPaths({\n",
    "    #'apa': [('author', 'ap', 'paper'),('paper', 'pa', 'author')],\n",
    "    #'pap': [('paper', 'pa', 'author'),('author','ap','paper')],\n",
    "    #'ptp': [('paper', 'pt', 'term'),('term','tp','paper')],\n",
    "    #'tpt': [('term', 'tp', 'paper'),('paper','pt','term')],\n",
    "    #'pcp': [('paper', 'pc', 'conf'),('conf','cp','paper')],\n",
    "    #'cpc': [('conf', 'cp', 'paper'),('paper','pc','conf')]\n",
    "    #})\n",
    "    #hg1 = transform(hg2)\n",
    "    g = dgl.to_homogeneous(hg2)\n",
    "    nxg = g.to_networkx()\n",
    "    #n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    \n",
    "    d = dict(G2.degree)\n",
    "    \n",
    "    allow_zero_in_degree = True\n",
    "    \n",
    "    authors = {k: d[k] for k in list(d)[:4057]}\n",
    "    confs = {k: d[k] for k in list(d)[4057:4077]}\n",
    "    papers = {k: d[k] for k in list(d)[4077:18405]}\n",
    "    terms = {k: d[k] for k in list(d)[18405:26128]}\n",
    "    \n",
    "    authors_deg = list(authors.values())\n",
    "    confs_deg = list(confs.values())\n",
    "    papers_deg = list(papers.values())\n",
    "    terms_deg = list(terms.values())\n",
    "    \n",
    "    author = torch.LongTensor(authors_deg)\n",
    "    conf = torch.LongTensor(confs_deg)\n",
    "    paper = torch.LongTensor(papers_deg)\n",
    "    term = torch.LongTensor(terms_deg)\n",
    "    \n",
    "    author = torch.unsqueeze(author, 1)\n",
    "    conf = torch.unsqueeze(conf, 1)\n",
    "    paper = torch.unsqueeze(paper, 1)\n",
    "    term = torch.unsqueeze(term, 1)\n",
    "    \n",
    "    tupl_a = (hg.nodes[\"author\"].data['feat'] ,author)\n",
    "    tupl_c = (hg.nodes[\"conf\"].data['feat'] ,conf)\n",
    "    tupl_p = (hg.nodes[\"paper\"].data['feat'] ,paper)\n",
    "    tupl_t = (hg.nodes[\"term\"].data['feat'] ,term)\n",
    "    \n",
    "    author = torch.cat(tupl_a,1)\n",
    "    conf = torch.cat(tupl_c,1)\n",
    "    paper = torch.cat(tupl_p,1)\n",
    "    term = torch.cat(tupl_t,1)\n",
    "    \n",
    "    hg.nodes[\"author\"].data['feat'] =  author\n",
    "    hg.nodes[\"conf\"].data['feat'] =  conf\n",
    "    hg.nodes[\"paper\"].data['feat'] =  paper\n",
    "    hg.nodes[\"term\"].data['feat'] =  term\n",
    "               \n",
    "    return hg\n",
    "\n",
    "\n",
    "def get_triangle_count_dblp(hg):\n",
    "    \n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    \n",
    "    transform = AddMetaPaths({\n",
    "    #'apa': [('author', 'ap', 'paper'),('paper', 'pa', 'author')],\n",
    "    'pap': [('paper', 'pa', 'author'),('author','ap','paper')],\n",
    "    #'ptp': [('paper', 'pt', 'term'),('term','tp','paper')],\n",
    "    #'tpt': [('term', 'tp', 'paper'),('paper','pt','term')],\n",
    "    #'pcp': [('paper', 'pc', 'conf'),('conf','cp','paper')],\n",
    "    #'cpc': [('conf', 'cp', 'paper'),('paper','pc','conf')]\n",
    "    })\n",
    "    hg1 = transform(hg2)\n",
    "    g = dgl.to_homogeneous(hg1)\n",
    "    nxg = g.to_networkx()\n",
    "    #n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    \n",
    "    d = nx.triangles(G2)\n",
    "    \n",
    "    authors = {k: d[k] for k in list(d)[:4057]}\n",
    "    confs = {k: d[k] for k in list(d)[4057:4077]}\n",
    "    papers = {k: d[k] for k in list(d)[4077:18405]}\n",
    "    terms = {k: d[k] for k in list(d)[18405:26128]}\n",
    "    \n",
    "    \n",
    "    authors_deg = list(authors.values())\n",
    "    confs_deg = list(confs.values())\n",
    "    papers_deg = list(papers.values())\n",
    "    terms_deg = list(terms.values())\n",
    "    \n",
    "    author = torch.LongTensor(authors_deg)\n",
    "    conf = torch.LongTensor(confs_deg)\n",
    "    paper = torch.LongTensor(papers_deg)\n",
    "    term = torch.LongTensor(terms_deg)\n",
    "    \n",
    "    author = torch.unsqueeze(author, 1)\n",
    "    conf = torch.unsqueeze(conf, 1)\n",
    "    paper = torch.unsqueeze(paper, 1)\n",
    "    term = torch.unsqueeze(term, 1)\n",
    "    \n",
    "    tupl_a = (hg.nodes[\"author\"].data['feat'] ,author)\n",
    "    tupl_c = (hg.nodes[\"conf\"].data['feat'] ,conf)\n",
    "    tupl_p = (hg.nodes[\"paper\"].data['feat'] ,paper)\n",
    "    tupl_t = (hg.nodes[\"term\"].data['feat'] ,term)\n",
    "    \n",
    "    author = torch.cat(tupl_a,1)\n",
    "    conf = torch.cat(tupl_c,1)\n",
    "    paper = torch.cat(tupl_p,1)\n",
    "    term = torch.cat(tupl_t,1)\n",
    "    \n",
    "    hg.nodes[\"author\"].data['feat'] =  author\n",
    "    hg.nodes[\"conf\"].data['feat'] =  conf\n",
    "    hg.nodes[\"paper\"].data['feat'] =  paper\n",
    "    hg.nodes[\"term\"].data['feat'] =  term\n",
    "    \n",
    "    print(hg.nodes[\"author\"].data['feat'])\n",
    "    print(len(hg.nodes[\"author\"].data['feat'][0]))\n",
    "                         \n",
    "    return hg\n",
    "\n",
    "\n",
    "def get_PageRank_dblp(hg):\n",
    "    \n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    \n",
    "    #transform = AddMetaPaths({\n",
    "    #'apa': [('author', 'ap', 'paper'),('paper', 'pa', 'author')],\n",
    "    #'pap': [('paper', 'pa', 'author'),('author','ap','paper')],\n",
    "    #'ptp': [('paper', 'pt', 'term'),('term','tp','paper')],\n",
    "    #'tpt': [('term', 'tp', 'paper'),('paper','pt','term')],\n",
    "    #'pcp': [('paper', 'pc', 'conf'),('conf','cp','paper')],\n",
    "    #'cpc': [('conf', 'cp', 'paper'),('paper','pc','conf')]\n",
    "    #})\n",
    "    #hg1 = transform(hg2)\n",
    "    g = dgl.to_homogeneous(hg2)\n",
    "    nxg = g.to_networkx()\n",
    "    #n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    \n",
    "    d = nx.pagerank(G2, alpha=0.9)\n",
    "    \n",
    "    authors = {k: d[k] for k in list(d)[:4057]}\n",
    "    confs = {k: d[k] for k in list(d)[4057:4077]}\n",
    "    papers = {k: d[k] for k in list(d)[4077:18405]}\n",
    "    terms = {k: d[k] for k in list(d)[18405:26128]}\n",
    "    \n",
    "    authors_deg = list(authors.values())\n",
    "    confs_deg = list(confs.values())\n",
    "    papers_deg = list(papers.values())\n",
    "    terms_deg = list(terms.values())\n",
    "    \n",
    "    author = torch.FloatTensor(authors_deg)\n",
    "    conf = torch.FloatTensor(confs_deg)\n",
    "    paper = torch.FloatTensor(papers_deg)\n",
    "    term = torch.FloatTensor(terms_deg)\n",
    "    \n",
    "    author = torch.unsqueeze(author, 1)\n",
    "    conf = torch.unsqueeze(conf, 1)\n",
    "    paper = torch.unsqueeze(paper, 1)\n",
    "    term = torch.unsqueeze(term, 1)\n",
    "    \n",
    "    tupl_a = (hg.nodes[\"author\"].data['feat'] ,author)\n",
    "    tupl_c = (hg.nodes[\"conf\"].data['feat'] ,conf)\n",
    "    tupl_p = (hg.nodes[\"paper\"].data['feat'] ,paper)\n",
    "    tupl_t = (hg.nodes[\"term\"].data['feat'] ,term)\n",
    "    \n",
    "    author = torch.cat(tupl_a,1)\n",
    "    conf = torch.cat(tupl_c,1)\n",
    "    paper = torch.cat(tupl_p,1)\n",
    "    term = torch.cat(tupl_t,1)\n",
    "    \n",
    "    hg.nodes[\"author\"].data['feat'] =  author\n",
    "    hg.nodes[\"conf\"].data['feat'] =  conf\n",
    "    hg.nodes[\"paper\"].data['feat'] =  paper\n",
    "    hg.nodes[\"term\"].data['feat'] =  term\n",
    "                    \n",
    "    return hg\n",
    "\n",
    "\n",
    "def get_LCC_dblp(hg):\n",
    "    \n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    \n",
    "    transform = AddMetaPaths({\n",
    "    #'apa': [('author', 'ap', 'paper'),('paper', 'pa', 'author')],\n",
    "    #'pap': [('paper', 'pa', 'author'),('author','ap','paper')],\n",
    "    #'ptp': [('paper', 'pt', 'term'),('term','tp','paper')],\n",
    "    'tpt': [('term', 'tp', 'paper'),('paper','pt','term')],\n",
    "    #'pcp': [('paper', 'pc', 'conf'),('conf','cp','paper')],\n",
    "    #'cpc': [('conf', 'cp', 'paper'),('paper','pc','conf')]\n",
    "    })\n",
    "    hg1 = transform(hg2)\n",
    "    g = dgl.to_homogeneous(hg1)\n",
    "    nxg = g.to_networkx()\n",
    "    #n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    \n",
    "    d= nx.clustering(G2)\n",
    "    \n",
    "    authors = {k: d[k] for k in list(d)[:4057]}\n",
    "    confs = {k: d[k] for k in list(d)[4057:4077]}\n",
    "    papers = {k: d[k] for k in list(d)[4077:18405]}\n",
    "    terms = {k: d[k] for k in list(d)[18405:26128]}\n",
    "    \n",
    "    authors_deg = list(authors.values())\n",
    "    confs_deg = list(confs.values())\n",
    "    papers_deg = list(papers.values())\n",
    "    terms_deg = list(terms.values())\n",
    "    \n",
    "    author = torch.FloatTensor(authors_deg)\n",
    "    conf = torch.FloatTensor(confs_deg)\n",
    "    paper = torch.FloatTensor(papers_deg)\n",
    "    term = torch.FloatTensor(terms_deg)\n",
    "    \n",
    "    author = torch.unsqueeze(author, 1)\n",
    "    conf = torch.unsqueeze(conf, 1)\n",
    "    paper = torch.unsqueeze(paper, 1)\n",
    "    term = torch.unsqueeze(term, 1)\n",
    "    \n",
    "    tupl_a = (hg.nodes[\"author\"].data['feat'] ,author)\n",
    "    tupl_c = (hg.nodes[\"conf\"].data['feat'] ,conf)\n",
    "    tupl_p = (hg.nodes[\"paper\"].data['feat'] ,paper)\n",
    "    tupl_t = (hg.nodes[\"term\"].data['feat'] ,term)\n",
    "    \n",
    "    author = torch.cat(tupl_a,1)\n",
    "    conf = torch.cat(tupl_c,1)\n",
    "    paper = torch.cat(tupl_p,1)\n",
    "    term = torch.cat(tupl_t,1)\n",
    "    \n",
    "    hg.nodes[\"author\"].data['feat'] =  author\n",
    "    hg.nodes[\"conf\"].data['feat'] =  conf\n",
    "    hg.nodes[\"paper\"].data['feat'] =  paper\n",
    "    hg.nodes[\"term\"].data['feat'] =  term\n",
    "                \n",
    "    return hg\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d3e460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    dgl.seed(seed)\n",
    "    \n",
    "    \n",
    "def accuracy(logits, labels):\n",
    "    \"\"\"Calculate accuracy\n",
    "    :param logits: tensor(N, C) Prediction probability, N is the number of samples, C is the number of categories\n",
    "    :param labels: tensor(N) correct label\n",
    "    :return: float accuracy\n",
    "    \"\"\"\n",
    "    return torch.sum(torch.argmax(logits, dim=1) == labels).item() * 1.0 / len(labels)\n",
    "\n",
    "\n",
    "def micro_macro_f1_score(logits, labels):\n",
    "    \"\"\"Calculate Micro-F1 and Macro-F1 scores\n",
    "    :param logits: tensor(N, C) Prediction probability, N is the number of samples, C is the number of categories\n",
    "    :param labels: tensor(N) \n",
    "    Macro-average precision score can be defined as the arithmetic mean of all the precision scores of different classes.\n",
    "    \"\"\"\n",
    "    prediction = torch.argmax(logits, dim=1).long().numpy()\n",
    "    labels = labels.numpy()\n",
    "    micro_f1 = f1_score(labels, prediction, average='micro')\n",
    "    macro_f1 = f1_score(labels, prediction, average='macro')\n",
    "    return micro_f1, macro_f1\n",
    "\n",
    "def split_idx(samples, train_size, val_size, random_state=None):\n",
    "    \"\"\"The samples are divided into training set, test set and validation set, which must be satisfied (represented by floating point numbers):\n",
    "    * 0 < train_size < 1\n",
    "    * 0 < val_size < 1\n",
    "    * train_size + val_size < 1\n",
    "    \"\"\"\n",
    "    train, val = train_test_split(samples, train_size=train_size, random_state=random_state)\n",
    "    if isinstance(val_size, float):\n",
    "        val_size *= len(samples) / len(val)\n",
    "    val, test = train_test_split(val, train_size=val_size, random_state=random_state)\n",
    "    return train, val, test\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
