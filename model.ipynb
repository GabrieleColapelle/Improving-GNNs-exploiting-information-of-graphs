{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a86fd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import itertools\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "import dgl\n",
    "from dgl.nn import HeteroGraphConv\n",
    "from dgl.data import citation_graph, rdf, knowledge_graph\n",
    "from dgl.utils import extract_node_subframes, set_new_frames\n",
    "import dgl.function as fn\n",
    "from dgl.ops import edge_softmax\n",
    "from dgl.utils import expand_as_pair\n",
    "from dgl.data import DGLDataset\n",
    "from dgl.data.utils import _get_dgl_url, download, save_graphs, load_graphs, \\\n",
    "    generate_mask_tensor, idx2mask, makedirs\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import f1_score, normalized_mutual_info_score, adjusted_rand_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bffba915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "\"\"\"Heterogeneous Graph Transformer (HGT)\n",
    "论文链接：https://arxiv.org/pdf/2003.01332\n",
    "\"\"\"\n",
    "import math\n",
    "\n",
    "import dgl.function as fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import HeteroGraphConv\n",
    "from dgl.ops import edge_softmax\n",
    "from dgl.utils import expand_as_pair\n",
    "\n",
    "\n",
    "class HGTAttention(nn.Module):\n",
    "    \"\"\"HGT attention module\n",
    "        :param out_dim: int output feature dimension\n",
    "        :param num_heads: int Number of attention heads K\n",
    "        :param k_linear: nn.Linear(d_in, d_out)\n",
    "        :param q_linear: nn.Linear(d_in, d_out)\n",
    "        :param v_linear: nn.Linear(d_in, d_out)\n",
    "        :param w_att: tensor(K, d_out/K, d_out/K)\n",
    "        :param w_msg: tensor(K, d_out/K, d_out/K)\n",
    "        :param mu: tensor(1)\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, out_dim, num_heads, k_linear, q_linear, v_linear, w_att, w_msg, mu):\n",
    "   \n",
    "        super().__init__()\n",
    "        self.out_dim = out_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = out_dim // num_heads\n",
    "        self.k_linear = k_linear\n",
    "        self.q_linear = q_linear\n",
    "        self.v_linear = v_linear\n",
    "        self.w_att = w_att\n",
    "        self.w_msg = w_msg\n",
    "        self.mu = mu\n",
    "\n",
    "    def forward(self, g, feat):\n",
    "        \"\"\"\n",
    "        :param g: DGLGraph bipartite graph (contains only one relation)\n",
    "        :param feat: tensor(N_src, d_in) or (tensor(N_src, d_in), tensor(N_dst, d_in)) input feature\n",
    "        :return: tensor(N_dst, d_out) The target vertex's representation of the relationship\n",
    "        \"\"\"\n",
    "        \n",
    "        #avoids changing the graph features when exiting the function.\n",
    "        with g.local_scope():\n",
    "            feat_src, feat_dst = expand_as_pair(feat, g)\n",
    "            # (N_src, d_in) -> (N_src, d_out) -> (N_src, K, d_out/K)\n",
    "            k = self.k_linear(feat_src).view(-1, self.num_heads, self.d_k)\n",
    "            v = self.v_linear(feat_src).view(-1, self.num_heads, self.d_k)\n",
    "            q = self.q_linear(feat_dst).view(-1, self.num_heads, self.d_k)\n",
    "\n",
    "            # k[:, h] @= w_att[h] => k[n, h, j] = ∑(i) k[n, h, i] * w_att[h, i, j]\n",
    "            k = torch.einsum('nhi,hij->nhj', k, self.w_att)\n",
    "            v = torch.einsum('nhi,hij->nhj', v, self.w_msg)\n",
    "\n",
    "            g.srcdata.update({'k': k, 'v': v})\n",
    "            g.dstdata['q'] = q\n",
    "            g.apply_edges(fn.v_dot_u('q', 'k', 't'))  # g.edata['t']: (E, K, 1)\n",
    "            attn = g.edata.pop('t').squeeze(dim=-1) * self.mu / math.sqrt(self.d_k)\n",
    "            attn = edge_softmax(g, attn)  # (E, K)\n",
    "            g.edata['t'] = attn.unsqueeze(dim=-1)  # (E, K, 1)\n",
    "\n",
    "            g.update_all(fn.u_mul_e('v', 't', 'm'), fn.sum('m', 'h'))\n",
    "            out = g.dstdata['h'].view(-1, self.out_dim)  # (N_dst, d_out)\n",
    "            return out\n",
    "\n",
    "\n",
    "class HGTLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, num_heads, ntypes, etypes, dropout=0.2, use_norm=True):\n",
    "        \"\"\"HGT layer\n",
    "        :param in_dim: int input feature dimension\n",
    "        :param out_dim: int output feature dimension\n",
    "        :param num_heads: int Number of attention heads K\n",
    "        :param ntypes: List[str] list of vertex types\n",
    "        :param etypes: List[(str, str, str)] list of canonical edge types\n",
    "        :param dropout: dropout: float, optional Dropout probability, default is 0.2\n",
    "        :param use_norm: bool, optional whether to use layer normalization, the default is True\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        d_k = out_dim // num_heads\n",
    "        k_linear = {ntype: nn.Linear(in_dim, out_dim) for ntype in ntypes}\n",
    "        q_linear = {ntype: nn.Linear(in_dim, out_dim) for ntype in ntypes}\n",
    "        v_linear = {ntype: nn.Linear(in_dim, out_dim) for ntype in ntypes}\n",
    "        w_att = {r[1]: nn.Parameter(torch.Tensor(num_heads, d_k, d_k)) for r in etypes}\n",
    "        w_msg = {r[1]: nn.Parameter(torch.Tensor(num_heads, d_k, d_k)) for r in etypes}\n",
    "        mu = {r[1]: nn.Parameter(torch.ones(num_heads)) for r in etypes}\n",
    "        self.reset_parameters(w_att, w_msg)\n",
    "        self.conv = HeteroGraphConv({\n",
    "            etype: HGTAttention(\n",
    "                out_dim, num_heads, k_linear[stype], q_linear[dtype], v_linear[stype],\n",
    "                w_att[etype], w_msg[etype], mu[etype]\n",
    "            ) for stype, etype, dtype in etypes\n",
    "        }, 'mean')\n",
    "\n",
    "        self.a_linear = nn.ModuleDict({ntype: nn.Linear(out_dim, out_dim) for ntype in ntypes})\n",
    "        self.skip = nn.ParameterDict({ntype: nn.Parameter(torch.ones(1)) for ntype in ntypes})\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.use_norm = use_norm\n",
    "        if use_norm:\n",
    "            self.norms = nn.ModuleDict({ntype: nn.LayerNorm(out_dim) for ntype in ntypes})\n",
    "\n",
    "    def reset_parameters(self, w_att, w_msg):\n",
    "        for etype in w_att:\n",
    "            nn.init.xavier_uniform_(w_att[etype])\n",
    "            nn.init.xavier_uniform_(w_msg[etype])\n",
    "\n",
    "    def forward(self, g, feats):\n",
    "        \"\"\"\n",
    "        :param g: DGLGraph heterogeneous graph\n",
    "        :param feats: Dict[str, tensor(N_i, d_in)] mapping of vertex types to input vertex features\n",
    "        :return: Dict[str, tensor(N_i, d_out)] mapping of vertex types to output features\n",
    "        \"\"\"\n",
    "        if g.is_block:\n",
    "            feats_dst = {ntype: feats[ntype][:g.num_dst_nodes(ntype)] for ntype in feats}\n",
    "        else:\n",
    "            feats_dst = feats\n",
    "        with g.local_scope():\n",
    "            # STEP 1 --> Heterogeneous Mutual Attention + Heterogeneous Messaging + Goal-Related Aggregation\n",
    "            hs = self.conv(g, (feats, feats))  # {ntype: tensor(N_i, d_out)}\n",
    "            # Residual connections\n",
    "            out_feats = {}\n",
    "            for ntype in g.dsttypes:\n",
    "                if g.num_dst_nodes(ntype) == 0:\n",
    "                    continue\n",
    "                alpha = torch.sigmoid(self.skip[ntype])\n",
    "                trans_out = self.drop(self.a_linear[ntype](hs[ntype]))\n",
    "                out = alpha * trans_out + (1 - alpha) * feats_dst[ntype]\n",
    "                out_feats[ntype] = self.norms[ntype](out) if self.use_norm else out\n",
    "            return out_feats\n",
    "\n",
    "\n",
    "class HGT(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, in_dims, hidden_dim, out_dim, num_heads, ntypes, etypes,\n",
    "            predict_ntype, num_layers,num_com, com_dim_emb, dropout=0.2, use_norm=True ): \n",
    "        \n",
    "            #self, in_dims, hidden_dim, out_dim, num_heads, ntypes, etypes,\n",
    "            #predict_ntype, num_layers,num_com, com_dim_emb, dropout=0.2, use_norm=True ):\n",
    "\n",
    "        \"\"\"HGT model\n",
    "        :param in_dims: Dict[str, int] mapping of vertex types to input feature dimensions\n",
    "        :param hidden_dim: int hidden feature dimension\n",
    "        :param out_dim: int output feature dimension\n",
    "        :param num_heads: int Number of attention heads K\n",
    "        :param ntypes: List[str] list of vertex types\n",
    "        :param etypes: List[(str, str, str)] list of canonical edge types\n",
    "        :param predict_ntype: str The type of vertex to be predicted\n",
    "        :param num_layers: int number of layers\n",
    "        :param dropout: dropout: float, optional Dropout probability, default is 0.2\n",
    "        :param use_norm: bool, optional whether to use layer normalization, the default is True\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.predict_ntype = predict_ntype\n",
    "        self.adapt_fcs = nn.ModuleDict({\n",
    "            ntype: nn.Linear(in_dim + com_dim_emb, hidden_dim) for ntype, in_dim in in_dims.items()\n",
    "        })\n",
    "        #create the HGT layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            HGTLayer(hidden_dim, hidden_dim, num_heads, ntypes, etypes, dropout, use_norm)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.predict = nn.Linear(hidden_dim, out_dim)\n",
    "        \n",
    "        #self.dict1,self.dict2 = \n",
    "        \n",
    "        self.embed = nn.Embedding(num_com, com_dim_emb)\n",
    "        #print(\"embedding in model init\")\n",
    "        #print(self.embed(torch.LongTensor([0])))\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, g, feats, com_indexes):\n",
    "        \"\"\"\n",
    "        :param g: DGLGraph heterogeneous graph\n",
    "        :param feats: Dict[str, tensor(N_i, d_in)] mapping of vertex types to input vertex features\n",
    "        :return: tensor(N_i, d_out) The final embedding of the vertex to be predicted\n",
    "        \"\"\"\n",
    "        #embedding_community_DBLP(g,self.embed,self.dict1,self.dict2)\n",
    "        #h = {}\n",
    "        #for ntype in feats:\n",
    "            #h[ntype] = F.gelu(self.adapt_fcs[ntype](g.nodes[ntype].data[\"com\"]))\n",
    "        #for layer in self.layers:\n",
    "            #h = layer(g, h)\n",
    "        \n",
    "        #print(\"embeding in forward\")\n",
    "        #print(self.embed(torch.LongTensor([0])))\n",
    "        \n",
    "        #return self.predict(h[self.predict_ntype])\n",
    "        hs = {}\n",
    "        #tupl = (hg.nodes['paper'].data['feat'], embedding(torch.LongTensor([community])[0]))\n",
    "        #hs = {ntype: F.gelu(self.adapt_fcs[ntype](torch.cat(tupl(feats[ntype] ,self.embed(torch.LongTensor(com_indexes[ntype])))))) for ntype in feats}\n",
    "        for ntype in feats:\n",
    "            tupl = (feats[ntype] ,self.embed(torch.LongTensor(com_indexes[ntype])))\n",
    "            hs[ntype] = F.gelu(self.adapt_fcs[ntype](torch.cat(tupl,1)))\n",
    "        #print(g.nodes['author'].data['com'])\n",
    "        for layer in self.layers:\n",
    "            hs = layer(g, hs)  # {ntype: tensor(N_i, d_hid)}\n",
    "        out = self.predict(hs[self.predict_ntype])  # tensor(N_i, d_out)\n",
    "        #print(self.embed(torch.LongTensor([0])))\n",
    "        return out\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        #com_indexes = dict chiave il tipo di nodo come valore lista ordinata di indici delle comunià\n",
    "        \n",
    "        \n",
    "       \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0352c783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
