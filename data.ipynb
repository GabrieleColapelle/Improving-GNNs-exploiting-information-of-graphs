{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6698ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import itertools\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "\n",
    "import math\n",
    "\n",
    "import dgl\n",
    "from dgl.nn import HeteroGraphConv\n",
    "from dgl.data import citation_graph, rdf, knowledge_graph\n",
    "from dgl.utils import extract_node_subframes, set_new_frames\n",
    "import dgl.function as fn\n",
    "from dgl.ops import edge_softmax\n",
    "from dgl.utils import expand_as_pair\n",
    "from dgl.data import DGLDataset\n",
    "from dgl.data.utils import _get_dgl_url, download, save_graphs, load_graphs, \\\n",
    "    generate_mask_tensor, idx2mask, makedirs\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import f1_score, normalized_mutual_info_score, adjusted_rand_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4b7ed7",
   "metadata": {},
   "source": [
    "#### GET THE ACM DATASET AND CONVERT TO A DGL GRAPH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f9d4bd",
   "metadata": {},
   "source": [
    "The basic DGL dataset for creating graph datasets. This class defines a basic template class for DGL Dataset. The following steps will be executed automatically:\n",
    "\n",
    "Check whether there is a dataset cache on disk (already processed and stored on the disk) by invoking has_cache(). If true, goto 5.\n",
    "\n",
    "1-Call download() to download the data if url is not None.\n",
    "\n",
    "2-Call process() to process the data.\n",
    "\n",
    "3-Call save() to save the processed dataset on disk and goto 6.\n",
    "\n",
    "4-Call load() to load the processed dataset from disk.\n",
    "\n",
    "Done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f922c0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACMDataset(DGLDataset):\n",
    "    \"\"\"ACM dataset, only one heterogeneous graph\n",
    "    Statistical data\n",
    "    -----\n",
    "    * Apex: 17351 author, 4025 paper, 72 field\n",
    "    * Sides: 13407 paper-author, 4025 paper-field\n",
    "    * Number of categories: 3\n",
    "    * paper vertex division: 808 train, 401 valid, 2816 test\n",
    "    Attributes\n",
    "    -----\n",
    "    * num_classes: number of classes\n",
    "    * metapaths: metapaths to use\n",
    "    * predict_ntype: predict vertex type\n",
    "    paper vertex attribute\n",
    "    -----\n",
    "    * feat: tensor(4025, 1903) bag-of-words representation of keywords\n",
    "    * label: tensor(4025)\n",
    "    * train_mask, val_mask, test_mask: tensor(4025)\n",
    "    author vertex attribute\n",
    "    -----\n",
    "    * feat: tensor(17351, 1903) average of associated paper features\n",
    "    field vertex attribute\n",
    "    -----\n",
    "    * feat: tensor(72, 72) one-hot encoding \"\"\"\n",
    "    \n",
    "\n",
    "    def __init__(self):\n",
    "        #Get DGL online url for download.\n",
    "        super().__init__('ACM', _get_dgl_url('dataset/ACM.mat'))\n",
    "\n",
    "    def download(self):\n",
    "        file_path = os.path.join(self.raw_dir, 'ACM.mat')\n",
    "        if not os.path.exists(file_path):\n",
    "            download(self.url, path=file_path)\n",
    "\n",
    "    def save(self):\n",
    "        save_graphs(os.path.join(self.save_path, self.name + '_dgl_graph.bin'), [self.g])\n",
    "\n",
    "    def load(self):\n",
    "        graphs, _ = load_graphs(os.path.join(self.save_path, self.name + '_dgl_graph.bin'))\n",
    "        self.g = graphs[0]\n",
    "        for k in ('train_mask', 'val_mask', 'test_mask'):\n",
    "            self.g.nodes['paper'].data[k] = self.g.nodes['paper'].data[k].bool()\n",
    "\n",
    "    def process(self):\n",
    "        data = sio.loadmat(os.path.join(self.raw_dir, 'ACM.mat'))\n",
    "        p_vs_l = data['PvsL']  # paper-field\n",
    "        p_vs_a = data['PvsA']  # paper-author\n",
    "        p_vs_t = data['PvsT']  # paper-term, bag of words\n",
    "        p_vs_c = data['PvsC']  # paper-conference, labels come from that\n",
    "\n",
    "        # We assign\n",
    "        # (1) KDD papers as class 0 (data mining),\n",
    "        # (2) SIGMOD and VLDB papers as class 1 (database),\n",
    "        # (3) SIGCOMM and MobiCOMM papers as class 2 (communication)\n",
    "        conf_ids = [0, 1, 9, 10, 13]\n",
    "        label_ids = [0, 1, 2, 2, 1]\n",
    "\n",
    "        p_vs_c_filter = p_vs_c[:, conf_ids]\n",
    "        #get indeces of all papers\n",
    "        p_selected = (p_vs_c_filter.sum(1) != 0).A1.nonzero()[0]\n",
    "        p_vs_l = p_vs_l[p_selected]\n",
    "        p_vs_a = p_vs_a[p_selected]\n",
    "        p_vs_t = p_vs_t[p_selected]\n",
    "        p_vs_c = p_vs_c[p_selected]\n",
    "        \n",
    "        #building the graph\n",
    "        self.g = dgl.heterograph({\n",
    "            ('paper', 'pa', 'author'): p_vs_a.nonzero(),\n",
    "            ('author', 'ap', 'paper'): p_vs_a.transpose().nonzero(),\n",
    "            ('paper', 'pf', 'field'): p_vs_l.nonzero(),\n",
    "            ('field', 'fp', 'paper'): p_vs_l.transpose().nonzero(),\n",
    "            ('author', 'aa', 'author') : ((),()),\n",
    "            ('paper', 'pp', 'paper') : ((),()),\n",
    "        })\n",
    "        #the features of a paper are the bag of words associated to a paper\n",
    "        paper_features = torch.FloatTensor(p_vs_t.toarray())  # (4025, 1903)\n",
    "        \n",
    "        #get indces and labels\n",
    "        pc_p, pc_c = p_vs_c.nonzero()\n",
    "        paper_labels = np.zeros(len(p_selected), dtype=np.int64)\n",
    "        for conf_id, label_id in zip(conf_ids, label_ids):\n",
    "            paper_labels[pc_p[pc_c == conf_id]] = label_id\n",
    "        paper_labels = torch.from_numpy(paper_labels)\n",
    "\n",
    "        float_mask = np.zeros(len(pc_p))\n",
    "        for conf_id in conf_ids:\n",
    "            pc_c_mask = (pc_c == conf_id)\n",
    "            float_mask[pc_c_mask] = np.random.permutation(np.linspace(0, 1, pc_c_mask.sum()))\n",
    "        train_idx = np.where(float_mask <= 0.2)[0]\n",
    "        val_idx = np.where((float_mask > 0.2) & (float_mask <= 0.3))[0]\n",
    "        test_idx = np.where(float_mask > 0.3)[0]\n",
    "\n",
    "        num_paper_nodes = self.g.num_nodes('paper')\n",
    "        train_mask = generate_mask_tensor(idx2mask(train_idx, num_paper_nodes))\n",
    "        val_mask = generate_mask_tensor(idx2mask(val_idx, num_paper_nodes))\n",
    "        test_mask = generate_mask_tensor(idx2mask(test_idx, num_paper_nodes))\n",
    "\n",
    "        self.g.nodes['paper'].data['feat'] = paper_features\n",
    "        self.g.nodes['paper'].data['label'] = paper_labels\n",
    "        self.g.nodes['paper'].data['train_mask'] = train_mask\n",
    "        self.g.nodes['paper'].data['val_mask'] = val_mask\n",
    "        self.g.nodes['paper'].data['test_mask'] = test_mask\n",
    "        # The feature of the author vertex is the average of the features of its associated paper vertex\n",
    "        self.g.multi_update_all({'pa': (fn.copy_u('feat', 'm'), fn.mean('m', 'feat'))}, 'sum')\n",
    "        #self.g.nodes['field'].data['feat'] = torch.eye(self.g.num_nodes('field'))\n",
    "        self.g.nodes['field'].data['feat'] = torch.eye(self.g.num_nodes('field'))\n",
    "        self.g.nodes['author'].data['feat'] = torch.ones(self.g.num_nodes('author'), 16)\n",
    "        print(\"-\"*100)\n",
    "\n",
    "    #def has_cache(self):\n",
    "        #return os.path.exists(os.path.join(self.save_path, self.name + '_dgl_graph.bin'))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx != 0:\n",
    "            raise IndexError('This dataset has only one graph')\n",
    "        return self.g\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return 3\n",
    "\n",
    "    @property\n",
    "    def metapaths(self):\n",
    "        return [['pa', 'ap'], ['pf', 'fp']]\n",
    "\n",
    "    @property\n",
    "    def predict_ntype(self):\n",
    "        return 'paper'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "061c0613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      color      director_name  num_critic_for_reviews  duration  \\\n",
      "0     Color      James Cameron                   723.0     178.0   \n",
      "1     Color     Gore Verbinski                   302.0     169.0   \n",
      "2     Color         Sam Mendes                   602.0     148.0   \n",
      "3     Color  Christopher Nolan                   813.0     164.0   \n",
      "4       NaN        Doug Walker                     NaN       NaN   \n",
      "...     ...                ...                     ...       ...   \n",
      "4927  Color       Edward Burns                    14.0      95.0   \n",
      "4928  Color        Scott Smith                     1.0      87.0   \n",
      "4929  Color   Benjamin Roberds                    13.0      76.0   \n",
      "4930  Color        Daniel Hsia                    14.0     100.0   \n",
      "4931  Color           Jon Gunn                    43.0      90.0   \n",
      "\n",
      "      director_facebook_likes  actor_3_facebook_likes        actor_2_name  \\\n",
      "0                         0.0                   855.0    Joel David Moore   \n",
      "1                       563.0                  1000.0       Orlando Bloom   \n",
      "2                         0.0                   161.0        Rory Kinnear   \n",
      "3                     22000.0                 23000.0      Christian Bale   \n",
      "4                       131.0                     NaN          Rob Walker   \n",
      "...                       ...                     ...                 ...   \n",
      "4927                      0.0                   133.0  Caitlin FitzGerald   \n",
      "4928                      2.0                   318.0       Daphne Zuniga   \n",
      "4929                      0.0                     0.0       Maxwell Moody   \n",
      "4930                      0.0                   489.0       Daniel Henney   \n",
      "4931                     16.0                    16.0    Brian Herzlinger   \n",
      "\n",
      "      actor_1_facebook_likes        gross                           genres  \\\n",
      "0                     1000.0  760505847.0  Action|Adventure|Fantasy|Sci-Fi   \n",
      "1                    40000.0  309404152.0         Action|Adventure|Fantasy   \n",
      "2                    11000.0  200074175.0        Action|Adventure|Thriller   \n",
      "3                    27000.0  448130642.0                  Action|Thriller   \n",
      "4                      131.0          NaN                      Documentary   \n",
      "...                      ...          ...                              ...   \n",
      "4927                   296.0       4584.0                     Comedy|Drama   \n",
      "4928                   637.0          NaN                     Comedy|Drama   \n",
      "4929                     0.0          NaN            Drama|Horror|Thriller   \n",
      "4930                   946.0      10443.0             Comedy|Drama|Romance   \n",
      "4931                    86.0      85222.0                      Documentary   \n",
      "\n",
      "      ... num_user_for_reviews language  country  content_rating       budget  \\\n",
      "0     ...               3054.0  English      USA           PG-13  237000000.0   \n",
      "1     ...               1238.0  English      USA           PG-13  300000000.0   \n",
      "2     ...                994.0  English       UK           PG-13  245000000.0   \n",
      "3     ...               2701.0  English      USA           PG-13  250000000.0   \n",
      "4     ...                  NaN      NaN      NaN             NaN          NaN   \n",
      "...   ...                  ...      ...      ...             ...          ...   \n",
      "4927  ...                 14.0  English      USA       Not Rated       9000.0   \n",
      "4928  ...                  6.0  English   Canada             NaN          NaN   \n",
      "4929  ...                  3.0  English      USA             NaN       1400.0   \n",
      "4930  ...                  9.0  English      USA           PG-13          NaN   \n",
      "4931  ...                 84.0  English      USA              PG       1100.0   \n",
      "\n",
      "      title_year actor_2_facebook_likes imdb_score  aspect_ratio  \\\n",
      "0         2009.0                  936.0        7.9          1.78   \n",
      "1         2007.0                 5000.0        7.1          2.35   \n",
      "2         2015.0                  393.0        6.8          2.35   \n",
      "3         2012.0                23000.0        8.5          2.35   \n",
      "4            NaN                   12.0        7.1           NaN   \n",
      "...          ...                    ...        ...           ...   \n",
      "4927      2011.0                  205.0        6.4           NaN   \n",
      "4928      2013.0                  470.0        7.7           NaN   \n",
      "4929      2013.0                    0.0        6.3           NaN   \n",
      "4930      2012.0                  719.0        6.3          2.35   \n",
      "4931      2004.0                   23.0        6.6          1.85   \n",
      "\n",
      "     movie_facebook_likes  \n",
      "0                   33000  \n",
      "1                       0  \n",
      "2                   85000  \n",
      "3                  164000  \n",
      "4                       0  \n",
      "...                   ...  \n",
      "4927                  413  \n",
      "4928                   84  \n",
      "4929                   16  \n",
      "4930                  660  \n",
      "4931                  456  \n",
      "\n",
      "[4932 rows x 28 columns]\n",
      "       num_critic_for_reviews     duration  director_facebook_likes  \\\n",
      "count             4888.000000  4919.000000              4932.000000   \n",
      "mean               142.711538   108.159382               687.449311   \n",
      "std                121.617362    22.578583              2815.213712   \n",
      "min                  1.000000     7.000000                 0.000000   \n",
      "25%                 53.000000    94.000000                 7.000000   \n",
      "50%                112.000000   104.000000                49.000000   \n",
      "75%                197.000000   118.000000               195.500000   \n",
      "max                813.000000   330.000000             23000.000000   \n",
      "\n",
      "       actor_3_facebook_likes  actor_1_facebook_likes         gross  \\\n",
      "count              4919.00000             4932.000000  4.152000e+03   \n",
      "mean                651.20553             6668.287713  4.854980e+07   \n",
      "std                1681.08616            15150.446402  6.848194e+07   \n",
      "min                   0.00000                0.000000  1.620000e+02   \n",
      "25%                 133.00000              618.000000  5.377819e+06   \n",
      "50%                 372.00000              991.500000  2.556371e+07   \n",
      "75%                 637.00000            11000.000000  6.234110e+07   \n",
      "max               23000.00000           640000.000000  7.605058e+08   \n",
      "\n",
      "       num_voted_users  cast_total_facebook_likes  facenumber_in_poster  \\\n",
      "count     4.932000e+03                4932.000000           4919.000000   \n",
      "mean      8.491599e+04                9854.138686              1.365725   \n",
      "std       1.396285e+05               18326.222891              2.013732   \n",
      "min       5.000000e+00                   0.000000              0.000000   \n",
      "25%       8.998000e+03                1430.750000              0.000000   \n",
      "50%       3.504150e+04                3133.000000              1.000000   \n",
      "75%       9.799350e+04               14030.000000              2.000000   \n",
      "max       1.689764e+06              656730.000000             43.000000   \n",
      "\n",
      "       num_user_for_reviews        budget   title_year  \\\n",
      "count           4914.000000  4.537000e+03  4928.000000   \n",
      "mean             276.882173  3.986040e+07  2002.462256   \n",
      "std              380.777408  2.064230e+08    12.480022   \n",
      "min                1.000000  2.180000e+02  1916.000000   \n",
      "25%               68.000000  6.000000e+06  1999.000000   \n",
      "50%              160.000000  2.000000e+07  2005.000000   \n",
      "75%              331.750000  4.500000e+07  2011.000000   \n",
      "max             5060.000000  1.221550e+10  2016.000000   \n",
      "\n",
      "       actor_2_facebook_likes   imdb_score  aspect_ratio  movie_facebook_likes  \n",
      "count             4928.000000  4932.000000   4625.000000           4932.000000  \n",
      "mean              1675.850852     6.418289      2.129250           7598.097932  \n",
      "std               4080.170643     1.115312      0.789375          19459.191923  \n",
      "min                  0.000000     1.600000      1.180000              0.000000  \n",
      "25%                281.750000     5.800000      1.850000              0.000000  \n",
      "50%                598.000000     6.500000      2.350000            165.500000  \n",
      "75%                920.000000     7.200000      2.350000           3000.000000  \n",
      "max             137000.000000     9.500000     16.000000         349000.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Jhy1993/HAN/master/data/imdb/movie_metadata.csv', encoding='utf8') \\\n",
    "            .dropna(axis=0, subset=['actor_1_name', 'director_name']).reset_index(drop=True)\n",
    "\n",
    "print(df)\n",
    "\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a65f9b5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DGLDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIMDbDataset\u001b[39;00m(\u001b[43mDGLDataset\u001b[49m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124;03m\"\"\"IMDb movie dataset, only one heterogeneous graph\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Statistical data\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    -----\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    * feat: tensor(2081, 1299) average of associated movie features\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     _url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/Jhy1993/HAN/master/data/imdb/movie_metadata.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DGLDataset' is not defined"
     ]
    }
   ],
   "source": [
    "class IMDbDataset(DGLDataset):\n",
    "    \"\"\"IMDb movie dataset, only one heterogeneous graph\n",
    "    Statistical data\n",
    "    -----\n",
    "    * Apex: 4278 movies, 5257 actors, 2081 directors\n",
    "    * Sides: 12828 movie-actor, 4278 movie-director\n",
    "    * Number of categories: 3\n",
    "    * Movie vertex division: 400 train, 400 valid, 3478 test\n",
    "    Attributes\n",
    "    -----\n",
    "    * num_classes: number of classes\n",
    "    * metapaths: metapaths to use\n",
    "    * predict_ntype: predict vertex type\n",
    "    \n",
    "    \n",
    "    movie vertex attribute\n",
    "    -----\n",
    "    * feat: tensor(4278, 1299) bag-of-words representation of plot keywords\n",
    "    * label: tensor(4278) 0: Action, 1: Comedy, 2: Drama\n",
    "    * train_mask, val_mask, test_mask: tensor(4278)\n",
    "    actor vertex attributes\n",
    "    \n",
    "    \n",
    "    -----\n",
    "    * feat: tensor(5257, 1299) average of associated movie features\n",
    "    \n",
    "    \n",
    "    director vertex attribute\n",
    "    -----\n",
    "    * feat: tensor(2081, 1299) average of associated movie features\n",
    "    \"\"\"\n",
    "    _url = 'https://raw.githubusercontent.com/Jhy1993/HAN/master/data/imdb/movie_metadata.csv'\n",
    "    _seed = 42\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__('imdb', self._url)\n",
    "\n",
    "    def download(self):\n",
    "        file_path = os.path.join(self.raw_dir, 'imdb.csv')\n",
    "        if not os.path.exists(file_path):\n",
    "            download(self.url, path=file_path)\n",
    "\n",
    "    def save(self):\n",
    "        save_graphs(os.path.join(self.save_path, self.name + '_dgl_graph.bin'), [self.g])\n",
    "\n",
    "    def load(self):\n",
    "        graphs, _ = load_graphs(os.path.join(self.save_path, self.name + '_dgl_graph.bin'))\n",
    "        self.g = graphs[0]\n",
    "        for k in ('train_mask', 'val_mask', 'test_mask'):\n",
    "            self.g.nodes['movie'].data[k] = self.g.nodes['movie'].data[k].bool()\n",
    "\n",
    "    def process(self):\n",
    "        self.data = pd.read_csv(os.path.join(self.raw_dir, 'imdb.csv'), encoding='utf8') \\\n",
    "            .dropna(axis=0, subset=['actor_1_name', 'director_name']).reset_index(drop=True)\n",
    "        self.labels = self._extract_labels()\n",
    "        self.movies = list(sorted(m.strip() for m in self.data['movie_title']))\n",
    "        self.directors = list(sorted(set(self.data['director_name'])))\n",
    "        self.actors = list(sorted(set(itertools.chain.from_iterable(\n",
    "            self.data[c].dropna().to_list()\n",
    "            for c in ('actor_1_name', 'actor_2_name', 'actor_3_name')\n",
    "        ))))\n",
    "        self.g = self._build_graph()\n",
    "        self._add_ndata()\n",
    "        return self.data\n",
    "\n",
    "    def _extract_labels(self):\n",
    "        labels = np.full(len(self.data), -1)\n",
    "        for i, genres in self.data['genres'].iteritems():\n",
    "            for genre in genres.split('|'):\n",
    "                if genre == 'Action':\n",
    "                    labels[i] = 0\n",
    "                    break\n",
    "                elif genre == 'Comedy':\n",
    "                    labels[i] = 1\n",
    "                    break\n",
    "                elif genre == 'Drama':\n",
    "                    labels[i] = 2\n",
    "                    break\n",
    "        other_idx = np.where(labels == -1)[0]\n",
    "        self.data = self.data.drop(other_idx).reset_index(drop=True)\n",
    "        return np.delete(labels, other_idx, axis=0)\n",
    "\n",
    "    def _build_graph(self):\n",
    "        ma, md = set(), set()\n",
    "        for m, row in self.data.iterrows():\n",
    "            d = self.directors.index(row['director_name'])\n",
    "            md.add((m, d))\n",
    "            for c in ('actor_1_name', 'actor_2_name', 'actor_3_name'):\n",
    "                if row[c] in self.actors:\n",
    "                    a = self.actors.index(row[c])\n",
    "                    ma.add((m, a))\n",
    "        ma, md = list(ma), list(md)\n",
    "        ma_m, ma_a = [e[0] for e in ma], [e[1] for e in ma]\n",
    "        md_m, md_d = [e[0] for e in md], [e[1] for e in md]\n",
    "        return dgl.heterograph({\n",
    "            ('movie', 'ma', 'actor'): (ma_m, ma_a),\n",
    "            ('actor', 'am', 'movie'): (ma_a, ma_m),\n",
    "            ('movie', 'md', 'director'): (md_m, md_d),\n",
    "            ('director', 'dm', 'movie'): (md_d, md_m),\n",
    "            ('actor', 'aa', 'actor') : ((),()),\n",
    "            ('movie', 'mm', 'movie') : ((),()),\n",
    "            ('director', 'dd', 'director') : ((),()),\n",
    "        })\n",
    "\n",
    "    def _add_ndata(self):\n",
    "        vectorizer = CountVectorizer(min_df=5)\n",
    "        features = vectorizer.fit_transform(self.data['plot_keywords'].fillna('').values)\n",
    "        self.g.nodes['movie'].data['feat'] = torch.from_numpy(features.toarray()).float()\n",
    "        self.g.nodes['movie'].data['label'] = torch.from_numpy(self.labels).long()\n",
    "\n",
    "        #Actor and director vertex features are the average of their associated movie vertex features\n",
    "        #self.g.multi_update_all({\n",
    "            #'ma': (fn.copy_u('feat', 'm'), fn.mean('m', 'feat')),\n",
    "            #'md': (fn.copy_u('feat', 'm'), fn.mean('m', 'feat'))\n",
    "        #}, 'sum')\n",
    "\n",
    "        n_movies = len(self.movies)\n",
    "        train_idx, val_idx, test_idx = split_idx(np.arange(n_movies), 400, 400, self._seed)\n",
    "        self.g.nodes['movie'].data['train_mask'] = generate_mask_tensor(idx2mask(train_idx, n_movies))\n",
    "        self.g.nodes['movie'].data['val_mask'] = generate_mask_tensor(idx2mask(val_idx, n_movies))\n",
    "        self.g.nodes['movie'].data['test_mask'] = generate_mask_tensor(idx2mask(test_idx, n_movies))\n",
    "        self.g.nodes['actor'].data['feat'] = torch.ones(5257,16)\n",
    "        self.g.nodes['director'].data['feat'] = torch.ones(2081,16)\n",
    "\n",
    "    #def has_cache(self):\n",
    "        #return os.path.exists(os.path.join(self.save_path, self.name + '_dgl_graph.bin'))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx != 0:\n",
    "            raise IndexError('This dataset has only one graph')\n",
    "        return self.g\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return 3\n",
    "\n",
    "    @property\n",
    "    def metapaths(self):\n",
    "        return [['ma', 'am'], ['md', 'dm']]\n",
    "\n",
    "    @property\n",
    "    def predict_ntype(self):\n",
    "        return 'movie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61e90c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stopwords\n",
    "nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37ef312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBLPFourAreaDataset(DGLDataset):\n",
    "    \"\"\"4-domain DBLP academic network dataset, only one heterogeneous graph\n",
    "    Statistical data\n",
    "    -----\n",
    "    * Apex: 4057 author, 14328 paper, 20 conf, 7723 term\n",
    "    * Sides: 19645 paper-author, 14328 paper-conf, 85810 paper-term\n",
    "    * Number of categories: 4\n",
    "    * author vertex division: 800 train, 400 valid, 2857 test\n",
    "    Attributes\n",
    "    -----\n",
    "    * num_classes: number of classes\n",
    "    * metapaths: metapaths to use\n",
    "    * predict_ntype: predict vertex type\n",
    "    \n",
    "    author vertex attribute\n",
    "    -----\n",
    "    * feat: tensor(4057, 334), bag-of-words representation of keywords (from dataset preprocessed by HAN authors)\n",
    "    * label: tensor(4057), 0: DB, 1: DM, 2: AI, 3: IR\n",
    "    * train_mask, val_mask, test_mask: tensor(4057)\n",
    "    conf vertex attribute\n",
    "    -----\n",
    "    * label: tensor(20), the category is 0~3\n",
    "    \"\"\"\n",
    "   \n",
    "    _url = 'https://raw.githubusercontent.com/Jhy1993/HAN/master/data/DBLP_four_area/'\n",
    "    _url2 = 'https://pan.baidu.com/s/1Qr2e97MofXsBhUvQqgJqDg'\n",
    "    _raw_files = [\n",
    "        'readme.txt', 'author_label.txt', 'paper.txt', 'conf_label.txt', 'term.txt',\n",
    "        'paper_author.txt', 'paper_conf.txt', 'paper_term.txt'\n",
    "    ]\n",
    "    _seed = 42\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__('DBLP_four_area', self._url)\n",
    "\n",
    "    def download(self):\n",
    "        if not os.path.exists(self.raw_path):\n",
    "            makedirs(self.raw_path)\n",
    "        for file in self._raw_files:\n",
    "            download(self.url + file, os.path.join(self.raw_path, file))\n",
    "\n",
    "    def save(self):\n",
    "        save_graphs(os.path.join(self.save_path, self.name + '_dgl_graph.bin'), [self.g])\n",
    "\n",
    "    def load(self):\n",
    "        graphs, _ = load_graphs(os.path.join(self.save_path, self.name + '_dgl_graph.bin'))\n",
    "        self.g = graphs[0]\n",
    "        for k in ('train_mask', 'val_mask', 'test_mask'):\n",
    "            self.g.nodes['author'].data[k] = self.g.nodes['author'].data[k].bool()\n",
    "\n",
    "    def process(self):\n",
    "        self.authors, self.papers, self.confs, self.terms, \\\n",
    "            self.paper_author, self.paper_conf, self.paper_term = self._read_raw_data()\n",
    "        self._filter_nodes_and_edges()\n",
    "        self._lemmatize_terms()\n",
    "        self._remove_stopwords()\n",
    "        self._reset_index()\n",
    "\n",
    "\n",
    "        self.g = self._build_graph()\n",
    "        self._add_ndata()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _read_raw_data(self):\n",
    "        authors = self._read_file('author_label.txt', names=['id', 'label', 'name'], index_col='id')\n",
    "        papers = self._read_file('paper.txt', names=['id', 'title'], index_col='id', encoding='cp1252')\n",
    "        confs = self._read_file('conf_label.txt', names=['id', 'label', 'name', 'dummy'], index_col='id')\n",
    "        terms = self._read_file('term.txt', names=['id', 'name'], index_col='id')\n",
    "        paper_author = self._read_file('paper_author.txt', names=['paper_id', 'author_id'])\n",
    "        paper_conf = self._read_file('paper_conf.txt', names=['paper_id', 'conf_id'])\n",
    "        paper_term = self._read_file('paper_term.txt', names=['paper_id', 'term_id'])\n",
    "        return authors, papers, confs, terms, paper_author, paper_conf, paper_term\n",
    "\n",
    "    def _read_file(self, filename, names, index_col=None, encoding='utf8'):\n",
    "        return pd.read_csv(\n",
    "            os.path.join(self.raw_path, filename), sep='\\t', names=names, index_col=index_col,\n",
    "            keep_default_na=False, encoding=encoding\n",
    "        )\n",
    "\n",
    "    def _filter_nodes_and_edges(self):\n",
    "        \"\"\"Filter out vertices and edges not associated with scholars\"\"\"\n",
    "        self.paper_author = self.paper_author[self.paper_author['author_id'].isin(self.authors.index)]\n",
    "        paper_ids = self.paper_author['paper_id'].drop_duplicates()\n",
    "        self.papers = self.papers.loc[paper_ids]\n",
    "        self.paper_conf = self.paper_conf[self.paper_conf['paper_id'].isin(paper_ids)]\n",
    "        self.paper_term = self.paper_term[self.paper_term['paper_id'].isin(paper_ids)]\n",
    "        self.terms = self.terms.loc[self.paper_term['term_id'].drop_duplicates()]\n",
    "\n",
    "    def _lemmatize_terms(self):\n",
    "        \"\"\"Lemmatization of keywords and deduplication\"\"\"\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemma_id_map, term_lemma_map = {}, {}\n",
    "        for index, row in self.terms.iterrows():\n",
    "            lemma = lemmatizer.lemmatize(row['name'])\n",
    "            term_lemma_map[index] = lemma_id_map.setdefault(lemma, index)\n",
    "        self.terms = pd.DataFrame(\n",
    "            list(lemma_id_map.keys()), columns=['name'],\n",
    "            index=pd.Index(lemma_id_map.values(), name='id')\n",
    "        )\n",
    "        self.paper_term.loc[:, 'term_id'] = [\n",
    "            term_lemma_map[row['term_id']] for _, row in self.paper_term.iterrows()\n",
    "        ]\n",
    "        self.paper_term.drop_duplicates(inplace=True)\n",
    "        \n",
    "\n",
    "    def _remove_stopwords(self):\n",
    "        \"\"\"Remove stop words in keywords\"\"\"\n",
    "        stop_words = sklearn_stopwords.union(nltk_stopwords.words('english'))\n",
    "        self.terms = self.terms[~(self.terms['name'].isin(stop_words))]\n",
    "        self.paper_term = self.paper_term[self.paper_term['term_id'].isin(self.terms.index)]\n",
    "\n",
    "    def _reset_index(self):\n",
    "        \"\"\"Reset vertex id to 0~n-1\"\"\"\n",
    "        self.authors.reset_index(inplace=True)\n",
    "        self.papers.reset_index(inplace=True)\n",
    "        self.confs.reset_index(inplace=True)\n",
    "        self.terms.reset_index(inplace=True)\n",
    "        author_id_map = {row['id']: index for index, row in self.authors.iterrows()}\n",
    "        paper_id_map = {row['id']: index for index, row in self.papers.iterrows()}\n",
    "        conf_id_map = {row['id']: index for index, row in self.confs.iterrows()}\n",
    "        term_id_map = {row['id']: index for index, row in self.terms.iterrows()}\n",
    "\n",
    "        self.paper_author.loc[:, 'author_id'] = [author_id_map[i] for i in self.paper_author['author_id'].to_list()]\n",
    "        self.paper_conf.loc[:, 'conf_id'] = [conf_id_map[i] for i in self.paper_conf['conf_id'].to_list()]\n",
    "        self.paper_term.loc[:, 'term_id'] = [term_id_map[i] for i in self.paper_term['term_id'].to_list()]\n",
    "        for df in (self.paper_author, self.paper_conf, self.paper_term):\n",
    "            df.loc[:, 'paper_id'] = [paper_id_map[i] for i in df['paper_id']]\n",
    "\n",
    "    def _build_graph(self):\n",
    "        pa_p, pa_a = self.paper_author['paper_id'].to_list(), self.paper_author['author_id'].to_list()\n",
    "        pc_p, pc_c = self.paper_conf['paper_id'].to_list(), self.paper_conf['conf_id'].to_list()\n",
    "        pt_p, pt_t = self.paper_term['paper_id'].to_list(), self.paper_term['term_id'].to_list()\n",
    "        \n",
    "        return dgl.heterograph({\n",
    "            ('paper', 'pa', 'author'): (pa_p, pa_a),\n",
    "            ('author', 'ap', 'paper'): (pa_a, pa_p),\n",
    "            ('paper', 'pc', 'conf'): (pc_p, pc_c),\n",
    "            ('conf', 'cp', 'paper'): (pc_c, pc_p),\n",
    "            ('paper', 'pt', 'term'): (pt_p, pt_t),\n",
    "            ('term', 'tp', 'paper'): (pt_t, pt_p),\n",
    "            ('author', 'aa', 'author') : ((),()),\n",
    "        })\n",
    "\n",
    "    def _add_ndata(self):\n",
    "        _raw_file2 = os.path.join(self.raw_dir, 'DBLP4057_GAT_with_idx.mat')\n",
    "        if not os.path.exists(_raw_file2):\n",
    "            raise FileNotFoundError('Please manually download the file {} Extraction code: 6b3h and save it to {}'.format(\n",
    "                self._url2, _raw_file2))\n",
    "        mat = sio.loadmat(_raw_file2)\n",
    "        self.g.nodes['author'].data['feat'] = torch.from_numpy(mat['features']).float()\n",
    "        self.g.nodes['author'].data['label'] = torch.tensor(self.authors['label'].to_list())\n",
    "\n",
    "        n_authors = len(self.authors)\n",
    "       \n",
    "        \n",
    "        train_idx, val_idx, test_idx = split_idx(np.arange(n_authors), 800, 400, self._seed)\n",
    "        self.g.nodes['author'].data['train_mask'] = generate_mask_tensor(idx2mask(train_idx, n_authors))\n",
    "        self.g.nodes['author'].data['val_mask'] = generate_mask_tensor(idx2mask(val_idx, n_authors))\n",
    "        self.g.nodes['author'].data['test_mask'] = generate_mask_tensor(idx2mask(test_idx, n_authors))\n",
    "    \n",
    "        self.g.nodes['conf'].data['label'] = torch.tensor(self.confs['label'].to_list())\n",
    "        self.g.nodes['conf'].data['feat'] = torch.ones(20, 16)\n",
    "        self.g.nodes['term'].data['feat'] = torch.ones(7723, 16)\n",
    "        self.g.nodes['paper'].data['feat'] = torch.ones(14328, 16)\n",
    "        \n",
    "        #self.g.nodes['term'].data['feat'] = torch.ones(7723, 16)\n",
    "        #self.g.multi_update_all({\n",
    "            #'ap': (fn.copy_u('feat', 'm'), fn.mean('m', 'feat')),\n",
    "        #}, 'sum')\n",
    "        \n",
    "        #self.g.multi_update_all({\n",
    "            #'pt': (fn.copy_u('feat', 'm'), fn.mean('m', 'feat')),\n",
    "        #}, 'sum')\n",
    "        \n",
    "        #self.g.multi_update_all({\n",
    "            #'pc': (fn.copy_u('feat', 'm'), fn.mean('m', 'feat')),\n",
    "        #}, 'sum')\n",
    "\n",
    "        \n",
    "\n",
    "    #def has_cache(self):\n",
    "        #return os.path.exists(os.path.join(self.save_path, self.name + '_dgl_graph.bin'))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx != 0:\n",
    "            raise IndexError('This dataset has only one graph')\n",
    "        return self.g\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return 4\n",
    "\n",
    "    @property\n",
    "    def metapaths(self):\n",
    "        return [['ap', 'pa'], ['ap', 'pc', 'cp', 'pa'], ['ap', 'pt', 'tp', 'pa']]\n",
    "\n",
    "    @property\n",
    "    def predict_ntype(self):\n",
    "        return 'author'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903aa71d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
