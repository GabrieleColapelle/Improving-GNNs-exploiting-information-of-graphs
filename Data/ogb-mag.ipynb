{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4MkK2ct4VKj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import argparse\n",
    "import warnings\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import logging\n",
    "\n",
    "\n",
    "import dgl\n",
    "from dgl import AddReverse\n",
    "\n",
    "from ogb.nodeproppred import DglNodePropPredDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6haU6vF_OkQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, normalized_mutual_info_score, adjusted_rand_score\n",
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    dgl.seed(seed)\n",
    "    \n",
    "    \n",
    "def accuracy(logits, labels):\n",
    "    \"\"\"Calculate accuracy\n",
    "    :param logits: tensor(N, C) Prediction probability, N is the number of samples, C is the number of categories\n",
    "    :param labels: tensor(N) correct label\n",
    "    :return: float accuracy\n",
    "    \"\"\"\n",
    "    return torch.sum(torch.argmax(logits, dim=1) == labels).item() * 1.0 / len(labels)\n",
    "\n",
    "\n",
    "def micro_macro_f1_score(logits, labels):\n",
    "    \"\"\"Calculate Micro-F1 and Macro-F1 scores\n",
    "    :param logits: tensor(N, C) Prediction probability, N is the number of samples, C is the number of categories\n",
    "    :param labels: tensor(N) \n",
    "    Macro-average precision score can be defined as the arithmetic mean of all the precision scores of different classes.\n",
    "    \"\"\"\n",
    "    prediction = torch.argmax(logits, dim=1).long().numpy()\n",
    "    labels = labels.numpy()\n",
    "    micro_f1 = f1_score(labels, prediction, average='micro')\n",
    "    macro_f1 = f1_score(labels, prediction, average='macro')\n",
    "    return micro_f1, macro_f1\n",
    "\n",
    "def split_idx(samples, train_size, val_size, random_state=None):\n",
    "    \"\"\"The samples are divided into training set, test set and validation set, which must be satisfied (represented by floating point numbers):\n",
    "    * 0 < train_size < 1\n",
    "    * 0 < val_size < 1\n",
    "    * train_size + val_size < 1\n",
    "    \"\"\"\n",
    "    train, val = train_test_split(samples, train_size=train_size, random_state=random_state)\n",
    "    if isinstance(val_size, float):\n",
    "        val_size *= len(samples) / len(val)\n",
    "    val, test = train_test_split(val, train_size=val_size, random_state=random_state)\n",
    "    return train, val, test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tiFfg9W04sOE"
   },
   "outputs": [],
   "source": [
    "#@title OGB - MAG DATASET\n",
    "\n",
    "#dataset = DglNodePropPredDataset(name = \"ogbn-mag\")\n",
    "#split_idx = dataset.get_idx_split()\n",
    "#train_idx, valid_idx, test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
    "#graph, label = dataset[0] # graph: dgl graph object, label: torch tensor of shape (num_nodes, num_tasks)\n",
    "#graph.nodes['paper'].data['label'] = label['paper'][:, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F3r6_zby44tv"
   },
   "outputs": [],
   "source": [
    "#@title HGT MODEL\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "\"\"\"Heterogeneous Graph Transformer (HGT)\n",
    "论文链接：https://arxiv.org/pdf/2003.01332\n",
    "\"\"\"\n",
    "import math\n",
    "\n",
    "import dgl.function as fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import HeteroGraphConv\n",
    "from dgl.ops import edge_softmax\n",
    "from dgl.utils import expand_as_pair\n",
    "\n",
    "\n",
    "class HGTAttention(nn.Module):\n",
    "    \"\"\"HGT attention module\n",
    "        :param out_dim: int output feature dimension\n",
    "        :param num_heads: int Number of attention heads K\n",
    "        :param k_linear: nn.Linear(d_in, d_out)\n",
    "        :param q_linear: nn.Linear(d_in, d_out)\n",
    "        :param v_linear: nn.Linear(d_in, d_out)\n",
    "        :param w_att: tensor(K, d_out/K, d_out/K)\n",
    "        :param w_msg: tensor(K, d_out/K, d_out/K)\n",
    "        :param mu: tensor(1)\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, out_dim, num_heads, k_linear, q_linear, v_linear, w_att, w_msg, mu):\n",
    "   \n",
    "        super().__init__()\n",
    "        self.out_dim = out_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = out_dim // num_heads\n",
    "        self.k_linear = k_linear\n",
    "        self.q_linear = q_linear\n",
    "        self.v_linear = v_linear\n",
    "        self.w_att = w_att\n",
    "        self.w_msg = w_msg\n",
    "        self.mu = mu\n",
    "\n",
    "    def forward(self, g, feat):\n",
    "        \"\"\"\n",
    "        :param g: DGLGraph bipartite graph (contains only one relation)\n",
    "        :param feat: tensor(N_src, d_in) or (tensor(N_src, d_in), tensor(N_dst, d_in)) input feature\n",
    "        :return: tensor(N_dst, d_out) The target vertex's representation of the relationship\n",
    "        \"\"\"\n",
    "        \n",
    "        #avoids changing the graph features when exiting the function.\n",
    "        with g.local_scope():\n",
    "            if g.is_block:\n",
    "                feat_src, feat_dst = feat\n",
    "            else:\n",
    "              feat_src, feat_dst = expand_as_pair(feat, g)\n",
    "            # (N_src, d_in) -> (N_src, d_out) -> (N_src, K, d_out/K)\n",
    "            k = self.k_linear(feat_src).view(-1, self.num_heads, self.d_k)\n",
    "            v = self.v_linear(feat_src).view(-1, self.num_heads, self.d_k)\n",
    "            q = self.q_linear(feat_dst).view(-1, self.num_heads, self.d_k)\n",
    "\n",
    "            # k[:, h] @= w_att[h] => k[n, h, j] = ∑(i) k[n, h, i] * w_att[h, i, j]\n",
    "            k = torch.einsum('nhi,hij->nhj', k, self.w_att)\n",
    "            v = torch.einsum('nhi,hij->nhj', v, self.w_msg)\n",
    "\n",
    "            g.srcdata.update({'k': k, 'v': v})\n",
    "            g.dstdata['q'] = q\n",
    "            g.apply_edges(fn.v_dot_u('q', 'k', 't'))  # g.edata['t']: (E, K, 1)\n",
    "            attn = g.edata.pop('t').squeeze(dim=-1) * self.mu / math.sqrt(self.d_k)\n",
    "            attn = edge_softmax(g, attn)  # (E, K)\n",
    "            g.edata['t'] = attn.unsqueeze(dim=-1)  # (E, K, 1)\n",
    "\n",
    "            g.update_all(fn.u_mul_e('v', 't', 'm'), fn.sum('m', 'h'))\n",
    "            out = g.dstdata['h'].view(-1, self.out_dim)  # (N_dst, d_out)\n",
    "            \n",
    "            return out\n",
    "\n",
    "\n",
    "class HGTLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, num_heads, ntypes, etypes, dropout=0.2, use_norm=True):\n",
    "        \"\"\"HGT layer\n",
    "        :param in_dim: int input feature dimension\n",
    "        :param out_dim: int output feature dimension\n",
    "        :param num_heads: int Number of attention heads K\n",
    "        :param ntypes: List[str] list of vertex types\n",
    "        :param etypes: List[(str, str, str)] list of canonical edge types\n",
    "        :param dropout: dropout: float, optional Dropout probability, default is 0.2\n",
    "        :param use_norm: bool, optional whether to use layer normalization, the default is True\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        d_k = out_dim // num_heads\n",
    "        k_linear = {ntype: nn.Linear(in_dim, out_dim) for ntype in ntypes}\n",
    "        q_linear = {ntype: nn.Linear(in_dim, out_dim) for ntype in ntypes}\n",
    "        v_linear = {ntype: nn.Linear(in_dim, out_dim) for ntype in ntypes}\n",
    "        w_att = {r[1]: nn.Parameter(torch.Tensor(num_heads, d_k, d_k)) for r in etypes}\n",
    "        w_msg = {r[1]: nn.Parameter(torch.Tensor(num_heads, d_k, d_k)) for r in etypes}\n",
    "        mu = {r[1]: nn.Parameter(torch.ones(num_heads)) for r in etypes}\n",
    "        self.reset_parameters(w_att, w_msg)\n",
    "        self.conv = HeteroGraphConv({\n",
    "            etype: HGTAttention(\n",
    "                out_dim, num_heads, k_linear[stype], q_linear[dtype], v_linear[stype],\n",
    "                w_att[etype], w_msg[etype], mu[etype]\n",
    "            ) for stype, etype, dtype in etypes\n",
    "        }, 'mean')\n",
    "\n",
    "        self.a_linear = nn.ModuleDict({ntype: nn.Linear(out_dim, out_dim) for ntype in ntypes})\n",
    "        self.skip = nn.ParameterDict({ntype: nn.Parameter(torch.ones(1)) for ntype in ntypes})\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.use_norm = use_norm\n",
    "        if use_norm:\n",
    "            self.norms = nn.ModuleDict({ntype: nn.LayerNorm(out_dim) for ntype in ntypes})\n",
    "\n",
    "    def reset_parameters(self, w_att, w_msg):\n",
    "        for etype in w_att:\n",
    "            nn.init.xavier_uniform_(w_att[etype])\n",
    "            nn.init.xavier_uniform_(w_msg[etype])\n",
    "\n",
    "    def forward(self, g, feats):\n",
    "        \"\"\"\n",
    "        :param g: DGLGraph heterogeneous graph\n",
    "        :param feats: Dict[str, tensor(N_i, d_in)] mapping of vertex types to input vertex features\n",
    "        :return: Dict[str, tensor(N_i, d_out)] mapping of vertex types to output features\n",
    "        \"\"\"\n",
    "        if g.is_block:\n",
    "            feats_dst = {ntype: feats[ntype][:g.num_dst_nodes(ntype)] for ntype in feats}\n",
    "        else:\n",
    "            feats_dst = feats\n",
    "        with g.local_scope():\n",
    "            # STEP 1 --> Heterogeneous Mutual Attention + Heterogeneous Messaging + Goal-Related Aggregation\n",
    "          \n",
    "            hs = self.conv(g, (feats, feats_dst))  # {ntype: tensor(N_i, d_out)}\n",
    "            \n",
    "            \n",
    "            # Residual connections\n",
    "            out_feats = {}\n",
    "            for ntype in hs.keys():\n",
    "                if g.num_dst_nodes(ntype) == 0:\n",
    "                    continue\n",
    "                alpha = torch.sigmoid(self.skip[ntype])\n",
    "\n",
    "                trans_out = self.drop(self.a_linear[ntype](hs[ntype]))\n",
    "                out = alpha * trans_out + (1 - alpha) * feats_dst[ntype]\n",
    "                out_feats[ntype] = self.norms[ntype](out) if self.use_norm else out\n",
    "            return out_feats\n",
    "\n",
    "\n",
    "class HGT1(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, in_dims, hidden_dim, out_dim, num_heads, ntypes, etypes,\n",
    "            predict_ntype, num_layers, dropout=0.2, use_norm=True ):\n",
    "        \"\"\"HGT model\n",
    "        :param in_dims: Dict[str, int] mapping of vertex types to input feature dimensions\n",
    "        :param hidden_dim: int hidden feature dimension\n",
    "        :param out_dim: int output feature dimension\n",
    "        :param num_heads: int Number of attention heads K\n",
    "        :param ntypes: List[str] list of vertex types\n",
    "        :param etypes: List[(str, str, str)] list of canonical edge types\n",
    "        :param predict_ntype: str The type of vertex to be predicted\n",
    "        :param num_layers: int number of layers\n",
    "        :param dropout: dropout: float, optional Dropout probability, default is 0.2\n",
    "        :param use_norm: bool, optional whether to use layer normalization, the default is True\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.predict_ntype = \"paper\"\n",
    "        self.adapt_fcs = nn.ModuleDict({\n",
    "            ntype: nn.Linear(in_dim , hidden_dim) for ntype, in_dim in in_dims.items()\n",
    "        })\n",
    "        #create the HGT layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            HGTLayer(hidden_dim, hidden_dim, num_heads, ntypes, etypes, dropout, use_norm)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.predict = nn.Linear(hidden_dim, out_dim)\n",
    "        \n",
    "       \n",
    "        #print(self.embed(torch.LongTensor([0])))\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, blocks):\n",
    "        h = {}\n",
    "        for ntype in blocks[0].srcdata['feat']:\n",
    "            h[ntype] = F.gelu(self.adapt_fcs[ntype](blocks[0].srcdata['feat'][ntype]))\n",
    "\n",
    "        x = self.layers[0](blocks[0], h)\n",
    "\n",
    "        for i in range(1, len(self.layers)):\n",
    "            x = self.layers[i](blocks[i], x)  \n",
    "        x = self.predict(x[self.predict_ntype])  \n",
    "        return x\n",
    "\n",
    "       \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        #com_indexes = dict chiave il t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-rWlRPC-O8H"
   },
   "outputs": [],
   "source": [
    "#@title TRAIN\n",
    "\n",
    "\n",
    "def train(data,g, train_idx,valid_idx, test_idx):\n",
    "    use_cuda = 1 >= 0 and torch.cuda.is_available()\n",
    "    if use_cuda:\n",
    "        torch.cuda.set_device('cuda:0')\n",
    "        device = \"cuda:0\"\n",
    "    import ast\n",
    "    feat = \"feat\"\n",
    "    predict_ntype = \"paper\"\n",
    "    mlflow.set_experiment(\"ogb_baseline\")\n",
    "    experiment = mlflow.get_experiment_by_name(\"experiment name\")\n",
    "    the_last_loss = 1000\n",
    "    patience = 50\n",
    "    trigger_times = 0\n",
    "    \n",
    "    #dictionary containing node type and features\n",
    "    features = {ntype: g.nodes[ntype].data[feat] for ntype in g.ntypes}\n",
    "    #lists containing labels, train-mask,val_mask and test-mask\n",
    "    labels = g.nodes[predict_ntype].data['label']\n",
    "    labels = labels.to(device)\n",
    "    train_mask = train_idx\n",
    "    val_mask = valid_idx\n",
    "    test_mask = test_idx\n",
    "    \n",
    "   \n",
    "  \n",
    "            \n",
    "    with mlflow.start_run():\n",
    "            #initialization of HGT model\n",
    "        model = HGT1(\n",
    "            {ntype: g.nodes[ntype].data[feat].shape[1] for ntype in g.ntypes},\n",
    "            256, data.num_classes, 8, g.ntypes, g.canonical_etypes,\n",
    "            predict_ntype, 2, 0.5\n",
    "        )\n",
    "        model.to(device)\n",
    "        \n",
    "        optimizer = optim.AdamW(model.parameters())\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer, 0.005, total_steps=1000)\n",
    "          \n",
    "        warnings.filterwarnings('ignore', 'Setting attributes on ParameterDict is not supported')\n",
    "\n",
    "        sampler_full = dgl.dataloading.MultiLayerFullNeighborSampler(2)\n",
    "        sampler = dgl.dataloading.NeighborSampler([256] * 2)\n",
    "\n",
    "        \n",
    "\n",
    "        train_loader = dgl.dataloading.DataLoader(\n",
    "          g, train_idx, sampler,\n",
    "          batch_size=256,  drop_last=False, shuffle=True, num_workers=0)\n",
    "        \n",
    "        input_nodes, output_nodes, blocks = next(iter(train_loader))\n",
    "        \n",
    "\n",
    "        val_loader = dgl.dataloading.DataLoader(\n",
    "          g, valid_idx, sampler_full, \n",
    "          batch_size=256,  drop_last=False, shuffle=False, num_workers=0)\n",
    "\n",
    "        test_loader = dgl.dataloading.DataLoader(\n",
    "          g, test_idx, sampler_full,\n",
    "          batch_size=256,  drop_last=False, shuffle=False, num_workers=0)\n",
    "          \n",
    "        epochs = 100\n",
    "        best_acc = 0 \n",
    "        for epoch in range(epochs):\n",
    "            total_loss=0\n",
    "            model.train()\n",
    "            for (input_nodes, output_nodes, blocks) in train_loader:\n",
    "                blocks = [blk.to(device) for blk in blocks]\n",
    "                optimizer.zero_grad()\n",
    "                lbl = (blocks[-1].dstdata['label']['paper'])        \n",
    "                logits =  model(blocks)\n",
    "                lbl= lbl.squeeze()\n",
    "                loss = F.cross_entropy(logits,lbl)\n",
    "                total_loss += loss.item()\n",
    "                #total_loss += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print(\"-\"*100)\n",
    "            print(\"epoch\" + \" \" + str(epoch)) \n",
    "            print(\"total loss\", total_loss)\n",
    "            print('Evaluating validation')\n",
    "            val_acc,val_loss,total_micro,total_macro  = evaluate(model, val_loader)\n",
    "\n",
    "            if (val_acc < best_acc):\n",
    "                trigger_times += 1\n",
    "                print(\"trigger\" + str(trigger_times))\n",
    "                if trigger_times >= patience:\n",
    "                    print('Early stopping!\\nStart to test process.')\n",
    "                    break\n",
    "            else:\n",
    "                print('trigger times: 0')\n",
    "                trigger_times = 0\n",
    "                best_acc = val_acc\n",
    "                best_model_dict = model.state_dict()\n",
    "                print(\"best model at epoch :\", epoch)\n",
    "                print('best_accuracy is ', val_acc)\n",
    "\n",
    "            \n",
    "            print('The current validation accuracy:', val_acc)\n",
    "            print('The current validation micro:', total_micro)\n",
    "            print('The current validation macro:', total_macro)\n",
    "\n",
    "                \n",
    "        #load the best model\n",
    "        model.load_state_dict(torch.load('best-model-parameters.pt', map_location=device))\n",
    "        #testing\n",
    "        test_acc,test_loss, test_micro, test_macro  = evaluate(model, test_loader)\n",
    "        #print('Test Micro-F1 {:.4f} | Test Macro-F1 {:.4f}'.format(*test_scores))\n",
    "        print(\"test micro\" + \" \" + str(test_micro))\n",
    "        print(\"test macro\" + \" \" + str(test_macro))\n",
    "        mlflow.log_param(\"epoch\", epochs)\n",
    "        mlflow.log_param(\"stop_at_epoch\", epoch)\n",
    "        mlflow.log_metric(\"loss_test\", loss)\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZe5jAwQCIEa"
   },
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "def evaluate(model, loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        count=0\n",
    "        total_acc=0\n",
    "        total_loss = 0\n",
    "        total_micro = 0\n",
    "        total_macro = 0\n",
    "        for input_nodes, output_nodes, blocks in loader:\n",
    "            blocks = [blk.to(device) for blk in blocks]\n",
    "            lbl = (blocks[-1].dstdata['label']['paper'])#labels start from 1 so we need -1\n",
    "            logits =  model(blocks)\n",
    "            lbl= lbl.squeeze()\n",
    "            acc = torch.sum(torch.argmax(logits,dim=1) == lbl).item()\n",
    "            loss = F.cross_entropy(logits, lbl)\n",
    "            log = logits.cpu()\n",
    "            label = lbl.cpu()\n",
    "            micro, macro = micro_macro_f1_score(log, label)\n",
    "            total_micro  += micro\n",
    "            total_macro += macro \n",
    "            total_acc += acc\n",
    "            total_loss += loss\n",
    "            count += lbl.size(0)\n",
    "        total_acc= total_acc / count\n",
    "        total_loss= total_loss / count\n",
    "        total_micro = total_micro / count\n",
    "        total_macro = total_macro / count\n",
    "        return total_acc,total_loss,total_micro,total_macro\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f36DUncOAIrh"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    dataset = DglNodePropPredDataset(name = \"ogbn-mag\")\n",
    "    split_idx = dataset.get_idx_split()\n",
    "    train_idx, valid_idx, test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
    "    graph, label = dataset[0] # graph: dgl graph object, label: torch tensor of shape (num_nodes, num_tasks)\n",
    "    transform = AddReverse()\n",
    "    graph = transform(graph)\n",
    "    print(graph)\n",
    "    \n",
    "    graph.nodes['paper'].data['label'] = label['paper'][:, 0]\n",
    "    graph.nodes['author'].data['feat'] = torch.ones(graph.num_nodes('author'), 128)\n",
    "    graph.nodes['field_of_study'].data['feat'] = torch.ones(graph.num_nodes('field_of_study'), 128)\n",
    "    graph.nodes['institution'].data['feat'] = torch.ones(graph.num_nodes('institution'), 128)\n",
    "    trained = train(dataset,graph,train_idx,valid_idx, test_idx)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khLDoDYSZGxt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
