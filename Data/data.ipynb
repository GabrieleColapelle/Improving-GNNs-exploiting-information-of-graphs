{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6698ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "\n",
    "import math\n",
    "\n",
    "import dgl\n",
    "from dgl.nn import HeteroGraphConv\n",
    "from dgl.data import citation_graph, rdf, knowledge_graph\n",
    "from dgl.utils import extract_node_subframes, set_new_frames\n",
    "import dgl.function as fn\n",
    "from dgl.ops import edge_softmax\n",
    "from dgl.utils import expand_as_pair\n",
    "from dgl.data import DGLDataset\n",
    "from dgl.data.utils import _get_dgl_url, download, save_graphs, load_graphs, \\\n",
    "    generate_mask_tensor, idx2mask, makedirs\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import f1_score, normalized_mutual_info_score, adjusted_rand_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4b7ed7",
   "metadata": {},
   "source": [
    "#### GET THE ACM DATASET AND CONVERT TO A DGL GRAPH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f9d4bd",
   "metadata": {},
   "source": [
    "The basic DGL dataset for creating graph datasets. This class defines a basic template class for DGL Dataset. The following steps will be executed automatically:\n",
    "\n",
    "Check whether there is a dataset cache on disk (already processed and stored on the disk) by invoking has_cache(). If true, goto 5.\n",
    "\n",
    "1-Call download() to download the data if url is not None.\n",
    "\n",
    "2-Call process() to process the data.\n",
    "\n",
    "3-Call save() to save the processed dataset on disk and goto 6.\n",
    "\n",
    "4-Call load() to load the processed dataset from disk.\n",
    "\n",
    "Done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f922c0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACMDataset(DGLDataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        #Get DGL online url for download.\n",
    "        super().__init__('ACM', _get_dgl_url('dataset/ACM.mat'))\n",
    "\n",
    "    def download(self):\n",
    "        file_path = os.path.join(self.raw_dir, 'ACM.mat')\n",
    "        if not os.path.exists(file_path):\n",
    "            download(self.url, path=file_path)\n",
    "\n",
    "    def save(self):\n",
    "        save_graphs(os.path.join(self.save_path, self.name + '_dgl_graph.bin'), [self.g])\n",
    "\n",
    "    def load(self):\n",
    "        graphs, _ = load_graphs(os.path.join(self.save_path, self.name + '_dgl_graph.bin'))\n",
    "        self.g = graphs[0]\n",
    "        for k in ('train_mask', 'val_mask', 'test_mask'):\n",
    "            self.g.nodes['paper'].data[k] = self.g.nodes['paper'].data[k].bool()\n",
    "\n",
    "    def process(self):\n",
    "        data = sio.loadmat(os.path.join(self.raw_dir, 'ACM.mat'))\n",
    "        p_vs_l = data['PvsL']  # paper-field\n",
    "        p_vs_a = data['PvsA']  # paper-author\n",
    "        p_vs_t = data['PvsT']  # paper-term, bag of words\n",
    "        p_vs_c = data['PvsC']  # paper-conference, labels come from that\n",
    "\n",
    "        # We assign\n",
    "        # (1) KDD papers as class 0 (data mining),\n",
    "        # (2) SIGMOD and VLDB papers as class 1 (database),\n",
    "        # (3) SIGCOMM and MobiCOMM papers as class 2 (communication)\n",
    "        conf_ids = [0, 1, 9, 10, 13]\n",
    "        label_ids = [0, 1, 2, 2, 1]\n",
    "\n",
    "        p_vs_c_filter = p_vs_c[:, conf_ids]\n",
    "        #get indeces of all papers\n",
    "        p_selected = (p_vs_c_filter.sum(1) != 0).A1.nonzero()[0]\n",
    "        p_vs_l = p_vs_l[p_selected]\n",
    "        p_vs_a = p_vs_a[p_selected]\n",
    "        p_vs_t = p_vs_t[p_selected]\n",
    "        p_vs_c = p_vs_c[p_selected]\n",
    "        \n",
    "        #building the graph\n",
    "        self.g = dgl.heterograph({\n",
    "            ('paper', 'pa', 'author'): p_vs_a.nonzero(),\n",
    "            ('author', 'ap', 'paper'): p_vs_a.transpose().nonzero(),\n",
    "            ('paper', 'pf', 'field'): p_vs_l.nonzero(),\n",
    "            ('field', 'fp', 'paper'): p_vs_l.transpose().nonzero()\n",
    "\n",
    "        })\n",
    "        #the features of a paper are the bag of words associated to a paper\n",
    "        paper_features = torch.FloatTensor(p_vs_t.toarray())  # (4025, 1903)\n",
    "        \n",
    "        #get indces and labels\n",
    "        pc_p, pc_c = p_vs_c.nonzero()\n",
    "        paper_labels = np.zeros(len(p_selected), dtype=np.int64)\n",
    "        for conf_id, label_id in zip(conf_ids, label_ids):\n",
    "            paper_labels[pc_p[pc_c == conf_id]] = label_id\n",
    "        paper_labels = torch.from_numpy(paper_labels)\n",
    "\n",
    "        float_mask = np.zeros(len(pc_p))\n",
    "        for conf_id in conf_ids:\n",
    "            pc_c_mask = (pc_c == conf_id)\n",
    "            float_mask[pc_c_mask] = np.random.permutation(np.linspace(0, 1, pc_c_mask.sum()))\n",
    "        #train_idx = np.where(float_mask <= 0.2)[0]\n",
    "        #val_idx = np.where((float_mask > 0.2) & (float_mask <= 0.3))[0]\n",
    "        #test_idx = np.where(float_mask > 0.3)[0]\n",
    "        \n",
    "        num_paper_nodes = self.g.num_nodes('paper')\n",
    "        train_idx, val_idx, test_idx = split_idx(np.arange(num_paper_nodes), 2400, 800, 42)\n",
    "        \n",
    "        train_mask = generate_mask_tensor(idx2mask(train_idx, num_paper_nodes))\n",
    "        val_mask = generate_mask_tensor(idx2mask(val_idx, num_paper_nodes))\n",
    "        test_mask = generate_mask_tensor(idx2mask(test_idx, num_paper_nodes))\n",
    "\n",
    "        self.g.nodes['paper'].data['feat'] = paper_features\n",
    "        self.g.nodes['paper'].data['label'] = paper_labels\n",
    "        self.g.nodes['paper'].data['train_mask'] = train_mask\n",
    "        self.g.nodes['paper'].data['val_mask'] = val_mask\n",
    "        self.g.nodes['paper'].data['test_mask'] = test_mask\n",
    "        # The feature of the author vertex is the average of the features of its associated paper vertex\n",
    "        #self.g.multi_update_all({'pa': (fn.copy_u('feat', 'm'), fn.mean('m', 'feat'))}, 'sum')\n",
    "        #self.g.multi_update_all({'pf': (fn.copy_u('feat', 'm'), fn.mean('m', 'feat'))}, 'sum')\n",
    "        \n",
    "        self.g.nodes['field'].data['feat'] = torch.eye(self.g.num_nodes('field'))\n",
    "        self.g.nodes['author'].data['feat'] = torch.ones(self.g.num_nodes('author'), 16)\n",
    "        print(\"-\"*100)\n",
    "\n",
    "    #def has_cache(self):\n",
    "        #return os.path.exists(os.path.join(self.save_path, self.name + '_dgl_graph.bin'))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx != 0:\n",
    "            raise IndexError('This dataset has only one graph')\n",
    "        return self.g\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return 3\n",
    "\n",
    "    @property\n",
    "    def metapaths(self):\n",
    "        return [['pa', 'ap'], ['pf', 'fp']]\n",
    "\n",
    "    @property\n",
    "    def predict_ntype(self):\n",
    "        return 'paper'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a65f9b5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DGLDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIMDbDataset\u001b[39;00m(\u001b[43mDGLDataset\u001b[49m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124;03m\"\"\"IMDb movie dataset, only one heterogeneous graph\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Statistical data\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    -----\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    * feat: tensor(2081, 1299) average of associated movie features\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     _url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/Jhy1993/HAN/master/data/imdb/movie_metadata.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DGLDataset' is not defined"
     ]
    }
   ],
   "source": [
    "class IMDbDataset(DGLDataset):\n",
    "    _url = 'https://raw.githubusercontent.com/Jhy1993/HAN/master/data/imdb/movie_metadata.csv'\n",
    "    _seed = 42\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__('imdb', self._url)\n",
    "\n",
    "    def download(self):\n",
    "        file_path = os.path.join(self.raw_dir, 'imdb.csv')\n",
    "        if not os.path.exists(file_path):\n",
    "            download(self.url, path=file_path)\n",
    "\n",
    "    def save(self):\n",
    "        save_graphs(os.path.join(self.save_path, self.name + '_dgl_graph.bin'), [self.g])\n",
    "\n",
    "    def load(self):\n",
    "        graphs, _ = load_graphs(os.path.join(self.save_path, self.name + '_dgl_graph.bin'))\n",
    "        self.g = graphs[0]\n",
    "        for k in ('train_mask', 'val_mask', 'test_mask'):\n",
    "            self.g.nodes['movie'].data[k] = self.g.nodes['movie'].data[k].bool()\n",
    "\n",
    "    def process(self):\n",
    "        self.data = pd.read_csv(os.path.join(self.raw_dir, 'imdb.csv'), encoding='utf8') \\\n",
    "            .dropna(axis=0, subset=['actor_1_name', 'director_name']).reset_index(drop=True)\n",
    "        self.labels = self._extract_labels()\n",
    "        self.movies = list(sorted(m.strip() for m in self.data['movie_title']))\n",
    "        self.directors = list(sorted(set(self.data['director_name'])))\n",
    "        self.actors = list(sorted(set(itertools.chain.from_iterable(\n",
    "            self.data[c].dropna().to_list()\n",
    "            for c in ('actor_1_name', 'actor_2_name', 'actor_3_name')\n",
    "        ))))\n",
    "        self.g = self._build_graph()\n",
    "        self._add_ndata()\n",
    "        return self.data\n",
    "\n",
    "    def _extract_labels(self):\n",
    "        labels = np.full(len(self.data), -1)\n",
    "        for i, genres in self.data['genres'].iteritems():\n",
    "            for genre in genres.split('|'):\n",
    "                if genre == 'Action':\n",
    "                    labels[i] = 0\n",
    "                    break\n",
    "                elif genre == 'Comedy':\n",
    "                    labels[i] = 1\n",
    "                    break\n",
    "                elif genre == 'Drama':\n",
    "                    labels[i] = 2\n",
    "                    break\n",
    "        other_idx = np.where(labels == -1)[0]\n",
    "        self.data = self.data.drop(other_idx).reset_index(drop=True)\n",
    "        return np.delete(labels, other_idx, axis=0)\n",
    "\n",
    "    def _build_graph(self):\n",
    "        ma, md = set(), set()\n",
    "        for m, row in self.data.iterrows():\n",
    "            d = self.directors.index(row['director_name'])\n",
    "            md.add((m, d))\n",
    "            for c in ('actor_1_name', 'actor_2_name', 'actor_3_name'):\n",
    "                if row[c] in self.actors:\n",
    "                    a = self.actors.index(row[c])\n",
    "                    ma.add((m, a))\n",
    "        ma, md = list(ma), list(md)\n",
    "        ma_m, ma_a = [e[0] for e in ma], [e[1] for e in ma]\n",
    "        md_m, md_d = [e[0] for e in md], [e[1] for e in md]\n",
    "        return dgl.heterograph({\n",
    "            ('movie', 'ma', 'actor'): (ma_m, ma_a),\n",
    "            ('actor', 'am', 'movie'): (ma_a, ma_m),\n",
    "            ('movie', 'md', 'director'): (md_m, md_d),\n",
    "            ('director', 'dm', 'movie'): (md_d, md_m),\n",
    "            #('actor', 'aa', 'actor') : ((),()),\n",
    "            #('movie', 'mm', 'movie') : ((),()),\n",
    "            #('director', 'dd', 'director') : ((),()),\n",
    "        })\n",
    "\n",
    "    def _add_ndata(self):\n",
    "        vectorizer = CountVectorizer(min_df=5)\n",
    "        features = vectorizer.fit_transform(self.data['plot_keywords'].fillna('').values)\n",
    "        self.g.nodes['movie'].data['feat'] = torch.from_numpy(features.toarray()).float()\n",
    "        self.g.nodes['movie'].data['label'] = torch.from_numpy(self.labels).long()\n",
    "\n",
    "        #Actor and director vertex features are the average of their associated movie vertex features\n",
    "        #self.g.multi_update_all({\n",
    "            #'ma': (fn.copy_u('feat', 'm'), fn.mean('m', 'feat')),\n",
    "            #'md': (fn.copy_u('feat', 'm'), fn.mean('m', 'feat'))\n",
    "        #}, 'sum')\n",
    "\n",
    "        n_movies = len(self.movies)\n",
    "        train_idx, val_idx, test_idx = split_idx(np.arange(n_movies), 400, 400, self._seed)\n",
    "        #train_idx, val_idx, test_idx = split_idx(np.arange(n_movies), 2600, 800, self._seed)\n",
    "        self.g.nodes['movie'].data['train_mask'] = generate_mask_tensor(idx2mask(train_idx, n_movies))\n",
    "        self.g.nodes['movie'].data['val_mask'] = generate_mask_tensor(idx2mask(val_idx, n_movies))\n",
    "        self.g.nodes['movie'].data['test_mask'] = generate_mask_tensor(idx2mask(test_idx, n_movies))\n",
    "        self.g.nodes['actor'].data['feat'] = torch.ones(5257,16)\n",
    "        self.g.nodes['director'].data['feat'] = torch.ones(2081,16)\n",
    "\n",
    "    #def has_cache(self):\n",
    "        #return os.path.exists(os.path.join(self.save_path, self.name + '_dgl_graph.bin'))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx != 0:\n",
    "            raise IndexError('This dataset has only one graph')\n",
    "        return self.g\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return 3\n",
    "\n",
    "    @property\n",
    "    def metapaths(self):\n",
    "        return [['ma', 'am'], ['md', 'dm']]\n",
    "\n",
    "    @property\n",
    "    def predict_ntype(self):\n",
    "        return 'movie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61e90c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stopwords\n",
    "nltk.download()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37ef312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBLPFourAreaDataset(DGLDataset):\n",
    "    _url = 'https://raw.githubusercontent.com/Jhy1993/HAN/master/data/DBLP_four_area/'\n",
    "    _url2 = 'https://pan.baidu.com/s/1Qr2e97MofXsBhUvQqgJqDg'\n",
    "    _raw_files = [\n",
    "        'readme.txt', 'author_label.txt', 'paper.txt', 'conf_label.txt', 'term.txt',\n",
    "        'paper_author.txt', 'paper_conf.txt', 'paper_term.txt'\n",
    "    ]\n",
    "    _seed = 42\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__('DBLP_four_area', self._url)\n",
    "\n",
    "    def download(self):\n",
    "        if not os.path.exists(self.raw_path):\n",
    "            makedirs(self.raw_path)\n",
    "        for file in self._raw_files:\n",
    "            download(self.url + file, os.path.join(self.raw_path, file))\n",
    "\n",
    "    def save(self):\n",
    "        save_graphs(os.path.join(self.save_path, self.name + '_dgl_graph.bin'), [self.g])\n",
    "\n",
    "    def load(self):\n",
    "        graphs, _ = load_graphs(os.path.join(self.save_path, self.name + '_dgl_graph.bin'))\n",
    "        self.g = graphs[0]\n",
    "        for k in ('train_mask', 'val_mask', 'test_mask'):\n",
    "            self.g.nodes['author'].data[k] = self.g.nodes['author'].data[k].bool()\n",
    "\n",
    "    def process(self):\n",
    "        self.authors, self.papers, self.confs, self.terms, \\\n",
    "            self.paper_author, self.paper_conf, self.paper_term = self._read_raw_data()\n",
    "        self._filter_nodes_and_edges()\n",
    "        self._lemmatize_terms()\n",
    "        self._remove_stopwords()\n",
    "        self._reset_index()\n",
    "\n",
    "\n",
    "        self.g = self._build_graph()\n",
    "        self._add_ndata()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _read_raw_data(self):\n",
    "        authors = self._read_file('author_label.txt', names=['id', 'label', 'name'], index_col='id')\n",
    "        papers = self._read_file('paper.txt', names=['id', 'title'], index_col='id', encoding='cp1252')\n",
    "        confs = self._read_file('conf_label.txt', names=['id', 'label', 'name', 'dummy'], index_col='id')\n",
    "        terms = self._read_file('term.txt', names=['id', 'name'], index_col='id')\n",
    "        paper_author = self._read_file('paper_author.txt', names=['paper_id', 'author_id'])\n",
    "        paper_conf = self._read_file('paper_conf.txt', names=['paper_id', 'conf_id'])\n",
    "        paper_term = self._read_file('paper_term.txt', names=['paper_id', 'term_id'])\n",
    "        return authors, papers, confs, terms, paper_author, paper_conf, paper_term\n",
    "\n",
    "    def _read_file(self, filename, names, index_col=None, encoding='utf8'):\n",
    "        return pd.read_csv(\n",
    "            os.path.join(self.raw_path, filename), sep='\\t', names=names, index_col=index_col,\n",
    "            keep_default_na=False, encoding=encoding\n",
    "        )\n",
    "\n",
    "    def _filter_nodes_and_edges(self):\n",
    "        \"\"\"Filter out vertices and edges not associated with scholars\"\"\"\n",
    "        self.paper_author = self.paper_author[self.paper_author['author_id'].isin(self.authors.index)]\n",
    "        paper_ids = self.paper_author['paper_id'].drop_duplicates()\n",
    "        self.papers = self.papers.loc[paper_ids]\n",
    "        self.paper_conf = self.paper_conf[self.paper_conf['paper_id'].isin(paper_ids)]\n",
    "        self.paper_term = self.paper_term[self.paper_term['paper_id'].isin(paper_ids)]\n",
    "        self.terms = self.terms.loc[self.paper_term['term_id'].drop_duplicates()]\n",
    "\n",
    "    def _lemmatize_terms(self):\n",
    "        \"\"\"Lemmatization of keywords and deduplication\"\"\"\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemma_id_map, term_lemma_map = {}, {}\n",
    "        for index, row in self.terms.iterrows():\n",
    "            lemma = lemmatizer.lemmatize(row['name'])\n",
    "            term_lemma_map[index] = lemma_id_map.setdefault(lemma, index)\n",
    "        self.terms = pd.DataFrame(\n",
    "            list(lemma_id_map.keys()), columns=['name'],\n",
    "            index=pd.Index(lemma_id_map.values(), name='id')\n",
    "        )\n",
    "        self.paper_term.loc[:, 'term_id'] = [\n",
    "            term_lemma_map[row['term_id']] for _, row in self.paper_term.iterrows()\n",
    "        ]\n",
    "        self.paper_term.drop_duplicates(inplace=True)\n",
    "        \n",
    "\n",
    "    def _remove_stopwords(self):\n",
    "        \"\"\"Remove stop words in keywords\"\"\"\n",
    "        stop_words = sklearn_stopwords.union(nltk_stopwords.words('english'))\n",
    "        self.terms = self.terms[~(self.terms['name'].isin(stop_words))]\n",
    "        self.paper_term = self.paper_term[self.paper_term['term_id'].isin(self.terms.index)]\n",
    "\n",
    "    def _reset_index(self):\n",
    "        \"\"\"Reset vertex id to 0~n-1\"\"\"\n",
    "        self.authors.reset_index(inplace=True)\n",
    "        self.papers.reset_index(inplace=True)\n",
    "        self.confs.reset_index(inplace=True)\n",
    "        self.terms.reset_index(inplace=True)\n",
    "        author_id_map = {row['id']: index for index, row in self.authors.iterrows()}\n",
    "        paper_id_map = {row['id']: index for index, row in self.papers.iterrows()}\n",
    "        conf_id_map = {row['id']: index for index, row in self.confs.iterrows()}\n",
    "        term_id_map = {row['id']: index for index, row in self.terms.iterrows()}\n",
    "\n",
    "        self.paper_author.loc[:, 'author_id'] = [author_id_map[i] for i in self.paper_author['author_id'].to_list()]\n",
    "        self.paper_conf.loc[:, 'conf_id'] = [conf_id_map[i] for i in self.paper_conf['conf_id'].to_list()]\n",
    "        self.paper_term.loc[:, 'term_id'] = [term_id_map[i] for i in self.paper_term['term_id'].to_list()]\n",
    "        for df in (self.paper_author, self.paper_conf, self.paper_term):\n",
    "            df.loc[:, 'paper_id'] = [paper_id_map[i] for i in df['paper_id']]\n",
    "\n",
    "    def _build_graph(self):\n",
    "        pa_p, pa_a = self.paper_author['paper_id'].to_list(), self.paper_author['author_id'].to_list()\n",
    "        pc_p, pc_c = self.paper_conf['paper_id'].to_list(), self.paper_conf['conf_id'].to_list()\n",
    "        pt_p, pt_t = self.paper_term['paper_id'].to_list(), self.paper_term['term_id'].to_list()\n",
    "        \n",
    "        return dgl.heterograph({\n",
    "            ('paper', 'pa', 'author'): (pa_p, pa_a),\n",
    "            ('author', 'ap', 'paper'): (pa_a, pa_p),\n",
    "            ('paper', 'pc', 'conf'): (pc_p, pc_c),\n",
    "            ('conf', 'cp', 'paper'): (pc_c, pc_p),\n",
    "            ('paper', 'pt', 'term'): (pt_p, pt_t),\n",
    "            ('term', 'tp', 'paper'): (pt_t, pt_p),\n",
    "            #('author', 'aa', 'author') : ((),()),\n",
    "        })\n",
    "\n",
    "    def _add_ndata(self):\n",
    "        _raw_file2 = os.path.join(self.raw_dir, 'DBLP4057_GAT_with_idx.mat')\n",
    "        if not os.path.exists(_raw_file2):\n",
    "            raise FileNotFoundError('Please manually download the file {} Extraction code: 6b3h and save it to {}'.format(\n",
    "                self._url2, _raw_file2))\n",
    "        mat = sio.loadmat(_raw_file2)\n",
    "        self.g.nodes['author'].data['feat'] = torch.from_numpy(mat['features']).float()\n",
    "        self.g.nodes['author'].data['label'] = torch.tensor(self.authors['label'].to_list())\n",
    "\n",
    "        n_authors = len(self.authors)\n",
    "       \n",
    "    \n",
    "        train_idx, val_idx, test_idx = split_idx(np.arange(n_authors), 800, 400, self._seed)\n",
    "        #train_idx, val_idx, test_idx = split_idx(np.arange(n_authors), 2400, 800, self._seed)\n",
    "        self.g.nodes['author'].data['train_mask'] = generate_mask_tensor(idx2mask(train_idx, n_authors))\n",
    "        self.g.nodes['author'].data['val_mask'] = generate_mask_tensor(idx2mask(val_idx, n_authors))\n",
    "        self.g.nodes['author'].data['test_mask'] = generate_mask_tensor(idx2mask(test_idx, n_authors))\n",
    "    \n",
    "        self.g.nodes['conf'].data['label'] = torch.tensor(self.confs['label'].to_list())\n",
    "        self.g.nodes['conf'].data['feat'] = torch.ones(20, 16)\n",
    "        self.g.nodes['term'].data['feat'] = torch.ones(7723, 16)\n",
    "        self.g.nodes['paper'].data['feat'] = torch.ones(14328, 16)\n",
    "        \n",
    "        #self.g.multi_update_all({\n",
    "            #'ap': (fn.copy_u('feat', 'm'), fn.mean('m', 'feat')),\n",
    "        #}, 'sum')\n",
    "        \n",
    "        #self.g.multi_update_all({\n",
    "            #'pt': (fn.copy_u('feat', 'm'), fn.mean('m', 'feat')),\n",
    "        #}, 'sum')\n",
    "        \n",
    "        #self.g.multi_update_all({\n",
    "            #'pc': (fn.copy_u('feat', 'm'), fn.mean('m', 'feat')),\n",
    "        #}, 'sum')\n",
    "\n",
    "        \n",
    "\n",
    "    #def has_cache(self):\n",
    "        #return os.path.exists(os.path.join(self.save_path, self.name + '_dgl_graph.bin'))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx != 0:\n",
    "            raise IndexError('This dataset has only one graph')\n",
    "        return self.g\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return 4\n",
    "\n",
    "    @property\n",
    "    def metapaths(self):\n",
    "        return [['ap', 'pa'], ['ap', 'pc', 'cp', 'pa'], ['ap', 'pt', 'tp', 'pa']]\n",
    "\n",
    "    @property\n",
    "    def predict_ntype(self):\n",
    "        return 'author'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903aa71d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
