{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcec58b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import itertools\n",
    "import dgl\n",
    "from dgl.nn import HeteroGraphConv\n",
    "from dgl.utils import extract_node_subframes, set_new_frames\n",
    "from sklearn.metrics import f1_score, normalized_mutual_info_score, adjusted_rand_score\n",
    "import copy\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "import json\n",
    "import ast\n",
    "import collections\n",
    "from dgl import AddMetaPaths\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766c333a",
   "metadata": {},
   "source": [
    "## Features computatipn from properties and structure of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9879bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''compute node degree and add it as node feture'''\n",
    "def get_degree_acm(hg):\n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    #find all actor of a film\n",
    "    #transform = AddMetaPaths({\n",
    "    #'pap': [('paper', 'pa', 'author'),('author', 'ap', 'paper')],\n",
    "    #'pfp': [('paper', 'pf', 'field'),('field','fp','paper')],\n",
    "    #'apa': [('author', 'ap', 'paper'),('paper','pa','author')],\n",
    "    #'fpf': [('field', 'fp', 'paper'),('paper','pf','field')]\n",
    "    #})\n",
    "    #hg2 = transform(hg2)\n",
    "    \n",
    "    #convert g to homogeneus graph\n",
    "    g = dgl.to_homogeneous(hg2)\n",
    "    #convert g to hnetworkx graph\n",
    "    nxg = g.to_networkx()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    \n",
    "    #get degree of nodes\n",
    "    d = dict(G2.degree)\n",
    "    \n",
    "    allow_zero_in_degree = True\n",
    "    \n",
    "    #split d in 3 dictionary, one for each node type\n",
    "    authors = {k: d[k] for k in list(d)[:17351]}\n",
    "    fields = {k: d[k] for k in list(d)[17351:17423]}\n",
    "    papers = {k: d[k] for k in list(d)[17423:21449]}\n",
    "    \n",
    "    #get values of dicts in 3 different lists\n",
    "    authors = list(authors.values())\n",
    "    papers= list(papers.values())\n",
    "    fields = list(fields.values())\n",
    "    \n",
    "    #convert into a FloatTensor\n",
    "    author = torch.FloatTensor(authors)\n",
    "    paper = torch.FloatTensor(papers)\n",
    "    field = torch.FloatTensor(fields)\n",
    "    \n",
    "    #prepare data to be added to features\n",
    "    author = torch.unsqueeze(author, 1)\n",
    "    paper = torch.unsqueeze(paper, 1)\n",
    "    field = torch.unsqueeze(field, 1)\n",
    "    \n",
    "    #concat new features with original features\n",
    "    tupl_au = (hg.nodes[\"author\"].data['feat'] ,author)\n",
    "    tupl_pa = (hg.nodes[\"paper\"].data['feat'] ,paper)\n",
    "    tupl_fi = (hg.nodes[\"field\"].data['feat'] ,field)\n",
    "    author = torch.cat(tupl_au,1)\n",
    "    paper = torch.cat(tupl_pa,1)\n",
    "    field = torch.cat(tupl_fi,1)\n",
    "    \n",
    "    #add computed tensors as features\n",
    "    hg.nodes[\"author\"].data['feat'] =  author\n",
    "    hg.nodes[\"paper\"].data['feat'] =  paper\n",
    "    hg.nodes[\"field\"].data['feat'] =  field\n",
    "    \n",
    "    print(hg.nodes[\"author\"].data['feat'])\n",
    "    print(hg.nodes[\"paper\"].data['feat'])\n",
    "    print(hg.nodes[\"field\"].data['feat'])\n",
    "                \n",
    "    return hg\n",
    "    \n",
    "'''compute page rank and add it as node feture'''\n",
    "def get_pageRank_acm(hg):\n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    \n",
    "    transform = AddMetaPaths({\n",
    "    #'pap': [('paper', 'pa', 'author'),('author', 'ap', 'paper')],\n",
    "    'pfp': [('paper', 'pf', 'field'),('field','fp','paper')],\n",
    "    #'apa': [('author', 'ap', 'paper'),('paper','pa','author')],\n",
    "    #'fpf': [('field', 'fp', 'paper'),('paper','pf','field')]  \n",
    "    })\n",
    "    hg3 = transform(hg2)\n",
    "    g = dgl.to_homogeneous(hg3)\n",
    "    nxg = g.to_networkx()\n",
    "    #n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    \n",
    "    #compute pagerank\n",
    "    d = nx.pagerank(G2, alpha=0.9)\n",
    "        \n",
    "    #split d in 3 dictionary, one for each node type\n",
    "    authors = {k: d[k] for k in list(d)[:17351]}\n",
    "    fields = {k: d[k] for k in list(d)[17351:17423]}\n",
    "    papers = {k: d[k] for k in list(d)[17423:21449]}\n",
    "    \n",
    "    #get values of dicts in 3 different lists\n",
    "    authors = list(authors.values())\n",
    "    papers= list(papers.values())\n",
    "    fields = list(fields.values())\n",
    "    \n",
    "    #convert list to tensor\n",
    "    author = torch.FloatTensor(authors)\n",
    "    paper = torch.FloatTensor(papers)\n",
    "    field = torch.FloatTensor(fields)\n",
    "    \n",
    "    #prepara data to be added as features\n",
    "    author = torch.unsqueeze(author, 1)\n",
    "    paper = torch.unsqueeze(paper, 1)\n",
    "    field = torch.unsqueeze(field, 1)\n",
    "    \n",
    "    tupl_au = (hg.nodes[\"author\"].data['feat'] ,author)\n",
    "    tupl_pa = (hg.nodes[\"paper\"].data['feat'] ,paper)\n",
    "    tupl_fi = (hg.nodes[\"field\"].data['feat'] ,field)\n",
    "    \n",
    "    author = torch.cat(tupl_au,1)\n",
    "    paper = torch.cat(tupl_pa,1)\n",
    "    field = torch.cat(tupl_fi,1)\n",
    "    \n",
    "    #add computed tensors as features\n",
    "    hg.nodes[\"author\"].data['feat'] =  author\n",
    "    hg.nodes[\"paper\"].data['feat'] =  paper\n",
    "    hg.nodes[\"field\"].data['feat'] =  field\n",
    "    \n",
    "    print(hg.nodes[\"author\"].data['feat'])\n",
    "    print(hg.nodes[\"paper\"].data['feat'])\n",
    "    print(hg.nodes[\"field\"].data['feat'])\n",
    "    \n",
    "    return hg\n",
    "\n",
    "'''compute LCC and add it as node feture'''\n",
    "def get_LCC_acm(hg):\n",
    "    \n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    \n",
    "    transform = AddMetaPaths({\n",
    "    'pap': [('paper', 'pa', 'author'),('author', 'ap', 'paper')],\n",
    "    'pfp': [('paper', 'pf', 'field'),('field','fp','paper')],\n",
    "    #'apa': [('author', 'ap', 'paper'),('paper','pa','author')],\n",
    "    #'fpf': [('field', 'fp', 'paper'),('paper','pf','field')]\n",
    "    })\n",
    "    hg2 = transform(hg2)\n",
    "    g = dgl.to_homogeneous(hg2)\n",
    "    nxg = g.to_networkx()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    #compute local clustering coefficient\n",
    "    d= nx.clustering(G2)\n",
    "    \n",
    "    #split d in 3 dictionary, one for each node type\n",
    "    authors = {k: d[k] for k in list(d)[:17351]}\n",
    "    fields = {k: d[k] for k in list(d)[17351:17423]}\n",
    "    papers = {k: d[k] for k in list(d)[17423:21449]}\n",
    "    \n",
    "    #get values of dicts in 3 different lists\n",
    "    authors = list(authors.values())\n",
    "    papers= list(papers.values())\n",
    "    fields = list(fields.values())\n",
    "    \n",
    "    #convert lists into tensors\n",
    "    author = torch.LongTensor(authors)\n",
    "    paper = torch.LongTensor(papers)\n",
    "    field = torch.LongTensor(fields)\n",
    "    \n",
    "    #prepare data to be added as features\n",
    "    author = torch.unsqueeze(author, 1)\n",
    "    paper = torch.unsqueeze(paper, 1)\n",
    "    field = torch.unsqueeze(field, 1)\n",
    "    \n",
    "    tupl_au = (hg.nodes[\"author\"].data['feat'] ,author)\n",
    "    tupl_pa = (hg.nodes[\"paper\"].data['feat'] ,paper)\n",
    "    tupl_fi = (hg.nodes[\"field\"].data['feat'] ,field)\n",
    "    \n",
    "    author = torch.cat(tupl_au,1)\n",
    "    paper = torch.cat(tupl_pa,1)\n",
    "    field = torch.cat(tupl_fi,1)\n",
    "    \n",
    "    #add computed tensors as features\n",
    "    hg.nodes[\"author\"].data['feat'] =  author\n",
    "    hg.nodes[\"paper\"].data['feat'] =  paper\n",
    "    hg.nodes[\"field\"].data['feat'] =  field\n",
    "    \n",
    "    return hg\n",
    "\n",
    "'''compute degree centrality and add it as node feture'''\n",
    "def get_degree_centrality_acm(hg):\n",
    "    \n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    \n",
    "    transform = AddMetaPaths({\n",
    "    'pap': [('paper', 'pa', 'author'),('author', 'ap', 'paper')],\n",
    "    'pfp': [('paper', 'pf', 'field'),('field','fp','paper')],\n",
    "    'apa': [('author', 'ap', 'paper'),('paper','pa','author')],\n",
    "    'fpf': [('field', 'fp', 'paper'),('paper','pf','field')]\n",
    "    })\n",
    "    hg3 = transform(hg2)\n",
    "    g = dgl.to_homogeneous(hg3)\n",
    "    nxg = g.to_networkx()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    \n",
    "    #compute degree centrality\n",
    "    d = nx.degree_centrality(G2)\n",
    "    \n",
    "    authors = {k: d[k] for k in list(d)[:17351]}\n",
    "    fields = {k: d[k] for k in list(d)[17351:17423]}\n",
    "    papers = {k: d[k] for k in list(d)[17423:21449]}\n",
    "    \n",
    "    authors = list(authors.values())\n",
    "    papers= list(papers.values())\n",
    "    fields = list(fields.values())\n",
    " \n",
    "    author = torch.LongTensor(authors)\n",
    "    paper = torch.LongTensor(papers)\n",
    "    field = torch.LongTensor(fields)\n",
    "    \n",
    "    author = torch.unsqueeze(author, 1)\n",
    "    paper = torch.unsqueeze(paper, 1)\n",
    "    field = torch.unsqueeze(field, 1)\n",
    "    \n",
    "    tupl_au = (hg.nodes[\"author\"].data['feat'] ,author)\n",
    "    tupl_pa = (hg.nodes[\"paper\"].data['feat'] ,paper)\n",
    "    tupl_fi = (hg.nodes[\"field\"].data['feat'] ,field)\n",
    "    \n",
    "    author = torch.cat(tupl_au,1)\n",
    "    paper = torch.cat(tupl_pa,1)\n",
    "    field = torch.cat(tupl_fi,1)\n",
    "    \n",
    "    hg.nodes[\"author\"].data['feat'] =  author\n",
    "    hg.nodes[\"paper\"].data['feat'] =  paper\n",
    "    hg.nodes[\"field\"].data['feat'] =  field\n",
    "    \n",
    "    return hg\n",
    "\n",
    "'''compute average degree connectivity and add it as node feture'''\n",
    "def get_average_degree_connectivity(hg):\n",
    "    \n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    \n",
    "    #transform = AddMetaPaths({\n",
    "    #'pap': [('paper', 'pa', 'author'),('author', 'ap', 'paper')],\n",
    "    #'pfp': [('paper', 'pf', 'field'),('field','fp','paper')],\n",
    "    #'apa': [('author', 'ap', 'paper'),('paper','pa','author')],\n",
    "    #'fpf': [('field', 'fp', 'paper'),('paper','pf','field')]\n",
    "    #})\n",
    "    #hg2 = transform(hg2)\n",
    "    g = dgl.to_homogeneous(hg2)\n",
    "    nxg = g.to_networkx()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    d = nx.average_degree_connectivity(G2)\n",
    "    \n",
    "    \n",
    "    authors = {k: d[k] for k in list(d)[:17351]}\n",
    "    fields = {k: d[k] for k in list(d)[17351:17423]}\n",
    "    papers = {k: d[k] for k in list(d)[17423:21449]}\n",
    "    \n",
    "    authors = list(authors.values())\n",
    "    papers= list(papers.values())\n",
    "    fields = list(fields.values())\n",
    " \n",
    "    author = torch.LongTensor(authors)\n",
    "    paper = torch.LongTensor(papers)\n",
    "    field = torch.LongTensor(fields)\n",
    "    \n",
    "    author = torch.unsqueeze(author, 1)\n",
    "    paper = torch.unsqueeze(paper, 1)\n",
    "    field = torch.unsqueeze(field, 1)\n",
    "    \n",
    "    tupl_au = (hg.nodes[\"author\"].data['feat'] ,author)\n",
    "    tupl_pa = (hg.nodes[\"paper\"].data['feat'] ,paper)\n",
    "    tupl_fi = (hg.nodes[\"field\"].data['feat'] ,field)\n",
    "    \n",
    "    author = torch.cat(tupl_au,1)\n",
    "    paper = torch.cat(tupl_pa,1)\n",
    "    field = torch.cat(tupl_fi,1)\n",
    "    \n",
    "    hg.nodes[\"author\"].data['feat'] =  author\n",
    "    hg.nodes[\"paper\"].data['feat'] =  paper\n",
    "    hg.nodes[\"field\"].data['feat'] =  field\n",
    "    \n",
    "    return hg\n",
    "\n",
    "\n",
    "'''compute triangle counter and add it as node feture'''\n",
    "def get_triangle_count_acm(hg):\n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    \n",
    "    transform = AddMetaPaths({\n",
    "    #'pap': [('paper', 'pa', 'author'),('author', 'ap', 'paper')],\n",
    "    #'pfp': [('paper', 'pf', 'field'),('field','fp','paper')],\n",
    "    'apa': [('author', 'ap', 'paper'),('paper','pa','author')],\n",
    "    'fpf': [('field', 'fp', 'paper'),('paper','pf','field')]\n",
    "    \n",
    "    })\n",
    "    \n",
    "    hg3 = transform(hg2)\n",
    "    g = dgl.to_homogeneous(hg2)\n",
    "    nxg = g.to_networkx()\n",
    "    n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(n)\n",
    "    d = nx.triangles(G2)\n",
    "\n",
    "    authors = {k: d[k] for k in list(d)[:17351]}\n",
    "    fields = {k: d[k] for k in list(d)[17351:17423]}\n",
    "    papers = {k: d[k] for k in list(d)[17423:21449]}\n",
    "    authors = list(authors.values())\n",
    "    papers= list(papers.values())\n",
    "    fields = list(fields.values())\n",
    " \n",
    "    author = torch.LongTensor(authors)\n",
    "    paper = torch.LongTensor(papers)\n",
    "    field = torch.LongTensor(fields)\n",
    "    \n",
    "    author = torch.unsqueeze(author, 1)\n",
    "    paper = torch.unsqueeze(paper, 1)\n",
    "    field = torch.unsqueeze(field, 1)\n",
    "    \n",
    "    tupl_au = (hg.nodes[\"author\"].data['feat'] ,author)\n",
    "    tupl_pa = (hg.nodes[\"paper\"].data['feat'] ,paper)\n",
    "    tupl_fi = (hg.nodes[\"field\"].data['feat'] ,field)\n",
    "    \n",
    "    author = torch.cat(tupl_au,1)\n",
    "    paper = torch.cat(tupl_pa,1)\n",
    "    field = torch.cat(tupl_fi,1)\n",
    "    \n",
    "    hg.nodes[\"author\"].data['feat'] =  author\n",
    "    hg.nodes[\"paper\"].data['feat'] =  paper\n",
    "    hg.nodes[\"field\"].data['feat'] =  field\n",
    "    \n",
    "    return hg\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe98862",
   "metadata": {},
   "source": [
    "## UTILS FOR IMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "282e73d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''compute page rank and add it as node feture'''\n",
    "def get_degree_imdb(hg):\n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    #find all actor of a film\n",
    "    transform = AddMetaPaths({\n",
    "    #'mam': [('movie', 'ma', 'actor'),('actor', 'am', 'movie')],\n",
    "    #'ama': [('actor', 'am', 'movie'),('movie','ma','actor')],\n",
    "    #'dmd': [('director', 'dm', 'movie'),('movie','md','director')],\n",
    "    'mdm': [('movie', 'md', 'director'),('director','dm','movie')]\n",
    "    })\n",
    "    hg1 = transform(hg2)\n",
    "    g = dgl.to_homogeneous(hg1)\n",
    "    nxg = g.to_networkx()\n",
    "    #n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    \n",
    "    d = dict(G2.degree)\n",
    "    \n",
    "    \n",
    "    allow_zero_in_degree = True\n",
    "    \n",
    "    actors = {k: d[k] for k in list(d)[:5257]}\n",
    "    directors = {k: d[k] for k in list(d)[5257:7338]}\n",
    "    movies = {k: d[k] for k in list(d)[7338:11616]}\n",
    "    \n",
    "    actors_deg = list(actors.values())\n",
    "    directors_deg = list(directors.values())\n",
    "    movies_deg = list(movies.values())\n",
    "    \n",
    "    actor = torch.LongTensor(actors_deg)\n",
    "    director = torch.LongTensor(directors_deg)\n",
    "    movie = torch.LongTensor(movies_deg)\n",
    "    \n",
    "    actor = torch.unsqueeze(actor, 1)\n",
    "    director = torch.unsqueeze(director, 1)\n",
    "    movie = torch.unsqueeze(movie, 1)\n",
    "    \n",
    "    tupl_a = (hg.nodes[\"actor\"].data['feat'] ,actor)\n",
    "    tupl_d = (hg.nodes[\"director\"].data['feat'] ,director)\n",
    "    tupl_m = (hg.nodes[\"movie\"].data['feat'] ,movie)\n",
    "    \n",
    "    actor = torch.cat(tupl_a,1)\n",
    "    director = torch.cat(tupl_d,1)\n",
    "    movie = torch.cat(tupl_m,1)\n",
    "    \n",
    "    hg.nodes[\"actor\"].data['feat'] =  actor\n",
    "    hg.nodes[\"director\"].data['feat'] =  director\n",
    "    hg.nodes[\"movie\"].data['feat'] =  movie\n",
    "    \n",
    "    print(hg.nodes[\"actor\"].data['feat'])\n",
    "                    \n",
    "    return hg\n",
    "\n",
    "\n",
    "\n",
    "'''compute triangle count and add it as node feture'''\n",
    "def get_triangle_count_imdb(hg):\n",
    "    hg1 = copy.deepcopy(hg)\n",
    "    #find all actor of a film\n",
    "    transform = AddMetaPaths({\n",
    "    #'mam': [('movie', 'ma', 'actor'),('actor', 'am', 'movie')],\n",
    "    #'ama': [('actor', 'am', 'movie'),('movie','ma','actor')],\n",
    "    'dmd': [('director', 'dm', 'movie'),('movie','md','director')],\n",
    "    #'mdm': [('movie', 'md', 'director'),('director','dm','movie')]\n",
    "    })\n",
    "    hg2 = transform(hg1)\n",
    "    g = dgl.to_homogeneous(hg2)\n",
    "    nxg = g.to_networkx()\n",
    "    #n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    d = nx.triangles(G2)\n",
    "    \n",
    "    \n",
    "    actors = {k: d[k] for k in list(d)[:5257]}\n",
    "    directors = {k: d[k] for k in list(d)[5257:7338]}\n",
    "    movies = {k: d[k] for k in list(d)[7338:11616]}\n",
    "    \n",
    "    actors_count = list(actors.values())\n",
    "    directors_count = list(directors.values())\n",
    "    movies_count = list(movies.values())\n",
    "    \n",
    "    actor = torch.LongTensor(actors_count)\n",
    "    director = torch.LongTensor(directors_count)\n",
    "    movie = torch.LongTensor(movies_count)\n",
    "    \n",
    "    actor = torch.unsqueeze(actor, 1)\n",
    "    director = torch.unsqueeze(director, 1)\n",
    "    movie = torch.unsqueeze(movie, 1)\n",
    "    \n",
    "    tupl_a = (hg.nodes[\"actor\"].data['feat'] ,actor)\n",
    "    tupl_d = (hg.nodes[\"director\"].data['feat'] ,director)\n",
    "    tupl_m = (hg.nodes[\"movie\"].data['feat'] ,movie)\n",
    "    \n",
    "    actor = torch.cat(tupl_a,1)\n",
    "    director = torch.cat(tupl_d,1)\n",
    "    movie = torch.cat(tupl_m,1)\n",
    "    \n",
    "    hg.nodes[\"actor\"].data['feat'] =  actor\n",
    "    hg.nodes[\"director\"].data['feat'] =  director\n",
    "    hg.nodes[\"movie\"].data['feat'] =  movie\n",
    "    \n",
    "    print(hg.nodes[\"actor\"].data['feat'])\n",
    "    print(len(hg.nodes[\"actor\"].data['feat'][0]))\n",
    "\n",
    "    return hg\n",
    "\n",
    "'''compute degree centrality and add it as node feture'''\n",
    "def get_degree_centrality_imdb(hg):\n",
    "    hg1 = copy.deepcopy(hg)\n",
    "    #find all actor of a film\n",
    "    transform = AddMetaPaths({\n",
    "    'mam': [('movie', 'ma', 'actor'),('actor', 'am', 'movie')],\n",
    "    #'ama': [('actor', 'am', 'movie'),('movie','ma','actor')],\n",
    "    #'dmd': [('director', 'dm', 'movie'),('movie','md','director')],\n",
    "    #'mdm': [('movie', 'md', 'director'),('director','dm','movie')]\n",
    "    })\n",
    "    hg2 = transform(hg1)\n",
    "    g = dgl.to_homogeneous(hg2)\n",
    "    nxg = g.to_networkx()\n",
    "    #n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    d= nx.degree_centrality(G2)\n",
    "    \n",
    "    \n",
    "    actors = {k: d[k] for k in list(d)[:5257]}\n",
    "    directors = {k: d[k] for k in list(d)[5257:7338]}\n",
    "    movies = {k: d[k] for k in list(d)[7338:11616]}\n",
    "    \n",
    "    actors_count = list(actors.values())\n",
    "    directors_count = list(directors.values())\n",
    "    movies_count = list(movies.values())\n",
    "    \n",
    "    actor = torch.LongTensor(actors_count)\n",
    "    director = torch.LongTensor(directors_count)\n",
    "    movie = torch.LongTensor(movies_count)\n",
    "    \n",
    "    actor = torch.unsqueeze(actor, 1)\n",
    "    director = torch.unsqueeze(director, 1)\n",
    "    movie = torch.unsqueeze(movie, 1)\n",
    "    \n",
    "    tupl_a = (hg.nodes[\"actor\"].data['feat'] ,actor)\n",
    "    tupl_d = (hg.nodes[\"director\"].data['feat'] ,director)\n",
    "    tupl_m = (hg.nodes[\"movie\"].data['feat'] ,movie)\n",
    "    \n",
    "    actor = torch.cat(tupl_a,1)\n",
    "    director = torch.cat(tupl_d,1)\n",
    "    movie = torch.cat(tupl_m,1)\n",
    "    \n",
    "    hg.nodes[\"actor\"].data['feat'] =  actor\n",
    "    hg.nodes[\"director\"].data['feat'] =  director\n",
    "    hg.nodes[\"movie\"].data['feat'] =  movie\n",
    "    \n",
    "    print(hg.nodes[\"actor\"].data['feat'])\n",
    "    print(len(hg.nodes[\"actor\"].data['feat'][0]))\n",
    "\n",
    "    return hg\n",
    "\n",
    "\n",
    "'''compute LCC and add it as node feture'''\n",
    "def get_LCC_imdb(hg):\n",
    "    hg1 = copy.deepcopy(hg)\n",
    "    #find all actor of a film\n",
    "    transform = AddMetaPaths({\n",
    "    'mam': [('movie', 'ma', 'actor'),('actor', 'am', 'movie')],\n",
    "    #'ama': [('actor', 'am', 'movie'),('movie','ma','actor')],\n",
    "    #'dmd': [('director', 'dm', 'movie'),('movie','md','director')],\n",
    "    #'mdm': [('movie', 'md', 'director'),('director','dm','movie')]\n",
    "    })\n",
    "    hg2 = transform(hg1)\n",
    "    g = dgl.to_homogeneous(hg2)\n",
    "    nxg = g.to_networkx()\n",
    "    #n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    d= nx.clustering(G2)\n",
    "    \n",
    "    \n",
    "    actors = {k: d[k] for k in list(d)[:5257]}\n",
    "    directors = {k: d[k] for k in list(d)[5257:7338]}\n",
    "    movies = {k: d[k] for k in list(d)[7338:11616]}\n",
    "    \n",
    "    actors_count = list(actors.values())\n",
    "    directors_count = list(directors.values())\n",
    "    movies_count = list(movies.values())\n",
    "    \n",
    "    actor = torch.LongTensor(actors_count)\n",
    "    director = torch.LongTensor(directors_count)\n",
    "    movie = torch.LongTensor(movies_count)\n",
    "    \n",
    "    actor = torch.unsqueeze(actor, 1)\n",
    "    director = torch.unsqueeze(director, 1)\n",
    "    movie = torch.unsqueeze(movie, 1)\n",
    "    \n",
    "    tupl_a = (hg.nodes[\"actor\"].data['feat'] ,actor)\n",
    "    tupl_d = (hg.nodes[\"director\"].data['feat'] ,director)\n",
    "    tupl_m = (hg.nodes[\"movie\"].data['feat'] ,movie)\n",
    "    \n",
    "    actor = torch.cat(tupl_a,1)\n",
    "    director = torch.cat(tupl_d,1)\n",
    "    movie = torch.cat(tupl_m,1)\n",
    "    \n",
    "    hg.nodes[\"actor\"].data['feat'] =  actor\n",
    "    hg.nodes[\"director\"].data['feat'] =  director\n",
    "    hg.nodes[\"movie\"].data['feat'] =  movie\n",
    "    \n",
    "    print(hg.nodes[\"actor\"].data['feat'])\n",
    "    print(len(hg.nodes[\"actor\"].data['feat'][0]))\n",
    "    print(len(hg.nodes[\"movie\"].data['feat'][0]))\n",
    "\n",
    "    return hg\n",
    "\n",
    "\n",
    "'''compute page rank and add it as node feture'''\n",
    "def get_PageRank_imdb(hg):\n",
    "    hg1 = copy.deepcopy(hg)\n",
    "    #find all actor of a film\n",
    "    transform = AddMetaPaths({\n",
    "    'mam': [('movie', 'ma', 'actor'),('actor', 'am', 'movie')],\n",
    "    #'ama': [('actor', 'am', 'movie'),('movie','ma','actor')],\n",
    "    #'dmd': [('director', 'dm', 'movie'),('movie','md','director')],\n",
    "    #'mdm': [('movie', 'md', 'director'),('director','dm','movie')]\n",
    "    })\n",
    "    hg2 = transform(hg1)\n",
    "    g = dgl.to_homogeneous(hg2)\n",
    "    nxg = g.to_networkx()\n",
    "    #n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    d = nx.pagerank(G2, alpha=0.9)\n",
    "    \n",
    "    \n",
    "    actors = {k: d[k] for k in list(d)[:5257]}\n",
    "    directors = {k: d[k] for k in list(d)[5257:7338]}\n",
    "    movies = {k: d[k] for k in list(d)[7338:11616]}\n",
    "    \n",
    "    actors_count = list(actors.values())\n",
    "    directors_count = list(directors.values())\n",
    "    movies_count = list(movies.values())\n",
    "    \n",
    "    actor = torch.FloatTensor(actors_count)\n",
    "    director = torch.FloatTensor(directors_count)\n",
    "    movie = torch.FloatTensor(movies_count)\n",
    "    \n",
    "    actor = torch.unsqueeze(actor, 1)\n",
    "    director = torch.unsqueeze(director, 1)\n",
    "    movie = torch.unsqueeze(movie, 1)\n",
    "    \n",
    "    tupl_a = (hg.nodes[\"actor\"].data['feat'] ,actor)\n",
    "    tupl_d = (hg.nodes[\"director\"].data['feat'] ,director)\n",
    "    tupl_m = (hg.nodes[\"movie\"].data['feat'] ,movie)\n",
    "    \n",
    "    actor = torch.cat(tupl_a,1)\n",
    "    director = torch.cat(tupl_d,1)\n",
    "    movie = torch.cat(tupl_m,1)\n",
    "    \n",
    "    hg.nodes[\"actor\"].data['feat'] =  actor\n",
    "    hg.nodes[\"director\"].data['feat'] =  director\n",
    "    hg.nodes[\"movie\"].data['feat'] =  movie\n",
    "    \n",
    "    print(hg.nodes[\"actor\"].data['feat'])\n",
    "    print(len(hg.nodes[\"actor\"].data['feat'][0]))\n",
    "\n",
    "    return hg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12104209",
   "metadata": {},
   "source": [
    "## UTILS FOR DBLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cfc55bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''compute mode degree and add it as node feture'''\n",
    "def get_degree_dblp(hg):\n",
    "    \n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    \n",
    "    transform = AddMetaPaths({\n",
    "    'apa': [('author', 'ap', 'paper'),('paper', 'pa', 'author')],\n",
    "    #'pap': [('paper', 'pa', 'author'),('author','ap','paper')],\n",
    "    #'ptp': [('paper', 'pt', 'term'),('term','tp','paper')],\n",
    "    #'tpt': [('term', 'tp', 'paper'),('paper','pt','term')],\n",
    "    #'pcp': [('paper', 'pc', 'conf'),('conf','cp','paper')],\n",
    "    'cpc': [('conf', 'cp', 'paper'),('paper','pc','conf')]\n",
    "    })\n",
    "    hg1 = transform(hg2)\n",
    "    g = dgl.to_homogeneous(hg2)\n",
    "    nxg = g.to_networkx()\n",
    "    #n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    \n",
    "    d = dict(G2.degree)\n",
    "    \n",
    "    allow_zero_in_degree = True\n",
    "    \n",
    "    authors = {k: d[k] for k in list(d)[:4057]}\n",
    "    confs = {k: d[k] for k in list(d)[4057:4077]}\n",
    "    papers = {k: d[k] for k in list(d)[4077:18405]}\n",
    "    terms = {k: d[k] for k in list(d)[18405:26128]}\n",
    "    \n",
    "    authors_deg = list(authors.values())\n",
    "    confs_deg = list(confs.values())\n",
    "    papers_deg = list(papers.values())\n",
    "    terms_deg = list(terms.values())\n",
    "    \n",
    "    author = torch.LongTensor(authors_deg)\n",
    "    conf = torch.LongTensor(confs_deg)\n",
    "    paper = torch.LongTensor(papers_deg)\n",
    "    term = torch.LongTensor(terms_deg)\n",
    "    \n",
    "    author = torch.unsqueeze(author, 1)\n",
    "    conf = torch.unsqueeze(conf, 1)\n",
    "    paper = torch.unsqueeze(paper, 1)\n",
    "    term = torch.unsqueeze(term, 1)\n",
    "    \n",
    "    tupl_a = (hg.nodes[\"author\"].data['feat'] ,author)\n",
    "    tupl_c = (hg.nodes[\"conf\"].data['feat'] ,conf)\n",
    "    tupl_p = (hg.nodes[\"paper\"].data['feat'] ,paper)\n",
    "    tupl_t = (hg.nodes[\"term\"].data['feat'] ,term)\n",
    "    \n",
    "    author = torch.cat(tupl_a,1)\n",
    "    conf = torch.cat(tupl_c,1)\n",
    "    paper = torch.cat(tupl_p,1)\n",
    "    term = torch.cat(tupl_t,1)\n",
    "    \n",
    "    hg.nodes[\"author\"].data['feat'] =  author\n",
    "    hg.nodes[\"conf\"].data['feat'] =  conf\n",
    "    hg.nodes[\"paper\"].data['feat'] =  paper\n",
    "    hg.nodes[\"term\"].data['feat'] =  term\n",
    "               \n",
    "    return hg\n",
    "\n",
    "'''compute degree centrality and add it as node feture'''\n",
    "def get_degree_centrality_dblp(hg):\n",
    "    \n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    \n",
    "    transform = AddMetaPaths({\n",
    "    'apa': [('author', 'ap', 'paper'),('paper', 'pa', 'author')],\n",
    "    #'pap': [('paper', 'pa', 'author'),('author','ap','paper')],\n",
    "    #'ptp': [('paper', 'pt', 'term'),('term','tp','paper')],\n",
    "    #'tpt': [('term', 'tp', 'paper'),('paper','pt','term')],\n",
    "    #'pcp': [('paper', 'pc', 'conf'),('conf','cp','paper')],\n",
    "    #'cpc': [('conf', 'cp', 'paper'),('paper','pc','conf')]\n",
    "    })\n",
    "    hg1 = transform(hg2)\n",
    "    g = dgl.to_homogeneous(hg1)\n",
    "    nxg = g.to_networkx()\n",
    "    #n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    \n",
    "    d= nx.degree_centrality(G2)\n",
    "    \n",
    "    allow_zero_in_degree = True\n",
    "    \n",
    "    authors = {k: d[k] for k in list(d)[:4057]}\n",
    "    confs = {k: d[k] for k in list(d)[4057:4077]}\n",
    "    papers = {k: d[k] for k in list(d)[4077:18405]}\n",
    "    terms = {k: d[k] for k in list(d)[18405:26128]}\n",
    "    \n",
    "    authors_deg = list(authors.values())\n",
    "    confs_deg = list(confs.values())\n",
    "    papers_deg = list(papers.values())\n",
    "    terms_deg = list(terms.values())\n",
    "    \n",
    "    author = torch.LongTensor(authors_deg)\n",
    "    conf = torch.LongTensor(confs_deg)\n",
    "    paper = torch.LongTensor(papers_deg)\n",
    "    term = torch.LongTensor(terms_deg)\n",
    "    \n",
    "    author = torch.unsqueeze(author, 1)\n",
    "    conf = torch.unsqueeze(conf, 1)\n",
    "    paper = torch.unsqueeze(paper, 1)\n",
    "    term = torch.unsqueeze(term, 1)\n",
    "    \n",
    "    tupl_a = (hg.nodes[\"author\"].data['feat'] ,author)\n",
    "    tupl_c = (hg.nodes[\"conf\"].data['feat'] ,conf)\n",
    "    tupl_p = (hg.nodes[\"paper\"].data['feat'] ,paper)\n",
    "    tupl_t = (hg.nodes[\"term\"].data['feat'] ,term)\n",
    "    \n",
    "    author = torch.cat(tupl_a,1)\n",
    "    conf = torch.cat(tupl_c,1)\n",
    "    paper = torch.cat(tupl_p,1)\n",
    "    term = torch.cat(tupl_t,1)\n",
    "    \n",
    "    hg.nodes[\"author\"].data['feat'] =  author\n",
    "    hg.nodes[\"conf\"].data['feat'] =  conf\n",
    "    hg.nodes[\"paper\"].data['feat'] =  paper\n",
    "    hg.nodes[\"term\"].data['feat'] =  term\n",
    "               \n",
    "    return hg\n",
    "\n",
    "'''compute triangle count and add it as node feture'''\n",
    "def get_triangle_count_dblp(hg):\n",
    "    \n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    \n",
    "    transform = AddMetaPaths({\n",
    "    #'apa': [('author', 'ap', 'paper'),('paper', 'pa', 'author')],\n",
    "    'pap': [('paper', 'pa', 'author'),('author','ap','paper')],\n",
    "    #'ptp': [('paper', 'pt', 'term'),('term','tp','paper')],\n",
    "    #'tpt': [('term', 'tp', 'paper'),('paper','pt','term')],\n",
    "    #'pcp': [('paper', 'pc', 'conf'),('conf','cp','paper')],\n",
    "    #'cpc': [('conf', 'cp', 'paper'),('paper','pc','conf')]\n",
    "    })\n",
    "    hg1 = transform(hg2)\n",
    "    g = dgl.to_homogeneous(hg1)\n",
    "    nxg = g.to_networkx()\n",
    "    #n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    \n",
    "    d = nx.triangles(G2)\n",
    "    \n",
    "    authors = {k: d[k] for k in list(d)[:4057]}\n",
    "    confs = {k: d[k] for k in list(d)[4057:4077]}\n",
    "    papers = {k: d[k] for k in list(d)[4077:18405]}\n",
    "    terms = {k: d[k] for k in list(d)[18405:26128]}\n",
    "    \n",
    "    \n",
    "    authors_deg = list(authors.values())\n",
    "    confs_deg = list(confs.values())\n",
    "    papers_deg = list(papers.values())\n",
    "    terms_deg = list(terms.values())\n",
    "    \n",
    "    author = torch.LongTensor(authors_deg)\n",
    "    conf = torch.LongTensor(confs_deg)\n",
    "    paper = torch.LongTensor(papers_deg)\n",
    "    term = torch.LongTensor(terms_deg)\n",
    "    \n",
    "    author = torch.unsqueeze(author, 1)\n",
    "    conf = torch.unsqueeze(conf, 1)\n",
    "    paper = torch.unsqueeze(paper, 1)\n",
    "    term = torch.unsqueeze(term, 1)\n",
    "    \n",
    "    tupl_a = (hg.nodes[\"author\"].data['feat'] ,author)\n",
    "    tupl_c = (hg.nodes[\"conf\"].data['feat'] ,conf)\n",
    "    tupl_p = (hg.nodes[\"paper\"].data['feat'] ,paper)\n",
    "    tupl_t = (hg.nodes[\"term\"].data['feat'] ,term)\n",
    "    \n",
    "    author = torch.cat(tupl_a,1)\n",
    "    conf = torch.cat(tupl_c,1)\n",
    "    paper = torch.cat(tupl_p,1)\n",
    "    term = torch.cat(tupl_t,1)\n",
    "    \n",
    "    hg.nodes[\"author\"].data['feat'] =  author\n",
    "    hg.nodes[\"conf\"].data['feat'] =  conf\n",
    "    hg.nodes[\"paper\"].data['feat'] =  paper\n",
    "    hg.nodes[\"term\"].data['feat'] =  term\n",
    "    \n",
    "    print(hg.nodes[\"author\"].data['feat'])\n",
    "    print(len(hg.nodes[\"author\"].data['feat'][0]))\n",
    "                         \n",
    "    return hg\n",
    "\n",
    "'''compute page rank and add it as node feture'''\n",
    "def get_PageRank_dblp(hg):\n",
    "    \n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    \n",
    "    #transform = AddMetaPaths({\n",
    "    #'apa': [('author', 'ap', 'paper'),('paper', 'pa', 'author')],\n",
    "    #'pap': [('paper', 'pa', 'author'),('author','ap','paper')],\n",
    "    #'ptp': [('paper', 'pt', 'term'),('term','tp','paper')],\n",
    "    #'tpt': [('term', 'tp', 'paper'),('paper','pt','term')],\n",
    "    #'pcp': [('paper', 'pc', 'conf'),('conf','cp','paper')],\n",
    "    #'cpc': [('conf', 'cp', 'paper'),('paper','pc','conf')]\n",
    "    #})\n",
    "    #hg1 = transform(hg2)\n",
    "    g = dgl.to_homogeneous(hg2)\n",
    "    nxg = g.to_networkx()\n",
    "    #n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    \n",
    "    d = nx.pagerank(G2, alpha=0.9)\n",
    "    \n",
    "    authors = {k: d[k] for k in list(d)[:4057]}\n",
    "    confs = {k: d[k] for k in list(d)[4057:4077]}\n",
    "    papers = {k: d[k] for k in list(d)[4077:18405]}\n",
    "    terms = {k: d[k] for k in list(d)[18405:26128]}\n",
    "    \n",
    "    authors_deg = list(authors.values())\n",
    "    confs_deg = list(confs.values())\n",
    "    papers_deg = list(papers.values())\n",
    "    terms_deg = list(terms.values())\n",
    "    \n",
    "    author = torch.FloatTensor(authors_deg)\n",
    "    conf = torch.FloatTensor(confs_deg)\n",
    "    paper = torch.FloatTensor(papers_deg)\n",
    "    term = torch.FloatTensor(terms_deg)\n",
    "    \n",
    "    author = torch.unsqueeze(author, 1)\n",
    "    conf = torch.unsqueeze(conf, 1)\n",
    "    paper = torch.unsqueeze(paper, 1)\n",
    "    term = torch.unsqueeze(term, 1)\n",
    "    \n",
    "    tupl_a = (hg.nodes[\"author\"].data['feat'] ,author)\n",
    "    tupl_c = (hg.nodes[\"conf\"].data['feat'] ,conf)\n",
    "    tupl_p = (hg.nodes[\"paper\"].data['feat'] ,paper)\n",
    "    tupl_t = (hg.nodes[\"term\"].data['feat'] ,term)\n",
    "    \n",
    "    author = torch.cat(tupl_a,1)\n",
    "    conf = torch.cat(tupl_c,1)\n",
    "    paper = torch.cat(tupl_p,1)\n",
    "    term = torch.cat(tupl_t,1)\n",
    "    \n",
    "    hg.nodes[\"author\"].data['feat'] =  author\n",
    "    hg.nodes[\"conf\"].data['feat'] =  conf\n",
    "    hg.nodes[\"paper\"].data['feat'] =  paper\n",
    "    hg.nodes[\"term\"].data['feat'] =  term\n",
    "                    \n",
    "    return hg\n",
    "\n",
    "'''compute LCC and add it as node feture'''\n",
    "def get_LCC_dblp(hg):\n",
    "    \n",
    "    hg2 = copy.deepcopy(hg)\n",
    "    \n",
    "    transform = AddMetaPaths({\n",
    "    #'apa': [('author', 'ap', 'paper'),('paper', 'pa', 'author')],\n",
    "    #'pap': [('paper', 'pa', 'author'),('author','ap','paper')],\n",
    "    #'ptp': [('paper', 'pt', 'term'),('term','tp','paper')],\n",
    "    'tpt': [('term', 'tp', 'paper'),('paper','pt','term')],\n",
    "    #'pcp': [('paper', 'pc', 'conf'),('conf','cp','paper')],\n",
    "    #'cpc': [('conf', 'cp', 'paper'),('paper','pc','conf')]\n",
    "    })\n",
    "    hg1 = transform(hg2)\n",
    "    g = dgl.to_homogeneous(hg1)\n",
    "    nxg = g.to_networkx()\n",
    "    #n = nxg.to_undirected()\n",
    "    G2 = nx.Graph(nxg)\n",
    "    \n",
    "    d= nx.clustering(G2)\n",
    "    \n",
    "    authors = {k: d[k] for k in list(d)[:4057]}\n",
    "    confs = {k: d[k] for k in list(d)[4057:4077]}\n",
    "    papers = {k: d[k] for k in list(d)[4077:18405]}\n",
    "    terms = {k: d[k] for k in list(d)[18405:26128]}\n",
    "    \n",
    "    authors_deg = list(authors.values())\n",
    "    confs_deg = list(confs.values())\n",
    "    papers_deg = list(papers.values())\n",
    "    terms_deg = list(terms.values())\n",
    "    \n",
    "    author = torch.FloatTensor(authors_deg)\n",
    "    conf = torch.FloatTensor(confs_deg)\n",
    "    paper = torch.FloatTensor(papers_deg)\n",
    "    term = torch.FloatTensor(terms_deg)\n",
    "    \n",
    "    author = torch.unsqueeze(author, 1)\n",
    "    conf = torch.unsqueeze(conf, 1)\n",
    "    paper = torch.unsqueeze(paper, 1)\n",
    "    term = torch.unsqueeze(term, 1)\n",
    "    \n",
    "    tupl_a = (hg.nodes[\"author\"].data['feat'] ,author)\n",
    "    tupl_c = (hg.nodes[\"conf\"].data['feat'] ,conf)\n",
    "    tupl_p = (hg.nodes[\"paper\"].data['feat'] ,paper)\n",
    "    tupl_t = (hg.nodes[\"term\"].data['feat'] ,term)\n",
    "    \n",
    "    author = torch.cat(tupl_a,1)\n",
    "    conf = torch.cat(tupl_c,1)\n",
    "    paper = torch.cat(tupl_p,1)\n",
    "    term = torch.cat(tupl_t,1)\n",
    "    \n",
    "    hg.nodes[\"author\"].data['feat'] =  author\n",
    "    hg.nodes[\"conf\"].data['feat'] =  conf\n",
    "    hg.nodes[\"paper\"].data['feat'] =  paper\n",
    "    hg.nodes[\"term\"].data['feat'] =  term\n",
    "                \n",
    "    return hg\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d3e460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
